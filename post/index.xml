<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | FS</title>
    <link>https://frankschae.github.io/post/</link>
      <atom:link href="https://frankschae.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Frank Schäfer</copyright><lastBuildDate>Wed, 16 Jun 2021 14:50:17 +0200</lastBuildDate>
    <image>
      <url>https://frankschae.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://frankschae.github.io/post/</link>
    </image>
    
    <item>
      <title>Neural Hybrid Differential Equations</title>
      <link>https://frankschae.github.io/post/hybridde/</link>
      <pubDate>Wed, 16 Jun 2021 14:50:17 +0200</pubDate>
      <guid>https://frankschae.github.io/post/hybridde/</guid>
      <description>&lt;p&gt;I am delighted that I have been awarded my second GSoC stipend this year.  I look forward to carrying out the ambitious project scope with my mentors 
&lt;a href=&#34;https://github.com/ChrisRackauckas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris Rackauckas&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mschauer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moritz Schauer&lt;/a&gt;,  
&lt;a href=&#34;https://github.com/YingboMa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yingbo Ma&lt;/a&gt;, and 
&lt;a href=&#34;https://github.com/mohamed82008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mohamed Tarek&lt;/a&gt;. This year&amp;rsquo;s project is embedded within the 
&lt;a href=&#34;https://summerofcode.withgoogle.com/organizations/5765643267211264/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NumFocus&lt;/a&gt;/
&lt;a href=&#34;https://sciml.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; organization and comprises adjoint sensitivity methods for events, shadowing methods for chaotic dynamics, symbolically generated adjoint methods, and further AD tooling within the Julia Language.&lt;/p&gt;
&lt;p&gt;This first post aims to illustrate our new (adjoint) sensitivity analysis tools with respect to event handling in (ordinary) differential equations (DEs).&lt;/p&gt;
&lt;h2 id=&#34;hybrid-differential-equations&#34;&gt;Hybrid Differential Equations&lt;/h2&gt;
&lt;p&gt;DEs with additional explicit or implicit discontinuities are called hybrid DEs. Within the SciML software suite, such discontinuities may be incorporated into DE models by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;callbacks&lt;/a&gt;. Evidently, the incorporation of discontinuities allows a user to specify changes (&lt;em&gt;events&lt;/em&gt;) in the system, i.e., changes of the state or the parameters of the DE, which cannot be modeled by a plain ordinary DE. While explicit events can be described by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/#DiscreteCallback-Examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiscreteCallbacks&lt;/a&gt;, implicit events have to be specified by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/#ContinuousCallback-Examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ContinuousCallbacks&lt;/a&gt;. That is, explicit events possess explicit event times, while implicit events are triggered when a continuous function evaluates to &lt;code&gt;0&lt;/code&gt;. Thus, implicit events require some sort of rootfinding procedure.&lt;/p&gt;
&lt;p&gt;Some relevant examples for hybrid DEs with discrete or continuous callbacks are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;quantum optics experiments, where photon-counting measurements lead to jumps in the quantum state that occur with a variable rate (&lt;code&gt;ContinuousCallback&lt;/code&gt;), see for instance Appendix A in Ref.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;a bouncing ball&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; (&lt;code&gt;ContinuousCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;classical point process models, such as a Poisson process&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;digital controllers&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, where a continuous system dynamics is controlled by a discrete-time controller (&lt;code&gt;DiscreteCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;pharmacokinetic models&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, where explicit dosing times change the drug concentration in the blood (&lt;code&gt;DiscreteCallback&lt;/code&gt;). The simplest possible example being the one-compartment PK model.&lt;/li&gt;
&lt;li&gt;kicked oscillator dynamics, e.g., a harmonic oscillator that gets a kick at some time points (&lt;code&gt;DiscreteCallback&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The associated sensitivity methods that allow us to differentiate through the respective hybrid DE systems have been recently introduced in Refs. &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kicked-harmonic-oscillator&#34;&gt;Kicked harmonic oscillator&lt;/h2&gt;
&lt;p&gt;Let us consider the simple physical model of a damped harmonic oscillator, described by an ODE of the form&lt;/p&gt;
&lt;p&gt;$$
\ddot{x}(t) + a\cdot\dot{x}(t) + b \cdot x(t) = 0 ,
$$&lt;/p&gt;
&lt;p&gt;where $a=0.1$ and $b=1$ with initial conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
x(t=0) &amp;amp;= 1  \\&lt;br&gt;
v(t=0) &amp;amp;= \dot{x}(t=0) = 0.
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This second order DE can be 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_differential_equation#Reduction_of_order&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reduced&lt;/a&gt; to two first order DEs, such that we can straightforwardly simulate the resulting ODE with the &lt;code&gt;DifferentialEquations.jl&lt;/code&gt; package. (Instead of doing this reduction manually, we could also use 
&lt;a href=&#34;https://mtk.sciml.ai/stable/tutorials/higher_order/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ModelingToolkit.jl&lt;/code&gt;&lt;/a&gt; to transform automatically the DE. Alternatively, for second order ODEs, there is also
a &lt;code&gt;SecondOrderODEProblem&lt;/code&gt; implemented.) The Julia code reads:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using DiffEqFlux, DifferentialEquations, Flux, Optim, Plots, DiffEqSensitivity
using Zygote
using Random
u0 = Float32[1.; 0.]

tspan = (0.0f0,50.0f0)

dtsave = 0.5f0
t = tspan[1]:dtsave:tspan[2]

function oscillator!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  return nothing
end

# ODE without kicks
pl = plot(solve(prob_data,Tsit5(),saveat=t), label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/forward_damped_oscillator_no_kicks.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/forward_damped_oscillator_no_kicks.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We now include a kick to the velocity of the oscillator at regular time steps. Here, we choose both the time difference between the kicks and the increase in velocity as &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;kicktimes = tspan[1]:1:tspan[2]
function kick!(integrator)
  integrator.u[end] += one(eltype(integrator.u))
end
cb_ = PresetTimeCallback(kicktimes,kick!,save_positions=(false,false))

prob_data = ODEProblem(oscillator!,u0,tspan)
sol_data = solve(prob_data,Tsit5(),callback=cb_,saveat=t)
t_data = sol_data.t
ode_data = Array(sol_data)

# visualize data
pl1 = plot(t_data,ode_data[1,:],label=&amp;quot;data x(t)&amp;quot;)
plot!(pl1,t_data,ode_data[2,:],label=&amp;quot;data v(t)&amp;quot;)

pl2 = plot(t_data[1:20],ode_data[1,1:20],label=&amp;quot;data x(t)&amp;quot;)
plot!(pl2,t_data[1:20],ode_data[2,1:20],label=&amp;quot;data v(t)&amp;quot;)
pl = plot(pl2, pl1, layout=(1,2), xlabel=&amp;quot;t&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/forward_damped_oscillator.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/forward_damped_oscillator.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

The left-hand side shows a zoom for short times to resolve the kicks better. Note that by setting &lt;code&gt;save_positions=(true,true)&lt;/code&gt;, the kicks would be saved before &lt;strong&gt;and&lt;/strong&gt; after the event such that the kicks would appear completely vertically in the plot. The data on the right-hand will be used as training data below. In the spirit of universal differential equations&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;, we now aim at learning (potentially) missing parts of the model from these data traces.&lt;/p&gt;
&lt;h3 id=&#34;high-domain-knowledge&#34;&gt;High domain knowledge&lt;/h3&gt;
&lt;p&gt;For simplicity, we assume that we have almost perfect knowledge about our system. That is, we assume to know the basic structure of the ODE, including its parameters $a$ and $b$, and that the &lt;code&gt;affect!&lt;/code&gt; function of the event only acts on the velocity. We then encode the affect as an additional component to the ODE. The task is thus to learn the dynamics of the third component of &lt;code&gt;integrator.u&lt;/code&gt;. If we further set the inital value of that component to &lt;code&gt;1&lt;/code&gt;, then the neural network only has to learn that &lt;code&gt;du[3]&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;. In other words, the output of the neural network must be &lt;code&gt;0&lt;/code&gt; for all states &lt;code&gt;u&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Random.seed!(123)
nn1 = FastChain(FastDense(2, 64, tanh),FastDense(64, 1))
p_nn1 = initial_params(nn1)

function f1!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  du[3] = nn1(u[1:2], p)[1]
  return nothing
end

affect!(integrator) = integrator.u[2] += integrator.u[3]
cb = PresetTimeCallback(kicktimes,affect!,save_positions=(false,false))
z0 = Float32[u0;one(u0[1])]
prob1 = ODEProblem(f1!,z0,tspan,p_nn1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily compare the time evolution of the neural hybrid DE with respect to the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# to visualize the predictions of the trained neural network below
function visualize(prob,p)
  _prob = remake(prob,p=p)
  ode_pred = Array(solve(_prob,Tsit5(),callback=cb,
                 saveat=dtsave))[1:2,:]
  pl1 = plot(t_data,ode_pred[1,:],label=&amp;quot;x(t)&amp;quot;)
  scatter!(pl1,t_data[1:5:end],ode_data[1,1:5:end],label=&amp;quot;data x(t)&amp;quot;)
  pl2 = plot(t_data,ode_pred[2,:],label=&amp;quot;v(t)&amp;quot;)
  scatter!(pl2,t_data[1:5:end],ode_data[2,1:5:end],label=&amp;quot;data v(t)&amp;quot;)

  pl = plot(pl1, pl2, layout=(1,2), xlabel=&amp;quot;t&amp;quot;)
  return pl, sum(abs2,ode_data .- ode_pred)
end

pl = plot(solve(prob1,Tsit5(),saveat=t,
  callback=cb
  ),label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot; &amp;quot;u3(t)&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/untrained_nn.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/untrained_nn.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;which (of course) doesn&amp;rsquo;t match the data due to the random initialization of the neural network parameters before training. The neural network can be trained, i.e., its parameters can be optimized, by minimizing a mean-squared error loss function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;### loss function
function loss(p; prob=prob1, sensealg = ReverseDiffAdjoint())
  _prob = remake(prob,p=p)
  pred = Array(solve(_prob,Tsit5(),callback=cb,
               saveat=dtsave,sensealg=sensealg))[1:2,:]
  sum(abs2,ode_data .- pred)
end

loss(p_nn1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The recently implemented tools are deeply hidden within the 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiffEqSensitivity.jl&lt;/a&gt; package. However, while the user could only choose discrete sensitivies such as &lt;code&gt;ReverseDiffAdjoint()&lt;/code&gt; or  &lt;code&gt;ForwardDiffAdjoint()&lt;/code&gt; that rely on direct differentiation through the solver operations to get accurate gradients, one can now also select continuous adjoint sensitivy methods such as &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;,  &lt;code&gt;InterpolatingAdjoint()&lt;/code&gt;, and &lt;code&gt;QuadratureAdjoint()&lt;/code&gt; as the &lt;code&gt;sensealg&lt;/code&gt; for hybrid DEs. Each choice has its own characteristics in terms of stability, scaling with parameters, and memory consumption, see, e.g., 
&lt;a href=&#34;https://www.youtube.com/watch?v=XRJ-rtP2fVE&amp;amp;list=PLP8iPy9hna6TxktMt-IzdU2vQpGp3bwDn&amp;amp;index=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris&#39; talk&lt;/a&gt; at the SciML symposiam at SIAM CSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;###################################
# training loop
# optimize the parameters for a few epochs with ADAM
function train(prob, p_nn; sensealg=BacksolveAdjoint())
  opt = ADAM(0.0003f0)
  list_plots = []
  losses = []
  for epoch in 1:200
    println(&amp;quot;epoch: $epoch / 200&amp;quot;)
    _dy, back = Zygote.pullback(p -&amp;gt; loss(p,
      prob=prob,
      sensealg=sensealg), p_nn)
    gs = @time back(one(_dy))[1]
    push!(losses, _dy)
    if epoch % 10 == 0
      # plot every xth epoch
      pl, test_loss = visualize(prob, p_nn)
      println(&amp;quot;Loss (epoch: $epoch): $test_loss&amp;quot;)
      display(pl)
      push!(list_plots, pl)
    end
    Flux.Optimise.update!(opt, p_nn, gs)
    println(&amp;quot;&amp;quot;)
  end
  return losses, list_plots
end

# plot training loss
losses, list_plots = train(prob1, p_nn1)
pl1 = plot(losses, lw = 1.5, xlabel = &amp;quot;epoch&amp;quot;, ylabel=&amp;quot;loss&amp;quot;, legend=false)
pl2 = list_plots[end]
pl3 = plot(solve(prob1,p=p_nn1,Tsit5(),saveat=t,
   callback=cb
  ), label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot; &amp;quot;u3(t)&amp;quot;])

pl = plot(pl2,pl3)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/trained1.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/trained1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;reducing-the-domain-knowledge&#34;&gt;Reducing the domain knowledge&lt;/h2&gt;
&lt;p&gt;The less physical information is included in the model design, the more difficult the training becomes, e.g., due to 
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/local_minima/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;local minima&lt;/a&gt;. Possible modification for the kicked oscillator could be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;changing the initial condition of the third component of &lt;code&gt;u&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;using another affect function &lt;code&gt;affect!(integrator) = integrator.u[2] = integrator.u[3]&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;dropping the knowledge that only &lt;code&gt;u[2]&lt;/code&gt; gets a kick by using a neural network with &lt;code&gt;2&lt;/code&gt; outputs (+ a fourth component in the ODE):&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;affect2!(integrator) = integrator.u[1:2] = integrator.u[3:4]
function f2!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  du[3:4] .= nn2(u[1:2], nn_weights)
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fit the parameters additionally:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function f3!(du,u,p,t)
  a = p[end-1]
  b = p[end]
  nn_weights = p[1:end-2]

  du[1] = u[2]
  du[2] = -b*u[1] - a*u[2]
  du[3:4] .= nn2(u[1:2], nn_weights)
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;infer the underlying dynamics using a neural network with &lt;code&gt;4&lt;/code&gt; outputs:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function f4!(du,u,p,t)
  Ω = nn3(u[1:2], p)

  du[1] = Ω[1]
  du[2] = Ω[2]
  du[3:4] .= Ω[3:4]
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;With respect to the adjoint sensitivity methods for hybrid DEs, we are planning to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;refine the adjoints in case of implicit discontinuities (&lt;code&gt;ContinuousCallbacks&lt;/code&gt;) and&lt;/li&gt;
&lt;li&gt;support direct usage through the 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/types/jump_types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jump problem&lt;/a&gt; interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in the upcoming weeks. For further information, the interested reader might take a look at the open 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issues&lt;/a&gt; in the DiffEqSensitivity.jl package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Frank Schäfer, Pavel Sekatski, et al., Mach. Learn.: Sci. Technol. &lt;strong&gt;2&lt;/strong&gt;, 035004 (2021) &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel, arXiv preprint arXiv:2011.03902 (2020). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Junteng Jia, Austin R. Benson, arXiv preprint arXiv:1905.10403 (2019). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Michael Poli, Stefano Massaroli, et al., arXiv preprint arXiv:2106.04165 (2021). &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chris Rackauckas, Yingbo Ma, et al., &amp;ldquo;Accelerated predictive healthcare analytics with pumas, a high performance pharmaceutical modeling and simulation platform.&amp;rdquo; (2020). &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chris Rackauckas, Yingbo Ma, et al., arXiv preprint arXiv:2001.04385 (2020). &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>High weak order solvers and adjoint sensitivity analysis for stochastic differential equations</title>
      <link>https://frankschae.github.io/post/gsoc-2020/</link>
      <pubDate>Wed, 26 Aug 2020 22:59:49 +0200</pubDate>
      <guid>https://frankschae.github.io/post/gsoc-2020/</guid>
      <description>&lt;h2 id=&#34;project-summary&#34;&gt;Project summary&lt;/h2&gt;
&lt;p&gt;In this project, we have implemented new promising tools within the 
&lt;a href=&#34;https://sciml.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; organization which are relevant for tasks such as optimal control or parameter estimation for stochastic differential equations.
The high weak order solvers will allow for massive performance advantages for fitting expectations of equations.
Instead of automatic differentiation (AD) through the operations of an SDE solver, which scales poorly in memory, one can now use efficient stochastic adjoint sensitivity methods.&lt;/p&gt;
&lt;h2 id=&#34;blog-posts&#34;&gt;Blog posts&lt;/h2&gt;
&lt;p&gt;The following posts describe the work during the entire period in more detail:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://frankschae.github.io/post/gsoc2020-high-weak-order-solvers-sde-adjoints/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GSoC 2020: High weak order SDE solvers and their utility in neural SDEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://frankschae.github.io/post/high-weak/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;High weak order SDE solvers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adjoint sensitivity methods  &amp;ndash; To be uploaded soon.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;docs&#34;&gt;Docs&lt;/h2&gt;
&lt;p&gt;The documentation of the solvers is available 
&lt;a href=&#34;https://diffeq.sciml.ai/latest/solvers/sde_solve/#High-Weak-Order-Methods&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
Docs with respect to the adjoint sensitivity tools will be available 
&lt;a href=&#34;https://diffeq.sciml.ai/latest/analysis/sensitivity/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;achievements&#34;&gt;Achievements&lt;/h2&gt;
&lt;p&gt;Please find below a list of the PRs carried out during GSoC in the different repositories in chronological order within the SciML ecosystem.&lt;/p&gt;
&lt;h4 id=&#34;stochasticdiffeqjl&#34;&gt;StochasticDiffEq.jl&lt;/h4&gt;
&lt;p&gt;Merged:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/285&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inplace version of DRI1 scheme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/289&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RI1 method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/305&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tstop fixes for reverse time propagation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/317&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fixes for EulerHeun scheme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/327&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speed up the tests for the DRI1 and RI1 schemes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/328&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RI3, RI5, RI6, RDI2WM, RDI3WM, and RDI4WM schemes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/329&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RDI1WM scheme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/332&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive version of the stochastic Runge-Kutta schemes by embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/333&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RS1 and RS2 schemes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/334&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PL1WM scheme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/337&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NON scheme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/338&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Citations for weak methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stochastic improved and modified Euler methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/343&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COM scheme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computationally more efficient NON variant (NON2)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Open:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Static array tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/pull/347&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Levy area for non-commutative noise processes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;diffeqsensitivityjl&#34;&gt;DiffEqSensitivity.jl&lt;/h4&gt;
&lt;p&gt;Merged:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/235&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adjoint sensitivities for steady states&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Concrete_solve dispatch for steady state problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/242&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BacksolveAdjoint for SDEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/256&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPU savety for SDE BacksolveAdjoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/258&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tests for concrete solve with respect to SDEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/260&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alternative differentiation choices (vjps) for noise Jacobian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/265&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fixes and tests for inplace formulation of BacksolveAdjoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/268&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient SDE BacksolveAdjoint for scalar noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/275&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalization of the SDE Adjoint for non-diagonal noise processes and diagonal noise processes with mixing terms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/295&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InterpolatingAdjoint for SDEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/298&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Citations for backsolve, steadystate and interpolation adjoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Allow for more general noise processes: replace NoiseGrid by NoiseWrapper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/303&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Checkpointing fix for BacksolveAdjoint in case of ODEs and SDEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/305&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cheaper non-diagonal noise tests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Open:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/317&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Support adjoints for SDEs written in the Ito sense&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;diffeqnoiseprocessjl&#34;&gt;DiffEqNoiseProcess.jl&lt;/h4&gt;
&lt;p&gt;Merged:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqNoiseProcess.jl/pull/48&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-dimensional Brownian motion tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqNoiseProcess.jl/pull/49&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bug fix for inplace form of NoiseGrid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqNoiseProcess.jl/pull/51&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reversible NoiseWrapper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqNoiseProcess.jl/pull/53&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Relax the size constraints of the available noise processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqNoiseProcess.jl/pull/54&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalization of the real-valued white noise process function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqNoiseProcess.jl/pull/55&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fix of an extraction issue with NoiseGrid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqNoiseProcess.jl/pull/56&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Allow NoiseWrapper to start at user-specified time points for interpolating parts of a trajectory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqNoiseProcess.jl/pull/56&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extraction and endpoint fixes on NoiseWrapper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqNoiseProcess.jl/pull/62&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reversal of SDEs written in the Ito sense&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;diffeqgpujl&#34;&gt;DiffEqGPU.jl&lt;/h4&gt;
&lt;p&gt;Merged:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqGPU.jl/pull/59&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Memory efficient reduction function for ensemble problems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;modelingtoolkitjl&#34;&gt;ModelingToolkit.jl&lt;/h4&gt;
&lt;p&gt;Merged:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/317&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modelingtoolkitize for SDESystem with conversion function between Ito and Stratonovich sense&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;diffeqdevtoolsjl&#34;&gt;DiffEqDevTools.jl&lt;/h4&gt;
&lt;p&gt;Merged:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqDevTools.jl/pull/62&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NoiseWrapper alternative for analyticless convergence tests of SDE solvers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqDevTools.jl/pull/74&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;test_convergence() dispatch for ensemble simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqDevTools.jl/pull/75&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Work precision set for ensemble problems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;diffeqbasejl&#34;&gt;DiffEqBase.jl&lt;/h4&gt;
&lt;p&gt;Merged:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/SciML/DiffEqBase.jl/pull/503&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fix concrete_solve tests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future work&lt;/h2&gt;
&lt;p&gt;There is still a lot that we&amp;rsquo;d like to do, e.g.,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Writing up more docs and examples&lt;/li&gt;
&lt;li&gt;Implementing drift-implicit weak stochastic Runge-Kutta solvers&lt;/li&gt;
&lt;li&gt;Finishing the SDE adjoints for the Ito sense&lt;/li&gt;
&lt;li&gt;Implementing a virtual Brownian tree to store the noise processes in O(1) memory&lt;/li&gt;
&lt;li&gt;Setting up an OptimalControl library that allows for easy usage of the new tools within a symbolic interface&lt;/li&gt;
&lt;li&gt;Benchmarking of the new solvers and adjoints&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Contributions, suggestions &amp;amp; comments are always welcome! You might like to join our slac channels #diffeq-bridged and #neuralsde to get in touch.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;I would like to thank my mentors 
&lt;a href=&#34;https://github.com/ChrisRackauckas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris Rackauckas&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mschauer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moritz Schauer&lt;/a&gt;, and  
&lt;a href=&#34;https://github.com/YingboMa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yingbo Ma&lt;/a&gt; for their amazing support during this project. It was a great opportunity to work in such an inspiring collaboration and I highly appreciate their detailed feedback.
I would also like to thank 
&lt;a href=&#34;https://www.quantumtheory-bruder.physik.unibas.ch/people.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christoph Bruder&lt;/a&gt;, Niels Lörch, Martin Koppenhöfer, and Michal Kloc for helpful comments on my blog posts.
Many thanks to the very supportive julia community and to Google&amp;rsquo;s open source program for funding this experience!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>High weak order SDE solvers</title>
      <link>https://frankschae.github.io/post/high-weak/</link>
      <pubDate>Mon, 17 Aug 2020 14:46:35 +0200</pubDate>
      <guid>https://frankschae.github.io/post/high-weak/</guid>
      <description>&lt;p&gt;This post summarizes our new 
&lt;a href=&#34;https://diffeq.sciml.ai/dev/solvers/sde_solve/#High-Weak-Order-Methods-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;high weak order methods&lt;/a&gt;
for the 
&lt;a href=&#34;https://sciml.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; ecosystem, as implemented within the 
&lt;a href=&#34;https://summerofcode.withgoogle.com/organizations/6363760870031360/?sp-page=2#5505348691034112&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Summer of Code 2020&lt;/a&gt; project.&lt;/p&gt;
&lt;p&gt;After an introductory part highlighting the differences between the strong and the weak approximation for stochastic differential equations, we look into the convergence and performance properties of a few representative new methods in case of a non-commutative noise process.
Based on the stochastic version of the Brusselator equations, we demonstrate how adaptive step-size control for the weak solvers can result in a better approximation of the system dynamics.
Finally, we discuss how to run simulations on GPU hardware.&lt;/p&gt;
&lt;p&gt;Throughout this post, we shall use the vector notation $X(t)$ to denote the solution of the &lt;em&gt;d&lt;/em&gt;-dimensional Ito SDE
system&lt;/p&gt;
&lt;p&gt;$$
dX(t) = a(t,X(t)) dt + b(t,X(t)) dW
$$&lt;/p&gt;
&lt;p&gt;with an &lt;em&gt;m&lt;/em&gt;-dimensional driving Wiener process &lt;em&gt;W(t)&lt;/em&gt; in the time span $\mathbb{I}=[t_0, T]$, where $a: \mathbb{I}\times\mathbb{R}^d \rightarrow \mathbb{R}^d$
and $b: \mathbb{I}\times \mathbb{R}^{d} \rightarrow \mathbb{R}^{d \times m}$ are continuous functions which fulfill a global Lipschitz condition.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;
For simplicity, we write $X(t)$ for both time-discrete approximations and continuous-time random variables in the following.&lt;/p&gt;
&lt;h2 id=&#34;strong-convergence&#34;&gt;Strong convergence&lt;/h2&gt;
&lt;p&gt;Suppose that we encounter the following problem: Given &lt;strong&gt;noisy&lt;/strong&gt; observations $Z(t)$ (e.g., originating from measurement noise),
what is the best estimate $\hat{X}(t)$ of a stochastic system $X(t)$
satisfying the form above. Intuitively, we aim at filtering away the noise from the observations in an optimal way.
Such tasks are well known as filtering problems.&lt;/p&gt;
&lt;p&gt;To solve a filtering problem, we need a solver whose sample paths $Y(t)$ are close to the ones of the
stochastic process $X(t)$, i.e., the solver should allow us to reconstruct correctly the numerical solution of each single trajectory of
an SDE.&lt;/p&gt;
&lt;p&gt;Introducing the absolute error at the final time $T$ as
$$
\rm{E}(|X(T) -Y(T)|) \leq \sqrt{\rm{E}(|X(T)-Y(T)|^2)},
$$
we define convergence in the &lt;strong&gt;strong sense&lt;/strong&gt; with order $p$ of a time discrete approximation $Y(T)$ with step size $h$
to the solution $X(T)$ of a SDE at time $T$ if there exists a constant $C$ (independent of $h$)
and a constant $\delta &amp;gt; 0$ such that
$$
\rm{E}(|X(T) -Y(T)|) \leq C \cdot h^p,
$$
for each $h \in [0, \delta]$.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StochasticDiffEq&lt;/a&gt; package contains various state-of-the-art solvers
for the strong approximation of SDEs. In most cases, the strong solvers are however restricted to special noise forms.
For example, the very powerful stability-optimized, adaptive strong order 3/2 stochastic Runge-Kutta method (SOSRI)
can only handle diagonal and scalar noise Ito SDEs, i.e., noise processes where &lt;em&gt;b&lt;/em&gt; has only entries on its diagonal or $m=1$.
The main difficulty for the construction of strong methods with an order &amp;gt; 1/2 arises from the need of an accurate estimation of
multiple stochastic integrals. While the iterated stochastic integrals can be expressed in terms of &lt;em&gt;dW&lt;/em&gt; in the case
of scalar, diagonal, and commutative noise processes, an approximation based on a Fourier expansion of a Brownian bridge must be employed
in the case of non-commutative noise processes.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; Currently, we are also 
&lt;a href=&#34;https:://github.com/SciML/StochasticDiffEq.jl/pull/347&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implementing those iterated integrals in the StochasticDiffEq library&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;weak-convergence&#34;&gt;Weak convergence&lt;/h2&gt;
&lt;p&gt;Instead of an accurate pathwise approximation of a stochastic process, we only require an estimation for the &lt;strong&gt;expected value of the solution&lt;/strong&gt; in many situations. In these cases, methods for the &lt;strong&gt;weak approximation&lt;/strong&gt; are sufficient and &amp;ndash; due to the less restrictive formulation of the objective &amp;ndash; those solvers are computationally cheaper than their strong counterparts.
For example, weak solvers are very efficient for simulations in quantum optics, if only mean values of many trajectories are required, e.g., when the expectation values of variables such as position and momentum operators are computed in the phase space framework (Wigner functions, positive P-functions, etc.) of quantum mechanics.
Our new contributions are particularly appealing for many-body simulations, which are the computationally most demanding problems in quantum mechanics.&lt;/p&gt;
&lt;p&gt;We define convergence in the &lt;strong&gt;weak sense&lt;/strong&gt; with order $p$ of a time-discrete approximation $Y(T)$ with step size $h$
to the solution $X(T)$ of a SDE at time $T$ if there exists a constant $C$ (independent of $h$)
and a constant $\delta &amp;gt; 0$ such that
$$
|\rm{E}(g(X(T))) -\rm{E}(g(Y(T)))| \leq C \cdot h^p,
$$
$~$&lt;/p&gt;
&lt;p&gt;for any polynomial $g$ for each $h \in [0, \delta]$.&lt;/p&gt;
&lt;p&gt;We demonstrate below that &lt;strong&gt;high weak order solvers&lt;/strong&gt; are specifically appealing, as they allow for using much larger time steps while attaining the same error in the mean, as compared with SDE solvers possessing a smaller weak-order convergence.&lt;/p&gt;
&lt;h2 id=&#34;new-high-weak-order-methods&#34;&gt;New high weak order methods&lt;/h2&gt;
&lt;p&gt;A list of all new weak solvers is available in the 
&lt;a href=&#34;https://diffeq.sciml.ai/dev/solvers/sde_solve/#High-Weak-Order-Methods-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML documentation&lt;/a&gt;.
Note that we also implemented methods designed for the Stratonovich sense.
For the subsequent examples regarding Ito SDEs, we use only a subset of the plethora of second-order weak solvers.
We employ the &lt;code&gt;DRI1()&lt;/code&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, &lt;code&gt;RD1WM()&lt;/code&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, and &lt;code&gt;RD2WM()&lt;/code&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; methods due to Debrabant &amp;amp; Rößler and Platen&amp;rsquo;s &lt;code&gt;PL1WM()&lt;/code&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; method.
We compare those methods to the strong Euler-Maruyama &lt;code&gt;EM()&lt;/code&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and the simplified Euler-Maruyama &lt;code&gt;SimplifiedEM()&lt;/code&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; schemes.
The latter is the simplest weak solver, where the Gaussian increments of the strong Euler-Maruyama scheme are replaced by
two-point distributed random variables with similar moment properties.&lt;/p&gt;
&lt;p&gt;Rößler&amp;rsquo;s SRK schemes are particularly designed to scale well with the number of Wiener processes &lt;code&gt;m&lt;/code&gt;, since only &lt;code&gt;2m-1&lt;/code&gt; random variables have to be drawn and since the number of function evaluations for the drift and the diffusion terms is independent of &lt;code&gt;m&lt;/code&gt;.
&lt;code&gt;PL1WM()&lt;/code&gt; in contrast needs to simulate &lt;code&gt;m(m+1)/2&lt;/code&gt; random variables but a smaller number of order conditions needs to be fulfilled.&lt;/p&gt;
&lt;h3 id=&#34;convergence-tests&#34;&gt;Convergence tests&lt;/h3&gt;
&lt;p&gt;As in the 
&lt;a href=&#34;https://frankschae.github.io/post/gsoc2020-high-weak-order-solvers-sde-adjoints/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first blog post&lt;/a&gt;, let us consider the multi-dimensional SDE with non-commuting noise&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;p&gt;$$
\scriptstyle d \begin{pmatrix} X_1 \\  X_2 \end{pmatrix} = \begin{pmatrix} -\frac{273}{512} &amp;amp;  \phantom{X_2}0 \\  -\frac{1}{160} \phantom{X_2}  &amp;amp; -\frac{785}{512}+\frac{\sqrt{2}}{8} \end{pmatrix}  \begin{pmatrix} X_1 \\  X_2 \end{pmatrix} dt + \begin{pmatrix} \frac{1}{4} X_1 &amp;amp;  \frac{1}{16} X_1 \\  \frac{1-2\sqrt{2}}{4} X_2 &amp;amp; \frac{1}{10}X_1  +\frac{1}{16} X_2 \end{pmatrix} d \begin{pmatrix} W_1 \\  W_2 \end{pmatrix} &lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;with initial value
$$ ~$$&lt;/p&gt;
&lt;p&gt;$$ X(t=0)=  \begin{pmatrix} 1 \\ 1\end{pmatrix},$$&lt;/p&gt;
&lt;p&gt;where the expected value of the solution can be computed analytically&lt;/p&gt;
&lt;p&gt;$$ \rm{E}\left[ f(X(t)) \right] =  \exp(-t),$$&lt;/p&gt;
&lt;p&gt;for the function $f(x)=(x_1)^2$, which we use to test the weak convergence order of the algorithms in the following.&lt;/p&gt;
&lt;p&gt;To compute the expected value numerically, we sample an ensemble of &lt;code&gt;numtraj = 1e6&lt;/code&gt; trajectories for different step sizes &lt;code&gt;dt&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using StochasticDiffEq
using Test
using Random
using Plots
using DiffEqDevTools

function prob_func(prob, i, repeat)
    remake(prob,seed=seeds[i])
end

u₀ = [1.0,1.0]
function f1!(du,u,p,t)
  @inbounds begin
    du[1] = -273//512*u[1]
    du[2] = -1//160*u[1]-(-785//512+sqrt(2)/8)*u[2]
  end
  return nothing
end
function g1!(du,u,p,t)
  @inbounds begin
    du[1,1] = 1//4*u[1]
    du[1,2] = 1//16*u[1]
    du[2,1] = (1-2*sqrt(2))/4*u[1]
    du[2,2] = 1//10*u[1]+1//16*u[2]
  end
  return nothing
end
dts = 1 .//2 .^(3: -1:0)
tspan = (0.0,3.0)

h2(z) = z^2 # but apply it only to u[1]

prob = SDEProblem(f1!,g1!,u₀,tspan,noise_rate_prototype=zeros(2,2))

numtraj = Int(1e6)
seed = 100
Random.seed!(seed)
seeds = rand(UInt, numtraj)
ensemble_prob = EnsembleProblem(prob;
        output_func = (sol,i) -&amp;gt; (h2(sol[end][1]),false),
        prob_func = prob_func
        )

sim = test_convergence(dts,ensemble_prob,DRI1(),
  save_everystep=false,trajectories=numtraj,save_start=false,adaptive=false,
  weak_timeseries_errors=false,weak_dense_errors=false,
  expected_value=exp(-3.0)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The object &lt;code&gt;sim&lt;/code&gt; defined in the last line contains all relevant quantities to test the weak convergence with respect to the final time point for the &lt;code&gt;DRI1()&lt;/code&gt; scheme.
Repeating this call to the &lt;code&gt;test_convergence()&lt;/code&gt; function for the other aforementioned solvers, we obtain the convergence plot:&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/weak_conv.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/weak_conv.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Note that the &lt;code&gt;SimplifiedEM&lt;/code&gt; and the &lt;code&gt;EM&lt;/code&gt; scheme fall on top of each other.
&lt;code&gt;DRI1()&lt;/code&gt; achieves the smallest errors for a fixed &lt;code&gt;dt&lt;/code&gt; in this study.&lt;/p&gt;
&lt;h3 id=&#34;work-precision-diagrams&#34;&gt;Work-Precision Diagrams&lt;/h3&gt;
&lt;p&gt;Ultimately, we are not only interested in the general convergence slope of an algorithm but also in its speed.
We&amp;rsquo;d like to select the fastest method depending on the permitted tolerance.
This is commonly studied using work-precision diagram.
Thanks to 
&lt;a href=&#34;https://github.com/SciML/DiffEqDevTools.jl/pull/75&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;new routines&lt;/a&gt;, a user can generate a work-precision diagram by the following code&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;reltols = 1.0 ./ 4.0 .^ (1:4)
abstols = reltols#[0.0 for i in eachindex(reltols)]
setups = [
          Dict(:alg=&amp;gt;DRI1(),:dts=&amp;gt;dts,:adaptive=&amp;gt;false),
          Dict(:alg=&amp;gt;PL1WM(),:dts=&amp;gt;dts,:adaptive=&amp;gt;false),
          Dict(:alg=&amp;gt;EM(),:dts=&amp;gt;dts,:adaptive=&amp;gt;false),
          Dict(:alg=&amp;gt;SimplifiedEM(),:dts=&amp;gt;dts,:adaptive=&amp;gt;false),
          Dict(:alg=&amp;gt;RDI2WM(),:dts=&amp;gt;dts,:adaptive=&amp;gt;false),
          Dict(:alg=&amp;gt;RDI1WM(),:dts=&amp;gt;dts,:adaptive=&amp;gt;false)
          ]
test_dt = 1//10000
appxsol_setup = Dict(:alg=&amp;gt;EM(), :dt=&amp;gt;test_dt)
wp = @time WorkPrecisionSet(ensemble_prob,
                            abstols,reltols,setups,test_dt;
                            maxiters = 1e7,verbose=false,
                            save_everystep=false,save_start=false,
                            appxsol_setup = appxsol_setup,
                            expected_value=exp(-3.0),
                            trajectories=numtraj, error_estimate=:weak_final)
plt = plot(wp;legend=:bottomleft)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/WorkPrecision.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/WorkPrecision.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Therefore, &lt;code&gt;DRI1&lt;/code&gt; has the best performance in this non-commutative noise case if the error is supposed to stay below 1e-3.
For larger permitted errors, the &lt;code&gt;SimplifiedEM&lt;/code&gt; scheme might be a good choice. However, the first order methods
are outclassed when high precision is more of a concern.
We plan to perform more in-depth benchmarks in the near future what will be reported on the 
&lt;a href=&#34;https://sciml.ai/news/2020/08/10/StochasticBonanza/#tons_of_methods_for_high_weak_order_solving_of_sdes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML news&lt;/a&gt;. Stay tuned!&lt;/p&gt;
&lt;h3 id=&#34;adaptive-step-size-control&#34;&gt;Adaptive step-size control&lt;/h3&gt;
&lt;p&gt;Already in 2004, Rößler proposed an adaptive discretization algorithm for the weak approximation of SDEs.&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; The idea is to employ an embedded SRK scheme: Using the same function evaluations but distinct Butcher tableaus, one constructs two stochastic Runge-Kutta methods with different convergence order, such that the local error can be estimated with only small additional computational overhead. Based on the error estimate, new step sizes are proposed.&lt;/p&gt;
&lt;p&gt;To use adaptive step-size control, it is sufficient to set &lt;code&gt;adaptive=true&lt;/code&gt; (default setting). Optionally, one may also pass absolute and relative tolerances.&lt;/p&gt;
&lt;p&gt;The following julia code simulates the stochastic version of the Brusselator equations with intitial condition&lt;/p&gt;
&lt;p&gt;$$ X(t=0)=  \begin{pmatrix} 0.1 \\ 0\end{pmatrix},$$&lt;/p&gt;
&lt;p&gt;on a time span $\mathbb{I}=[0, 100]$ for adaptive (&lt;code&gt;sol&lt;/code&gt;) and fixed step sizes (&lt;code&gt;sol_na&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using StochasticDiffEq, DiffEqNoiseProcess, Random
using Plots
using DiffEqGPU

function prob_func(prob, i, repeat)
  Random.seed!(seeds[i])
  W = WienerProcess(0.0,0.0,0.0)
  remake(prob,noise=W)
end

function brusselator_f!(du,u,p,t)
  @inbounds begin
    du[1] = (p[1]-1)*u[1]+p[1]*u[1]^2+(u[1]+1)^2*u[2]
    du[2] = -p[1]*u[1]-p[1]*u[1]^2-(u[1]+1)^2*u[2]
  end
  nothing
end

function scalar_noise!(du,u,p,t)
  @inbounds begin
    du[1] = p[2]*u[1]*(1+u[1])
    du[2] = -p[2]*u[1]*(1+u[1])
   end
   nothing
end


# fix seeds
seed = 100
Random.seed!(seed)
numtraj= 100
seeds = rand(UInt, numtraj)
W = WienerProcess(0.0,0.0,0.0)


u0 = [-0.1f0,0.0f0]
tspan = (0.0f0,100.0f0)
p = [1.9f0,0.1f0]

prob = SDEProblem(brusselator_f!,scalar_noise!,u0,tspan,p,noise=W)

ensembleprob = EnsembleProblem(prob, prob_func = prob_func)

sol = @time solve(ensembleprob,DRI1(),dt=0.1,EnsembleCPUArray(),trajectories=numtraj)
sol_na = @time solve(ensembleprob,DRI1(),dt=0.8,adaptive=false,EnsembleCPUArray(),trajectories=numtraj)


summ = EnsembleSummary(sol,0.0f0:0.5f0:100f0)
pl = plot(summ,fillalpha=0.5,xlabel = &amp;quot;time t&amp;quot;, yaxis=&amp;quot;X(t)&amp;quot;, label= [&amp;quot;x₁(t)&amp;quot; &amp;quot;x₂(t)&amp;quot;], legend=true)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The time evolution of both dependent variables ($x_1(t)$ and $x_2(t)$) displays damped oscillations.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/Brusselator_many_trajectories.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/Brusselator_many_trajectories.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We can confirm Rößler&amp;rsquo;s observation&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; that the adaptive scheme describes the time evolution of the SDE more accurately,
as oscillations are damped out stronger for the fixed step size method, thus approaching the origin too rapidly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using DifferentialEquations.EnsembleAnalysis
meansol = timeseries_steps_mean(sol)
meansol_na = timeseries_point_mean(sol_na,meansol.t)
dts = []
tmp1 = tspan[1]
for tmp2 in meansol.t
  global tmp1
  push!(dts,tmp2-tmp1)
  tmp1 = tmp2
end
#

list_plots = []
for i in 1:length(meansol.u)
  l = @layout [a  b]
  plt1 = plot(meansol[1, 1:i],meansol[2, 1:i],
         ylim = (-0.18, 0.18),
         xlim = (-0.13, 0.13),
         xlabel = &amp;quot;x₁(t)&amp;quot;,
         yaxis= &amp;quot;x₂(t)&amp;quot;,
         label=&amp;quot;adaptive&amp;quot;,
         lw=2,
         linecolor=1
         )
  plot!(meansol_na[1, 1:i],meansol_na[2, 1:i],
         ylim = (-0.18, 0.18),
         xlim = (-0.13, 0.13),
         xlabel = &amp;quot;x₁(t)&amp;quot;,
         yaxis= &amp;quot;x₂(t)&amp;quot;,
         label=&amp;quot;fixed step size&amp;quot;,
         lw=2,
         linecolor=2
         )
  pl2 = scatter(dts[1:i], xlabel = &amp;quot;step&amp;quot;, yaxis= &amp;quot;dtᵢ&amp;quot;, xlim = (0, length(meansol.u)),  ylim = (0.0, 2.3), legend=false)
  plt = plot(plt1, pl2, layout = l)
  push!(list_plots, plt)
end

anim = animate(list_plots,lw=2,every=1)

&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/Brusselator.gif&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/Brusselator.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;gpu-usage&#34;&gt;GPU usage&lt;/h3&gt;
&lt;p&gt;All necessary tools to accelerate the simulation of (stochastic) differential equations on GPUs within the SciML
ecosystem are collected in the 
&lt;a href=&#34;https://github.com/SciML/DiffEqGPU.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiffEqGPU&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Currently, bounds checking and return values are not allowed, i.e., functions must be
written in the form:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function f!(du,u,p,t)
  @inbounds begin
    du[1] = ..
  end
  nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Except from those limitations, a user can specifiy &lt;code&gt;ensemblealg=EnsembleGPUArray()&lt;/code&gt; to parallelize SDE solves across the GPU, see, e.g., 
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/blob/master/test/gpu/sde_weak_adaptive.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the GPU tests for StochasticDiffEq&lt;/a&gt; for some examples.
Note that for some high weak order solvers GPU usage is not recommended as scalar indexing is used.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Peter E. Kloeden and Eckhard Platen, Numerical solution of stochastic differential equations. &lt;strong&gt;23&lt;/strong&gt;, Springer Science &amp;amp; Business Media (2013). &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Peter E. Kloeden, Eckhard Platen, and Ian W. Wright, Stochastic analysis and applications &lt;strong&gt;10&lt;/strong&gt; 431-441 (1992). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Kristian Debrabant, Andreas Rößler, Applied Numerical Mathematics &lt;strong&gt;59&lt;/strong&gt;, 582–594 (2009). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Kristian Debrabant, Andreas Rößler, Mathematics and Computers in Simulation &lt;strong&gt;77&lt;/strong&gt;, 408-420 (2008) &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>GSoC 2020: High weak order SDE solvers and their utility in neural SDEs</title>
      <link>https://frankschae.github.io/post/gsoc2020-high-weak-order-solvers-sde-adjoints/</link>
      <pubDate>Sat, 30 May 2020 15:10:33 +0200</pubDate>
      <guid>https://frankschae.github.io/post/gsoc2020-high-weak-order-solvers-sde-adjoints/</guid>
      <description>&lt;p&gt;First and foremost, I would like to thank my mentors 
&lt;a href=&#34;https://github.com/ChrisRackauckas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris Rackauckas&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mschauer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moritz Schauer&lt;/a&gt;, and  
&lt;a href=&#34;https://github.com/YingboMa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yingbo Ma&lt;/a&gt; for their willingness to supervise me in this Google Summer of Code project.
Although we are still at the very beginning of the project, we already had plenty of very inspiring discussion. I will spend the following months implementing both  &lt;strong&gt;new high weak order solvers&lt;/strong&gt; as well as &lt;strong&gt;adjoint sensitivity methods&lt;/strong&gt; for stochastic differential equations (SDEs).
The project is embedded within the 
&lt;a href=&#34;https://sciml.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; organization which, among others, unifies the latest toolsets from scientific machine learning and differential equation solver software.
Ultimately, the planned contributions will allow researchers to simulate (or even 
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/LV-stochastic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;control&lt;/a&gt;) stochastic dynamics. Also inverse problems, where 
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/NN-SDE/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SDE models are fit to data&lt;/a&gt;, fall into the scope.
Therefore, relevant applications are found in many fields ranging from the simulation of (bio-)chemical processes over financial modeling to quantum mechanics.&lt;/p&gt;
&lt;p&gt;This post is supposed to summarize what we have implemented in this first period and what we are going to do next. Future posts are going to dig into the individual subjects in more details.&lt;/p&gt;
&lt;h2 id=&#34;high-weak-order-solvers&#34;&gt;High Weak Order Solvers&lt;/h2&gt;
&lt;p&gt;Currently, the 
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StochasticDiffEq&lt;/a&gt; package contains state-of-the-art solvers for the strong approximation of SDEs, i.e., solvers that allow one to reconstruct correctly the numerical solution of an SDE in a pathwise sense.
In general, an accurate estimation of multiple stochastic integrals is then required to produce a strong method of order greater than 1/2.&lt;/p&gt;
&lt;p&gt;However in many situations, we are only aiming for computing an estimation for the &lt;strong&gt;expected value of the solution&lt;/strong&gt;.
In such situations, methods for the &lt;strong&gt;weak approximation&lt;/strong&gt; are sufficient. The less restrictive formulation of the objective for weak methods has the advantage that they are computationally cheaper than strong methods.
&lt;strong&gt;High weak order solvers&lt;/strong&gt; are particularly appealing, as they allow for using much larger time steps while attaining the same error in the mean, as compared with SDE solvers having a smaller weak order convergence.
As an example, when Monte Carlo methods are used for SDE models, it is indeed often sufficient to be able to accurately sample random trajectories of the SDE, and it is not important to accurately approximate a particular trajectory. The former is exactly what a solver with high weak order provides.&lt;/p&gt;
&lt;h3 id=&#34;second-order-runge-kutta-methods-for-ito-sdes&#34;&gt;Second order Runge-Kutta methods for Ito SDEs&lt;/h3&gt;
&lt;p&gt;In the beginning of the community bonding period I finished the implementations of the &lt;code&gt;DRI1()&lt;/code&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;code&gt;RI1()&lt;/code&gt;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; methods. Both are representing second order Runge-Kutta schemes and were introduced by Rößler. Interestingly, these methods are designed to scale well with the number of Wiener processes &lt;code&gt;m&lt;/code&gt;. Specifically, only &lt;code&gt;2m-1&lt;/code&gt; random variables have to be drawn (in contrast to &lt;code&gt;m(m+1)/2&lt;/code&gt; from previous methods). Additionally, the number of function evaluations for the drift and the diffusion terms is independent of &lt;code&gt;m&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As an example, we can check the second order convergence property on a multi-dimensional SDE with non-commuting noise&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;p&gt;$$
\scriptstyle d \begin{pmatrix} X_1 \\  X_2 \end{pmatrix} = \begin{pmatrix} -\frac{273}{512} &amp;amp;  \phantom{X_2}0 \\  -\frac{1}{160} \phantom{X_2}  &amp;amp; -\frac{785}{512}+\frac{\sqrt{2}}{8} \end{pmatrix}  \begin{pmatrix} X_1 \\  X_2 \end{pmatrix} dt + \begin{pmatrix} \frac{1}{4} X_1 &amp;amp;  \frac{1}{16} X_1 \\  \frac{1-2\sqrt{2}}{4} X_2 &amp;amp; \frac{1}{10}X_1  +\frac{1}{16} X_2 \end{pmatrix} d \begin{pmatrix} W_1 \\  W_2 \end{pmatrix} &lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;with initial value $$ X(t=0)=  \begin{pmatrix} 1 \\ 1\end{pmatrix}.$$&lt;/p&gt;
&lt;p&gt;For the function $f(x)=(x_1)^2$, we can analytically compute the expected value of the solution&lt;/p&gt;
&lt;p&gt;$$ \rm{E}\left[ f(X(t)) \right] =  \exp(-t),$$&lt;/p&gt;
&lt;p&gt;which we use to test the weak convergence order of the algorithms in the following.&lt;/p&gt;
&lt;p&gt;To compute the expected value numerically, we sample an ensemble of &lt;code&gt;numtraj = 1e7&lt;/code&gt; trajectories for different step sizes &lt;code&gt;dt&lt;/code&gt;. The code for a single  &lt;code&gt;dt&lt;/code&gt; reads:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using StochasticDiffEq
numtraj = 1e7
u₀ = [1.0,1.0]
function f!(du,u,p,t)
  du[1] = -273//512*u[1]
  du[2] = -1//160*u[1]-(-785//512+sqrt(2)/8)*u[2]
end
function g!(du,u,p,t)
  du[1,1] = 1//4*u[1]
  du[1,2] = 1//16*u[1]
  du[2,1] = (1-2*sqrt(2))/4*u[1]
  du[2,2] = 1//10*u[1]+1//16*u[2]
end
dt = 1//8
tspan = (0.0,10.0)
prob = SDEProblem(f!,g!,u₀,tspan,noise_rate_prototype=zeros(2,2))

h(z) = z^2

ensemble_prob = EnsembleProblem(prob;
        output_func = (sol,i) -&amp;gt; (h(sol[end][1]),false)
        )
sol = solve(ensemble_prob, DRI1();
        dt=dt,
        save_start=false,
        save_everystep=false,
        weak_timeseries_errors=false,
        weak_dense_errors=false,
        trajectories=numtraj)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then compute the error of the numerically obtained expected value of the ensemble simulation with respect to the analytical result:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LinearAlgebra.norm(Statistics.mean(sol.u)-exp(-tspan[2]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Repeating this procedure for some more values of &lt;code&gt;dt&lt;/code&gt;, the log-log plot of the error as a function of &lt;code&gt;dt&lt;/code&gt; displays nicely the second order convergence (slope $\approx 2.2$).&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/DRI1.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/DRI1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;In the next couple of weeks, my focus will be on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;adding other high weak order solvers,&lt;/li&gt;
&lt;li&gt;implementing adaptive time stepping.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More of our near-term goals are collected in this 
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/issues/182&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;adjoint-sensitivity-methods-for-sdes&#34;&gt;Adjoint Sensitivity Methods for SDEs&lt;/h2&gt;
&lt;p&gt;In 
&lt;a href=&#34;https://mitmath.github.io/18337/lecture10/estimation_identification&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parameter estimation/inverse problems&lt;/a&gt;, one is interested to know the optimal choice of parameters &lt;code&gt;p&lt;/code&gt; such that a model &lt;code&gt;f(p)&lt;/code&gt;, e.g., a differential equation, optimally fits some data, y. The shooting method approaches this task by introducing some sort of loss function $L$. A common choice is the mean squared error&lt;/p&gt;
&lt;p&gt;$$
L = |f(p)-y|^2.
$$&lt;/p&gt;
&lt;p&gt;An optimizer is then used to update the parameters $p$ such that $L$ is minimized. For this fit, local optimizers use the gradient $\frac{dL}{dp}$ to minimize the loss function and ultimately solve the inverse problem.
One possibility to obtain the gradient information for (stochastic) differential equations is to use automatic differentiation (AD).
While forward mode AD is memory efficient, it scales poorly in time with increasing number of parameters. On the contrary, reverse-mode AD, i.e., a direct backpropagation through the solver, has a huge memory footprint.&lt;/p&gt;
&lt;p&gt;Alternatively to the &amp;ldquo;direct&amp;rdquo; AD approaches, the &lt;strong&gt;adjoint sensitivity method&lt;/strong&gt; can be used&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. The adjoint sensitivity method is well known to compute gradients of solutions to ordinary differential equations (ODEs) with respect to the parameters and initial states entering the ODE. The method was recently generalized to SDEs&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.
Importantly, this new approach has different complexities in terms of memory consumption or computation time as compared with forward- or reverse-mode AD (NP vs N+P where N is the number of state variables and P is the number of parameters).&lt;/p&gt;
&lt;p&gt;It turns out that the aforementioned gradients in the stochastic adjoint sensitivity method are given by solving an SDE with an &lt;strong&gt;augmented state backwards in time&lt;/strong&gt; launched at the end state of the forward evolution.  In other words, we first compute the forward time evolution of the model from the start time $t_0$ to the end time $t_1$. Subsequently, we reverse the SDE and run a second time evolution from $t_1$ to $t_0$. Please note that the authors in Ref. &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; are implementing a slightly modfified version where the time evolution of the augmented state runs from $-t_1$ to $-t_0$. We however are indeed using the former variant as it allows us to reuse/generalize many functions that were implemented in the 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiffEqSensitivity&lt;/a&gt; package for ODE adjoints earlier.&lt;/p&gt;
&lt;h3 id=&#34;reverse-sde-time-evolution&#34;&gt;Reverse SDE time evolution&lt;/h3&gt;
&lt;p&gt;The reversion of an SDE is more difficult than the reversion of an ODE. However, for SDEs written in the Stratonovich sense, it turns out that reversion can be achieved by negative signs in front of the drift and diffusion terms.
As one needs to follow the same trajectory backward, the noise sampled in the forward pass must be reconstructed.
In general, we would like to use adaptive time-stepping solvers which require some form of interpolation for the noise values.
After some fixes for the 
&lt;a href=&#34;https://docs.sciml.ai/latest/features/noise_process/#Adaptive-NoiseWrapper-Example-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available noise processes&lt;/a&gt;, we are now able to reverse a stochastic time evolution either by using &lt;code&gt;NoiseGrid&lt;/code&gt; which linearly interpolates between values of the noise on a given grid or by using a very general &lt;code&gt;NoiseWrapper&lt;/code&gt; which interpolates in a distributionally-exact manner based on Brownian bridges.&lt;/p&gt;
&lt;p&gt;As an example, the code below computes first the forward evolution of an SDE&lt;/p&gt;
&lt;p&gt;$$ dX  =  \alpha X dt + \beta X dW$$&lt;/p&gt;
&lt;p&gt;with $\alpha=1.01$, $\beta=0.87$, $x(0)=1/2$, in the time span ($t_{0}=0$, $t_{1}=1)$. This forward evolution is shown in blue in the animation below. Subsequently, also the reverse time evolution (red) launched at time $t_{1}=1$ with initial value $x(t=1)$, propagated in negative time direction until $t_{0}=0$, is computed. We see that both trajectories match very well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;  using StochasticDiffEq, DiffEqNoiseProcess

  α=1.01
  β=0.87

  dt = 1e-3
  tspan = (0.0,1.0)
  u₀=1/2

  tarray =  collect(tspan[1]:dt:tspan[2])

  f!(du,u,p,t) = du .= α*u
  g!(du,u,p,t) = du .= β*u


  prob = SDEProblem(f!,g!,[u₀],tspan)
  sol =solve(prob,EulerHeun(),dt=dt,save_noise=true, adaptive=false)

  _sol = deepcopy(sol) # to make sure the plot is correct
  W1 = NoiseGrid(reverse!(_sol.t),reverse!(_sol.W.W))
  prob1 = SDEProblem(f!,g!,sol[end],reverse(tspan),noise=W1)
  sol1 = solve(prob1,EulerHeun(),dt=dt)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/animation.gif&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/animation.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;gradients-of-diagonal-sdes&#34;&gt;Gradients of diagonal SDEs&lt;/h3&gt;
&lt;p&gt;I have already started to implement the stochastic adjoint sensitivity method for SDEs possessing diagonal noise. Currently, only out-of-place SDE functions are supported but I am optimistic that soon also the inplace formulation works.&lt;/p&gt;
&lt;p&gt;Let us consider again the linear SDE with multiplicative noise from above (with the same parameters). This SDE represents one of the few exact solvable cases. In the Stratonovich sense, the solution is given as&lt;/p&gt;
&lt;p&gt;$$ X(t) =  X(0) \exp(\alpha t + \beta W(t)).$$&lt;/p&gt;
&lt;p&gt;We might be interested in optimizing the parameters $\alpha$ and $\beta$ to minimize a certain loss function acting on the solution $X(t)$. For such an optimization task, a useful search direction is indicated by the gradient of the loss function with respect to the parameters. The latter however requires the differentiation through the SDE solver &amp;ndash; if no analytical solution of the SDE is available.&lt;/p&gt;
&lt;p&gt;As an example, let us consider a mean squared error loss&lt;/p&gt;
&lt;p&gt;$$
L(X(t)) = \sum_i |X(t_i)|^2,
$$&lt;/p&gt;
&lt;p&gt;acting on the solution $X(t)$ for some fixed time points $t_i$. Then, the analytical forms for the gradients here read&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{d L}{d \alpha} &amp;amp;= 2 \sum_i t_i |X(t_i)|^2 \\&lt;br&gt;
\frac{d L}{d \beta}  &amp;amp;= 2 \sum_i W(t_i) |X(t_i)|^2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;for $\alpha$ and $\beta$, respectively. We can confirm that this agrees with the gradients as obtained by the stochastic adjoint sensitivity method&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Test, LinearAlgebra
using DiffEqSensitivity, StochasticDiffEq
using Random

seed = 100
Random.seed!(seed)

u₀ = [0.5]
tstart = 0.0
tend = 0.1
dt = 0.005
trange = (tstart, tend)
t = tstart:dt:tend
tarray = collect(t)

function g(u,p,t)
  sum(u.^2.0)
end

function dg!(out,u,p,t,i)
  (out.=-2.0*u)
end

p2 = [1.01,0.87]

f(u,p,t) = p[1]*u
σ(u,p,t) = p[2]*u


Random.seed!(seed)
prob = SDEProblem(f,σ,u₀,trange,p2)
sol = solve(prob,RKMil(interpretation=:Stratonovich),dt=tend/1e7,adaptive=false,save_noise=true)
res_u0, res_p = adjoint_sensitivities(sol,EulerHeun(),dg!,t,dt=tend/1e7,sensealg=BacksolveAdjoint())


noise = vec((@. sol.W(tarray)))
Wextracted = [W[1][1] for W in noise]
resp1 = 2*sum(@. tarray*u₀^2*exp(2*(p2[1])*tarray+2*p2[2]*Wextracted))
resp2 = 2*sum(@. Wextracted*u₀^2*exp(2*(p2[1])*tarray+2*p2[2]*Wextracted))
resp = [resp1, resp2]

@test isapprox(res_p&#39;, resp, rtol = 1e-6)
# True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With respect to the adjoint sensitivity methods, we are planning to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;finish the current backsolve adjoint version,&lt;/li&gt;
&lt;li&gt;allow for computing the gradients of non-commuting SDEs,&lt;/li&gt;
&lt;li&gt;implement also an interpolation adjoint version,&lt;/li&gt;
&lt;li&gt;benchmark it with respect to AD approaches&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in the upcoming weeks. For more information, the interested reader might take a look at the open 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issues&lt;/a&gt; in the DiffEqSensitivity package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Kristian Debrabant, Andreas Rößler, Applied Numerical Mathematics &lt;strong&gt;59&lt;/strong&gt;, 582–594 (2009). &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Andreas Rößler, Journal on Numerical Analysis &lt;strong&gt;47&lt;/strong&gt;, 1713–1738 (2009). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Steven G. Johnson, &amp;ldquo;Notes on Adjoint Methods for 18.335.&amp;rdquo; Introduction to Numerical Methods (2012). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud, arXiv preprint arXiv:2001.01328 (2020). &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
