<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hybrid differential equations | FS</title>
    <link>https://frankschae.github.io/tag/hybrid-differential-equations/</link>
      <atom:link href="https://frankschae.github.io/tag/hybrid-differential-equations/index.xml" rel="self" type="application/rss+xml" />
    <description>Hybrid differential equations</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Frank Schäfer</copyright><lastBuildDate>Fri, 16 Jul 2021 13:24:04 +0200</lastBuildDate>
    <image>
      <url>https://frankschae.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Hybrid differential equations</title>
      <link>https://frankschae.github.io/tag/hybrid-differential-equations/</link>
    </image>
    
    <item>
      <title>Sensitivity Analysis of Hybrid Differential Equations</title>
      <link>https://frankschae.github.io/post/bouncing_ball/</link>
      <pubDate>Fri, 16 Jul 2021 13:24:04 +0200</pubDate>
      <guid>https://frankschae.github.io/post/bouncing_ball/</guid>
      <description>&lt;p&gt;In this post, we discuss sensitivity analysis of differential equations with state changes caused by events triggered at defined moments, for example reflections, bounces off a wall or other sudden forces. These are described by hybrid differential equations&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. We highlight differences between explicit&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and implicit events&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. As a paradigmatic example, we consider a bouncing ball described by the ODE&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\text{d}z(t) &amp;amp;= v(t) \text{d}t, \\&lt;br&gt;
\text{d}v(t) &amp;amp;= -\mathrm g\thinspace  \text{d}t
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;with initial condition&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z(t=0) &amp;amp;= z_0 = 5, \\&lt;br&gt;
v(t=0) &amp;amp;= v_0 = -0.1.
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The initial condition contains the initial height $z_0$ and initial velocity $v_0$ of the ball. We have two important parameters in this system. First, there is the gravitational constant $\mathrm g=10$ modeling the acceleration of the ball due to an approximately constant gravitational field.&lt;/p&gt;
&lt;p&gt;Second, we model the ground as barrier at $z = 0$ where the ball bounces off in opposite direction. We include a dissipation factor $\gamma=0.8$ (
&lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_restitution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coefficient of restitution&lt;/a&gt;) that accounts for a imperfect elastic bounce on the ground.&lt;/p&gt;
&lt;p&gt;When ignoring the bounces, we can straightforwardly integrate the ODE analytically&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z(t) &amp;amp;= z_0 + v_0 t - \frac{\mathrm g}{2} t^2, \\&lt;br&gt;
v(t) &amp;amp;= v_0 - \mathrm g\thinspace  t
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;or numerically using the OrdinaryDiffEq package from the 
&lt;a href=&#34;https://sciml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; ecosystem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;### simulate forward

using ForwardDiff, Zygote, OrdinaryDiffEq, DiffEqSensitivity
using Plots, LaTeXStrings

# dynamics
function f(du,u,p,t)
  du[1] = u[2]
  du[2] = -p[1]
end

# parameters and solve
z0 = 5.0
v0 = -0.1
t0 = 0.0
tend = 1.9
g = 10
γ = 0.8

u0 = [z0,v0]
tspan = (t0,tend)
p = [g, γ]
prob = ODEProblem(f,u0,tspan,p)

# plot forward trajectory
sol = solve(prob,Tsit5(),saveat=0.1)
pl = plot(sol, label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomleft)
hline!(pl, [0.0], label=false, color=&amp;quot;black&amp;quot;)
savefig(pl,&amp;quot;BB_forward_no_bounce.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/fKTiDEe.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/BB_forward_no_bounce.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/BB_forward_no_bounce.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Of course, this way the ball continues to fall through the barrier at $z=0$.&lt;/p&gt;
&lt;h2 id=&#34;forward-simulation-with-events&#34;&gt;Forward simulation with events&lt;/h2&gt;
&lt;p&gt;At time $\tau$ around $\tau \approx 1$, the ball hits the ground $z(\tau) = 0$, and is inelastically reflected while dissipating a fraction of its energy. This can be modeled by re-initializing the ODE at time $\tau$ with new initial conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}&lt;br&gt;
z({\tau}) &amp;amp;=  z(\tau-) ,\\&lt;br&gt;
v({\tau})&amp;amp;= -\gamma v(\tau-) ,
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;so that there is a jump in the velocity at the event time: the velocity right before the bounce, the left limit $v(\tau-)$, and the velocity with which the ball continues its movement after the bounce $v(\tau)$, are different.&lt;/p&gt;
&lt;p&gt;Given our analytical solution for the state as a function of time, we can easily compute the event time $\tau$ in terms of the initial condition and parameters as&lt;/p&gt;
&lt;p&gt;$$
\tau = \frac{v_0 + \sqrt{v_0^2 + 2 \mathrm g z_0}}{\mathrm g}.
$$&lt;/p&gt;
&lt;h3 id=&#34;explicit-events&#34;&gt;Explicit events&lt;/h3&gt;
&lt;p&gt;We can define the bounce of the ball as an explicit event by inserting the values of the initial condition and the parameters into the formula for $\tau$. We obtain&lt;/p&gt;
&lt;p&gt;$$
\tau = 0.99005.
$$&lt;/p&gt;
&lt;p&gt;The full explicit trajectory $z_{\rm exp}(t) = z(t)$ is determined by&lt;/p&gt;
&lt;p&gt;$$
z(t) = \begin{cases}
z_0 + v_0 t - \dfrac{\mathrm g}{2} t^2 ,&amp;amp; \forall t &amp;lt; \tau, \\&lt;br&gt;
-0.4901 \mathrm g - 0.5 \mathrm g (-0.99005 + t)^2 + 0.99005 v_0 + z_0\\&lt;br&gt;
\quad  - (-0.99005 + t) (-0.99005 \mathrm g + v_0)\gamma  ,&amp;amp; \forall t \ge \tau,
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;where we used&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}&lt;br&gt;
z({\tau})&amp;amp;=  z_0 + 0.99005 v_0 -0.4901 \mathrm g, \\&lt;br&gt;
v({\tau})&amp;amp;= -\gamma v({\tau-}) = -\gamma(v_0 - 0.99005 \mathrm g)  .
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Here the change in state $(z,v)$ at the event time is defined with the help of an &lt;em&gt;affect function&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$$
a(z,v) = (z, -\gamma v).
$$&lt;/p&gt;
&lt;p&gt;Numerically, we use a &lt;code&gt;DiscreteCallback&lt;/code&gt; in this case to simulate the system.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# solve with DiscreteCallback (explicit event)
tstar = (v0 + sqrt(v0^2+2*z0*g))/g
condition1(u,t,integrator) = (t == tstar)
affect!(integrator) = integrator.u[2] = -integrator.p[2]*integrator.u[2]
cb1 = DiscreteCallback(condition1,affect!,save_positions=(true,true))

sol1 = solve(prob,Tsit5(),callback=cb1, saveat=0.1, tstops=[tstar])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, by choosing an explicit definition of the event, the impact time is fixed. The reflection event is triggered at $\tau = 0.99005$, a time where under different initial configurations the ball perhaps hasn’t reached the ground.&lt;/p&gt;
&lt;h3 id=&#34;implicit-events&#34;&gt;Implicit events&lt;/h3&gt;
&lt;p&gt;The physically more meaningful description of a bouncing ball is therefore given by an implicit description of the event in form of a condition (event function)&lt;/p&gt;
&lt;p&gt;$$
g(z,v,p,t),
$$&lt;/p&gt;
&lt;p&gt;where an event occurs at time $\tau$ if $g(z(\tau),v(\tau),p,\tau) = 0$. We have already used this condition to define our impact time $\tau$ when modeling the bounce explicitly. The implicit formulation also lends itself to take multiple bounces into account by triggering the event every time  $g(z,v,p,t) = 0$.&lt;/p&gt;
&lt;p&gt;As in the previous case, we can analytically compute the full trajectory of the ball. By substituting the formula for $\tau$ we have at the event time&lt;/p&gt;
&lt;p&gt;\begin{aligned}
z({\tau})&amp;amp;= 0, \\&lt;br&gt;
v({\tau}-)&amp;amp;= - \sqrt{v_0^2 + 2 \mathrm g z_0}
\end{aligned}&lt;/p&gt;
&lt;p&gt;for the left limit and&lt;/p&gt;
&lt;p&gt;\begin{aligned}
v({\tau})&amp;amp;= \gamma \sqrt{v_0^2 + 2 \mathrm g z_0}
\end{aligned}&lt;/p&gt;
&lt;p&gt;right after the bounce. Thus, the full trajectory $z_{\rm imp}(t) = z(t)$ is given by&lt;/p&gt;
&lt;p&gt;$$
(\star) \quad z(t) = \begin{cases}
z_0 + v_0 t - \dfrac{\mathrm g}{2} t^2 ,&amp;amp; \forall t &amp;lt; \tau ,\\&lt;br&gt;
-\dfrac{-\mathrm g t + v_0 + \sqrt{v_0^2 + 2 \mathrm g z_0}}{2 \mathrm g}  \\&lt;br&gt;
\quad\cdot \space (-\mathrm g t + v_0 + \sqrt{v_0^2 + 2 \mathrm g z_0} (1 + 2 \gamma)), &amp;amp; \forall t \ge \tau.
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;This is correct even if one substitutes, e.g., a value with higher precision $\mathrm g = 9.81$ for the gravitation constant.&lt;/p&gt;
&lt;p&gt;Numerically, we use a &lt;code&gt;ContinuousCallback&lt;/code&gt; in this case.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# solve with ContinuousCallback (implicit event)
condition2(u,t,integrator) = u[1] # Event happens when condition2(u,t,integrator) == 0
cb2 = ContinuousCallback(condition2,affect!,save_positions=(true,true))
sol2 = solve(prob,Tsit5(),callback=cb2,saveat=0.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can verify that both callbacks lead to the same forward time evolution (for fixed initial conditions and parameters).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# plot forward trajectory
pl1 = plot(sol1, label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], title=&amp;quot;explicit event&amp;quot;, labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright)
pl2 = plot(sol2, label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], title=&amp;quot;implicit event&amp;quot;, labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright)
hline!(pl1, [0.0], label=false, color=&amp;quot;black&amp;quot;)
hline!(pl2, [0.0], label=false, color=&amp;quot;black&amp;quot;)
pl = plot(pl1,pl2)
savefig(pl,&amp;quot;BB_forward.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/BB_forward.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/BB_forward.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;In addition, the implicitly defined impact time via the &lt;code&gt;ContinuousCallback&lt;/code&gt; also changes appropriately when changing the initial conditions or the parameters, for example when using  $\mathrm g = 9.81$ for the gravitation constant. In other words, the event time $\tau=\tau(p,z_0,v_0,t_0)$ is a function of the parameters and initial conditions, and is implicitly defined by the event condition.&lt;/p&gt;
&lt;p&gt;Suppose we let the ball drop from a somewhat higher position now. Does an increase in height $z$ at $t=0$ give an increase or decrease in height at the end time $t_\text{end}=1.9$? This is something we can answer with sensitivity analysis. For example if we increase the height by (a fraction of) one unit then using $(\star)$&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d} z(t_\text{end})}{\text{d} z_0}  = 0.84,
$$&lt;/p&gt;
&lt;p&gt;meaning the height at $t_\text{end}$ is also by a corresponding fraction of 0.84 units higher.&lt;/p&gt;
&lt;p&gt;We can verify this visually:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# animate forward trajectory
sol3 = solve(remake(prob,u0=[u0[1]+0.5,u0[2]]),Tsit5(),callback=cb2,saveat=0.01)

plt2 = plot(sol2, label = false, labelfontsize=20, legendfontsize=20, lw = 1, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, color=&amp;quot;black&amp;quot;, xlims=(t0,tend))
hline!(plt2, [0.0], label=false, color=&amp;quot;black&amp;quot;)
plot!(plt2, sol3, tspan=(t0,tend), color=[1 2], label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, denseplot=true, xlims=(t0,tend), ylims=(-11,9))
# scatter!(plt2, [t2,t2], sol3(t2), color=[1, 2], label=false)

list_plots = []
for t in sol3.t
  tstart = 0.0

  plt1 = plot(sol2, label = false, labelfontsize=20, legendfontsize=20, lw = 1, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, color=&amp;quot;black&amp;quot;)
  hline!(plt1, [0.0], label=false, color=&amp;quot;black&amp;quot;)
  plot!(plt1, sol3, tspan=(t0,t), color=[1 2], label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, denseplot=true, xlims=(t0,tend), ylims=(-11,9))
  scatter!(plt1,[t,t], sol3(t), color=[1, 2], label=false)
  plt = plot(plt1,plt2)
  push!(list_plots, plt)
end

plot(list_plots[100])

anim = animate(list_plots,every=1)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/BB.gif&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/BB.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The original curve is shown in black in the figure above.&lt;/p&gt;
&lt;h2 id=&#34;sensitivity-analysis-with-events&#34;&gt;Sensitivity analysis with events&lt;/h2&gt;
&lt;p&gt;In more general terms, the last example can be seen as a loss function acting on $z$ at the end time&lt;/p&gt;
&lt;p&gt;$$
L^{\text{exp}} = z(t_{\text{end}}),
$$&lt;/p&gt;
&lt;p&gt;where the superscript &amp;ldquo;exp&amp;rdquo; refers to an explicit definition of the time ($t_{\text{end}}$) at which we evaluate the loss function.  Let $\alpha$ denote any of the inputs $(z_0,v_0,g,\gamma)$. The sensitivity with respect to $\alpha$ is then given by the chain rule&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}L^{\text{exp}}}{\text{d} \alpha} = \frac{\text{d}L^{\text{exp}}}{\text{d} z}  \frac{\text{d}z(t_{\text{end}})}{\text{d} \alpha} = \frac{\text{d}z(t_{\text{end}})}{\text{d} \alpha}.
$$&lt;/p&gt;
&lt;p&gt;Inserting our results for $z(t) = z_{\rm exp}(t)$ instead of $z(t) = z_{\rm imp}(t)$ at $t_{\text{end}}$, a different value for the sensitivity is obtained (a value ignoring the changes in $z$ due to changes in the bouncing time $\tau$), e.g.,&lt;/p&gt;
&lt;p&gt;$$
\quad \frac{\text{d}L^{\text{exp}}}{\text{d} z_0} = \begin{cases}
1 &amp;amp; \text{with } z(t) = z_{\rm exp}(t) ,\\&lt;br&gt;
0.84 &amp;amp; \text{with } z(t) = z_{\rm imp}(t) .
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Besides an explicit description of time points, we may also encounter implicitly defined time points. For example, for the bouncing ball with velocity at the impact time $\tau-$ (i.e., at the left limit) as the quantity of interest,&lt;/p&gt;
&lt;p&gt;$$
L = v(\tau-),
$$&lt;/p&gt;
&lt;p&gt;we could be interested in the sensitivity of $L$ with respect $g$. Using the velocity $v(t)$ at the explicit time $t = 0.9905$ instead of the implicit $v(\tau-)$, again a different value for the sensitivity is obtained (a value again ignoring the changes in $\tau$ and not the one we are looking for), even though $\tau = 0.9905$ in both cases:&lt;/p&gt;
&lt;p&gt;$$
\quad \frac{\text{d}L}{\text{d} g} = \begin{cases}
-0.99005 &amp;amp; \text{with } v(t) = v_{\rm exp}(t) ,\\&lt;br&gt;
-\frac{g}{\sqrt{v_0^2 + 2 g z_0}} = -0.99995 &amp;amp; \text{ with } v(t) = v_{\rm imp}(t) .
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Thanks to our analytical results for the bouncing ball, the sensitivity computation has been straightforward up to now. However, in most systems, we won&amp;rsquo;t be able to solve analytically a differential equation&lt;/p&gt;
&lt;p&gt;$$
\text{d}x(t) = f(x,p,t) \text{d}t
$$&lt;/p&gt;
&lt;p&gt;with initial condition $x_0=x(t_0)$. Instead, we have to numerically solve for the trajectory $x(t)$.&lt;/p&gt;
&lt;p&gt;More completely, we will in the following derive an adjoint sensitivity method for a loss function&lt;/p&gt;
&lt;p&gt;$$L = \sum_j L_j(\tau_j,x(\tau_j),p) + \sum_i L^{\text{exp}}_i(s_j,x(s_i),p)  ,$$&lt;/p&gt;
&lt;p&gt;with $L_i^{\text{exp}}$ at explicit time points $s_i$, such as $t_{\text{end}}$, and $L_j(\tau_j,x(\tau_j),p)$ at implicit time points, such as $\tau$, which allows us to compute the sensitivity of $L$ with respect to changes of the parameters or initial condition without the requirement of an analytical solution for $x(t)$.&lt;/p&gt;
&lt;h3 id=&#34;backsolve-adjoint-algorithm-for-ordinary-differential-equations&#34;&gt;Backsolve-Adjoint algorithm for ordinary differential equations&lt;/h3&gt;
&lt;p&gt;Taking derivatives (or finding sensitivities) works in a beautiful mechanical way. We or a computer can find the derivatives of complex expressions by just repeatedly applying the chain rule.&lt;/p&gt;
&lt;p&gt;We write&lt;/p&gt;
&lt;p&gt;$$\text{solve}(t_0, x_0, t, p)$$&lt;/p&gt;
&lt;p&gt;$(= x(t))$ for the functional solution of the ODE at time $t$.&lt;/p&gt;
&lt;p&gt;Regarding the computation of the sensitivities (the derivatives of the function &lt;code&gt;solve&lt;/code&gt;), we may then choose one of the 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/analysis/sensitivity/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available algorithms&lt;/a&gt; for the given differential equation. Currently, &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;, &lt;code&gt;InterpolatingAdjoint()&lt;/code&gt;, &lt;code&gt;QuadratureAdjoint()&lt;/code&gt;, &lt;code&gt;ReverseDiffAdjoint()&lt;/code&gt;, &lt;code&gt;TrackerAdjoint()&lt;/code&gt;, and &lt;code&gt;ForwardDiffAdjoint()&lt;/code&gt; are compatible with events in ordinary differential equations.&lt;/p&gt;
&lt;p&gt;Let us focus on the &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; algorithm which computes the sensitivities&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}\thinspace L(\text{solve}(t_0, x_0, t, p))}{\text{d}x_{0}} &amp;amp;= \lambda(t_{0}),\\&lt;br&gt;
\frac{\text{d}\thinspace L(\text{solve}(t_0, x_0, t, p))}{\text{d}p} &amp;amp;= \lambda_{p}(t_{0}),
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;of a loss function $L$ acting on the final state with respect to the initial state and the parameters. It does so by solving an ODE for $\lambda(s)$ in reverse time from $t$ to $t_0$&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}\lambda(s)}{\text{d}s} &amp;amp;= -\lambda(s)^\dagger \frac{\text{d} f(\rightarrow x(s), p, t)}{\text{d} x(s)} \\&lt;br&gt;
\frac{\text{d}\lambda_{p}(s)}{\text{d}s} &amp;amp;= -\lambda(s)^\dagger \frac{\text{d} f(x(s), \rightarrow p, s)}{\text{d} p},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;with initial conditions:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda(t)&amp;amp;= \frac{\text{d}\thinspace L(\text{solve}(t_0, x_0, t, p))}{\text{d}x_{T}}, \\&lt;br&gt;
\lambda_{p}(t) &amp;amp;= 0.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The arrows ($\rightarrow$) indicate the variable with respect to which we differentiate, which will become important later when the same variable shows up in multiple function arguments.&lt;/p&gt;
&lt;p&gt;Note that computing the vector-Jacobian products (vjp) in the adjoint ODE requires the value of $x(s)$ along its trajectory. In &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;, we recompute $x(s)$ &amp;ndash; together with the adjoint variables &amp;ndash; backwards in time starting with its final value $x(t)$. A derivation of the ODE adjoint is given in 
&lt;a href=&#34;https://mitmath.github.io/18337/lecture11/adjoints&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris&#39; MIT 18.337 lecture notes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BacksolveAdjoint()&lt;/code&gt;, essentially the custom primitive differentiation rule of &lt;code&gt;solve&lt;/code&gt;, is the elementary building block needed to derive sensitivities also in more complicated examples:&lt;/p&gt;
&lt;p&gt;Consider a loss depending on the state $x(s_i)$ at fix time points $s_i$ through loss functions $L_i^{\text{exp}}$,&lt;/p&gt;
&lt;p&gt;$$
L^{\text{exp}} = \sum_i L_i^{\text{exp}}(s_i, x(s_i), p).
$$&lt;/p&gt;
&lt;p&gt;Without contortions we can obtain the sensitivity of $L^{\text{exp}}$ in $p$ (or in $x_0$) using the tool we have. For those a bit familiar with automatic differentiation, this is perhaps easiest to see if we write $L^{\text{exp}}$ as pseudo code&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function loss(t0, x0, p)
    x1 = solve(t0, x0, s1, p)
    L = L1(s1, x1, p)
    x2 = solve(s1, x1, s2, p)
    L += L2(s2, x2, p)
    ...
    return L
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and consider the problem of automatically differentiating it. You&amp;rsquo;ll just need the primitives of &lt;code&gt;solve&lt;/code&gt;, &lt;code&gt;L1&lt;/code&gt; and &lt;code&gt;L2&lt;/code&gt;! The sensitivities can be computed by repeated calls to &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; on the intervals &lt;code&gt;(s_i, s_{i+1})&lt;/code&gt; backward in time, taking in the sensitivities $\sum_i L^{\text{exp}}(s_i, x(s_i), p)$ at times $s_i$, or according to the common notation&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; as single call &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; with discrete callbacks:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}\lambda(t)}{\text{d}t} &amp;amp;= -\lambda(t)^\dagger \frac{\text{d} f(\rightarrow x(t), p, t)}{\text{d} x(t)} - \frac{\text{d} L_i^{\text{exp}}(\rightarrow x(t), p)}{\text{d} x(t)}^\dagger \delta(t-s_i), \\&lt;br&gt;
\frac{\text{d}\lambda_{p}(t)}{\text{d}t} &amp;amp;= -\lambda(t)^\dagger \frac{\text{d} f(x(t), \rightarrow p, t)}{\text{d} p} - \frac{\text{d} L_i^{\text{exp}}(  x(t),\rightarrow p)}{\text{d} p}^\dagger \delta(t-s_i).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The sensitivities of the ordinary &lt;code&gt;solve&lt;/code&gt; with respect to the other arguments are also needed and given by&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}(\text{solve}(s, x, \rightarrow t, p))}{\text{d}t} = f(\text{solve}(s, x, t, p), p, t)
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}(\text{solve}(\rightarrow s, x, t, p))}{\text{d}s} = -f(x, p, s).
$$&lt;/p&gt;
&lt;p&gt;Now we can even properly define the &lt;code&gt;rrule&lt;/code&gt; of &lt;code&gt;solve&lt;/code&gt; in the sense of &lt;code&gt;DiffRules.jl&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;explicit-events-1&#34;&gt;Explicit events&lt;/h3&gt;
&lt;p&gt;To make &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; compatible with explicit events&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function loss(t0, x0, p)
    x1- = solve(t0, x0, s1, p)
    L = L1-(s1, x1-, p) # saved before affect
    x1 = a(x1-, p, s1) # affect
    L += L1(s1, x1, p) # saved after affect
    x2- = solve(s1, x1, s2, p)
    L += L2-(s2, x2-, p)
    x2 = a(x2-, p, s2) # affect
    L += L2(s2, x2, p)
    ...
    return L
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we have to store the event times $s_j$ as well as the state $x(s_j-)$ at the left limit of $s_i$.&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; We then solve the adjoint ODE backwards in time between the events. As soon as we reach an event time $s_j$ from the right, we update the augmented state according to&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda({s_j}-) &amp;amp;= \lambda({s_j})^\dagger \frac{\text{d} a(\rightarrow x({s_j}-), p, {s_j}-)}{\text{d} x(s_j-)} \\&lt;br&gt;
\lambda_p({s_j}-) &amp;amp;= \lambda_p({s_j}) -  \lambda({s_j})^\dagger \frac{\text{d} a(x({s_j}-), \rightarrow p, {s_j}-)}{\text{d} p}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $a$ is the affect function applied at the discontinuity. That is, to lift the adjoint from the right to the left limit, we compute a vjp with the adjoint $\lambda({s_j})$ from the right and the Jacobian of the affect function evaluated immediately before the event time at $s_j-$ .&lt;/p&gt;
&lt;p&gt;In particular, we apply a loss function callback before and after this update if the state was saved in the forward evolution and entered directly into the loss function.&lt;/p&gt;
&lt;h3 id=&#34;implicit-events-1&#34;&gt;Implicit events&lt;/h3&gt;
&lt;p&gt;With implicit events it is similar: Being able to differentiate the ODE when an implicit event terminates the ODE gives us the custom primitive differentiation rule of a &lt;code&gt;solve&lt;/code&gt; with implicit callback.&lt;/p&gt;
&lt;p&gt;We have to account for an important change: besides the value $\xi = x(\tau)$ at time of the implicit event, the solver returns the variable event time $\tau$ itself.&lt;/p&gt;
&lt;p&gt;We could write for an event condition function $g$&lt;/p&gt;
&lt;p&gt;$$(\tau_1, \xi_1) = \text{solve2}(t_0, x_0, g, p)$$&lt;/p&gt;
&lt;p&gt;to put emphasis on this, or equivalently, compute for a unspecified function $L(t, x, p)$ the result of $\frac{\text{d}L}{\text{d}p}$ with&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}L}{\text{d}p} &amp;amp;= \frac{\text{d}L( \text{solve2}(t_0, x_0, g, \rightarrow p),\rightarrow p)}{\text{d}p}\\&lt;br&gt;
&amp;amp;= \frac{\text{d}L(\tau_1({\color{black}\rightarrow} p), \text{solve}(t_0, x_0, \tau_1({\color{black}\rightarrow}p), \rightarrow p),\rightarrow p)}{\text{d}p},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which indicates that changing $p$ influences $L$ both through changes in $\tau_1$ as well as changes in&lt;/p&gt;
&lt;p&gt;$$\xi_1 = x(\tau_1-).$$&lt;/p&gt;
&lt;p&gt;This case where we have a loss function $L = L_1$ depending on $\tau_1$, $x(\tau_1)$, and $p$
was also considered by Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel in their ICLR 2021 paper&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, the sensitivity of the event time with respect to parameters $\frac{\text{d}\tau}{\text{d}p}$ must be taken into account.
Here and in the following we consider only the $p$-dependence of $\tau_1$ for simplicity. However, it is straightforward to include a dependence on the initial state $x_0$ in an analogues way&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;In a first step, we need to compute the sensitivity of $\tau_1(p)$ with respect to $p$ (or $x_0$) based on the event condition $g(t, x(t)) = 0$.  We can apply the 
&lt;a href=&#34;https://www.uni-siegen.de/fb6/analysis/overhagen/vorlesungsbeschreibungen/skripte/analysis3_1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implicit function theorem&lt;/a&gt;. For this, see that $\tau_1(p)$ is implicitly defined by $F(p, \tau_1) = g( \tau_1, \text{solve}(t_0, x_0, \tau_1, p)) = 0$ which yields&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}\tau_1(p)}{\text{d}p} &amp;amp;= - \left(\frac{\text{d}g(\rightarrow \tau_1, \text{solve}(t_0, x_0, \rightarrow \tau_1, p))}{\text{d}\tau_1}\right)^{-1} \frac{\text{d}g(\tau_1, \text{solve}(t_0, x_0, \tau_1, \rightarrow p))}{\text{d}p} .\\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The total derivative&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; inside the bracket is:
$$
\begin{aligned}
\frac{\text{d}g}{\text{d}\tau_1} \stackrel{\text{def}}{=} \frac{\text{d}g(\rightarrow \tau_1, \text{solve}(t_0, x_0, \rightarrow \tau_1, p))}{\text{d}\tau_1} &amp;amp;= \frac{\text{d}g(\rightarrow \tau_1, \xi_1)}{\text{d}\tau_1} + \frac{\text{d}g(\tau_1, \text{solve}(t_0, x_0, \rightarrow \tau_1, p))}{\text{d}\tau_1}\\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Since&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}(\text{solve}(t_0, x_0, \rightarrow \tau_1, p))}{\text{d}\tau_1} = f(\xi_1, p, \tau_1)
$$&lt;/p&gt;
&lt;p&gt;by definition of the ODE, we can write&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}g(\tau_1, \text{solve}(t_0, x_0, \rightarrow \tau_1, p))}{\text{d}\tau_1} = \frac{\text{d}g(\tau_1, \xi_1)}{\text{d} \xi_1}^{\dagger}  f(\xi_1, p, \tau_1).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Furthermore, we have
$$
\begin{aligned}
\frac{\text{d}g(\tau_1, \text{solve}(t_0, x_0, \tau_1, \rightarrow p))}{\text{d}p} = \frac{\text{d}g(\tau_1, \xi_1)}{\text{d} \xi_1}^{\dagger}  \frac{\text{d}\text{ solve}(t_0, x_0, \tau_1,\rightarrow p)}{\text{d}p}
\end{aligned}
$$
for the second term of $\dfrac{\text{d}\tau_1(p)}{\text{d}p}$.&lt;/p&gt;
&lt;p&gt;We can now write the gradient as:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}L(\tau_1({\color{black}\rightarrow} p), \text{solve}(t_0, x_0, \tau_1({\color{black}\rightarrow}p), \rightarrow p),\rightarrow p)}{\text{d}p} &amp;amp;= \frac{\text{d}L(\tau_1(p), \text{solve}(t_0, x_0, \tau_1(p),  p), \rightarrow p)}{\text{d}p} \\&lt;br&gt;
+&amp;amp; \frac{\text{d}L(\tau_1(p), \text{solve}(t_0, x_0, \tau_1(p),  \rightarrow p), p)}{\text{d}p} \\&lt;br&gt;
+&amp;amp; \frac{\text{d}L(\rightarrow \tau_1(p), \text{solve}(t_0, x_0, \rightarrow \tau_1(p),  p), p)}{\text{d}\tau_1} \frac{\text{d} \tau_1(p)}{\text{d}p},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which, after insertion of our results above, can be cast into the form:&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}L}{\text{d}p} = v^\dagger \frac{\text{d}\text{ solve}(t_0, x_0, \tau_1(p), \rightarrow p)}{\text{d}p} + \frac{\text{d}L(\tau_1(p), \text{solve}(t_0, x_0, \tau_1(p), p), \rightarrow p)}{\text{d}p},
$$&lt;/p&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
v &amp;amp;= \rho \left(-\frac{\text{d}g}{\text{d}\tau_1}\right)^{-1} \frac{\text{d}g}{\text{d}\xi_1} + \frac{\text{d}L(\tau_1, \xi_1)}{\text{d} \xi_1},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where we introduced the scalar pre-factor&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\rho = \left( \frac{\text{d}L(\rightarrow \tau_1, \xi_1)}{\text{d}\tau_1} +  \frac{\text{d}L(\tau_1, \xi_1)}{\text{d} \xi_1}^\dagger f(\xi_1, p, \tau_1)\right).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We have therefore reduced this case to a modification of the original &lt;code&gt;BacksolveAdjoint&lt;/code&gt;.
This means that if we terminate the ODE integration by an implicit event, we compute the sensitivities as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Use an ODE solver to solve forward from the starting value until the event is triggered
$$
\xi_1 = \text{solve}(t_0, x_0, \tau_1,  p).
$$
$(\tau_1,\xi_1)$ are the stored values which enter the loss function, which depend on $t_0, x_0$ and $p$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the loss function gradient with respect to the state and event time
$$
\lambda^0_1 = \frac{\text{d}L(\tau_1(p), \rightarrow \xi_1, p)}{\text{d} \xi_1}, \quad \lambda^0_{\tau_1} = \frac{\text{d}L(\rightarrow \tau_1(p),  \xi_1, p)}{\text{d} \tau_1(p)}.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Instead of using the &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; algorithm with $\lambda_1^0$ directly,) use the corrected version containing the dependence on the event time. For this, compute  $\frac{\text{d}g}{\text{d}\tau_1}, \frac{\text{d}g}{\text{d}\xi_1}$, and $f(\xi_1, p, \tau_1)$.
Then, the corrected version of the adjoint is given by&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
{\color{red}\lambda_1} = - \left( \lambda^0_{\tau_1} + {\lambda^\text{0}_1}^\dagger f(\xi_1, p, \tau_1) \right)\left(\frac{\text{d}g}{\text{d}\tau_1}\right)^{-1} \frac{\text{d}g}{\text{d}\xi_1} + \lambda^0_1.
$$&lt;/p&gt;
&lt;p&gt;The correction takes into account a change in the end time and end value of the ODE.  ${\color{red}\lambda_1}$ can then be used as initial condition to $\text{backsolve_adjoint}({\color{red}\lambda_1}, \tau_1, \xi_1, t_0)$ which backpropagates the adjoint ${\color{red}\lambda_1}$ at $\xi_1 = x(\tau_1-)$ from $\tau_1$ to $t_0$.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;If there is an additional affect function $a$ associated with the event, i.e. a right limit, we must additionally compute&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\begin{aligned}
{\lambda_{a,1}^0} =  \frac{\text{d}L(\tau_1(p), \rightarrow a(\xi_1, p),p)}{\text{d} a}.
\end{aligned}
$$&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Compute the vjp as in the case of a &amp;lsquo;DiscreteCallback&amp;rsquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\lambda_{a,1}^{1} = {\lambda_{a,1}^0}^\dagger \frac{\text{d}a(\xi_1,p)}{\text{d} \xi_1}
$$&lt;/p&gt;
&lt;p&gt;and correct it as above&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
{\color{blue}\lambda_{a,1}} = - \left( \lambda^0_{\tau_1} +  {{\lambda_{a,1}^1}}^\dagger f(\xi_1, p, \tau_1) \right)\left(\frac{\text{d}g}{\text{d}{\tau_1}}\right)^{-1} \frac{\text{d}g}{\text{d}{\xi_1}} + {\lambda_{a,1}^1}.
\end{aligned}
$$&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;If both limits contribute to the loss function, the contributions ${\color{red}\lambda_1}$ and ${\color{blue}\lambda_{a,1}}$ are added.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;generalization-several-events&#34;&gt;Generalization: several events&lt;/h4&gt;
&lt;p&gt;As implied by Chen et al. as well as by Timo C. Wunderlich and Christian Pehle&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, one can chain together the events and differentiate through the entire time evolution on a time interval $(t_0, t_{\text{end}})$. That is, we are generally allowed to segment the time evolution over an interval $[t_0, t]$ into one from $[t_0, s]$ and a subsequent one from $[s, t]$:&lt;/p&gt;
&lt;p&gt;$$
\text{solve}(t_0, x_0, t, p)  = \text{solve}(s, \text{solve}(t_0, x_0, s, p), t-s, p),
$$&lt;/p&gt;
&lt;p&gt;such that also loss function contributions are chained.&lt;/p&gt;
&lt;p&gt;(A good exercise to get familiar with these type of arguments is to verify&lt;/p&gt;
&lt;p&gt;$$\frac{\text{d}}{\text{d}s} \text{solve}(s, \text{solve}(t_0, x_0, s, p), t-s, p) = 0.)$$&lt;/p&gt;
&lt;p&gt;Essentially, we know how to address several events already by considering a loss function&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function loss2(t0, x0, p)
    tau1, xi1 = solve2(t0, x0, g, p)
    L = L1(tau1, xi1, p)
    tau2, xi2 = solve2(tau1, xi1, g, p)
    L += L2(tau2, xi2, p)
    ...
    return L
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and applying the same generic rules of (automatic) differentiation as in the previous example, now using the elementary differentiation rule for &lt;code&gt;solve2&lt;/code&gt; (&lt;code&gt;rrule&lt;/code&gt; for &lt;code&gt;solve2&lt;/code&gt;) we derived above.&lt;/p&gt;
&lt;p&gt;Differentiating by hand, we have the following modification of the method from the previous section, which can be derived choosing a particular function $L$ in the previous section which incorporates the &lt;code&gt;solve&lt;/code&gt; from $\tau_1$ to $t_{\text{end}}$. Note&lt;/p&gt;
&lt;p&gt;$$
\lambda_{\tau_1}^0 = \frac{\text{d}(\text{solve}(\rightarrow \tau_1,  a(\xi_1,p), t_{\text{end}}, p))}{\text{d} \tau_1} = - f(a(\xi_1,p), p, \tau_1).
$$&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;
&lt;p&gt;Segment the trajectory at the event times. Use $\text{backsolve_adjoint}(\lambda^0_\text{end}, t_\text{end}, x(t_\text{end}), \tau_{J})$ to backprogagate the loss function gradient $\lambda^0_\text{end} = \frac{\text{d}L(t_\text{end}, \rightarrow x(t_\text{end}), p)}{\text{d} x(t_\text{end})}$ from the end state until the right limit of the last event location, obtaining  $\lambda_{J}$ at time $\tau_{J}$ corresponding to the last event before $t_{\text{end}}$ with index $J$ (say).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute ${\color{red}\lambda_J}$ as in step 3 (at time $\tau_{J}$ with $\lambda^0_J = \frac{\text{d}L(\tau_J(p), \rightarrow \xi_J, p)}{\text{d} \xi_J}$).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As in step 4 compute the vjp (but this time with the sum of the two contributions, $\lambda_J$ and $\lambda^0_{a,J} = \frac{\text{d}L(\tau_J(p), \rightarrow a(\xi_J,p), p)}{\text{d} a}$)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\lambda_{a,J}^{1} = \left({\lambda_{a,J}^{0} + \lambda_{J}} \right)^\dagger \frac{\text{d} a(\xi_J, p)}{\text{d} \xi_J}.
$$&lt;/p&gt;
&lt;p&gt;${\color{blue}\lambda_{a,J}}$ follows from $\lambda_{a,J}^{1}$ as in step 5.&lt;/p&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li&gt;Compute an additional correction term:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
{\color{green}\lambda_{c,J}} = \left( \lambda_J^\dagger f(a(\xi_J, p), p, \tau_J) \right)\left(\frac{\text{d}g}{\text{d} \tau_J}\right)^{-1} \frac{\text{d}g}{\text{d}{\xi_J}},
$$&lt;/p&gt;
&lt;!--where ${\color{green}\lambda_+}$ is now the right-hand limit of the adjoint state before the loss gradient was added but is used as input as ${\color{green}\lambda_+}$ before. --&gt;
&lt;p&gt;The correction has the opposite sign and corresponds to a change in the starting time and starting value in the later time interval ($\tau_J, t_\text{end}$) of the ODE.&lt;/p&gt;
&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;Backpropagate $\lambda_J = {\color{red}\lambda_J} + {\color{blue}\lambda_{a,J}} + {\color{green}\lambda_{c,J}}$ to the next event time $\tau_{J-1}$ and iterate over the remaining $J-1$ events.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;We are still refining the adjoints in case of implicit discontinuities (&lt;code&gt;ContinuousCallbacks&lt;/code&gt;). For further information, the interested reader is encouraged to track the associated issues 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues/383&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#383&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues/374&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#374&lt;/a&gt;, and 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/445&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PR #445&lt;/a&gt; in the DiffEqSensitivity.jl package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact us (
&lt;a href=&#34;https://github.com/frankschae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github.com/frankschae&lt;/a&gt;)!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Michael Poli, Stefano Massaroli, et al., arXiv preprint arXiv:2106.04165 (2021). &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Junteng Jia, Austin R. Benson, arXiv preprint arXiv:1905.10403 (2019). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Timo C. Wunderlich and Christian Pehle, Sci. Rep. &lt;em&gt;11&lt;/em&gt;, 12829 (2021). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel, arXiv preprint arXiv:2011.03902 (2020). &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;If the affect function also changes the parameters of the differential equation, we must additionally store $p(t_i-)$ and compute another vjp to update $\lambda_p$. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For a function $f$ of more than one variable $y = f(t, x_1(t),x_2(t),\dots,x_N(t))$, the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Differential_of_a_function#Differentials_in_several_variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;total derivative&lt;/a&gt; with respect to the independent variable $t$ is given by the sum of all partial derivatives
$$
\begin{aligned}
\frac{\text{d}y}{\text{d}t} &amp;amp;= \frac{\text{d}f(\rightarrow t, x_1(\rightarrow t),x_2(\rightarrow t),\dots,x_N(\rightarrow t))}{\text{d}t} \\&lt;br&gt;
&amp;amp;= \frac{\text{d}f(\rightarrow t, x_1(t),x_2(t),\dots,x_N(t))}{\text{d}t} + \frac{\text{d}f(t, x_1(\rightarrow t),x_2(t),\dots,x_N(t))}{\text{d}t}\\&lt;br&gt;
&amp;amp;+ \frac{\text{d}f(t, x_1(t),x_2(\rightarrow t),\dots,x_N(t))}{\text{d}t} + \dots +  \frac{\text{d}f(t, x_1(t),x_2(t),\dots,x_N(\rightarrow t))}{\text{d}t}.
\end{aligned}
$$ &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Neural Hybrid Differential Equations</title>
      <link>https://frankschae.github.io/post/hybridde/</link>
      <pubDate>Wed, 16 Jun 2021 14:50:17 +0200</pubDate>
      <guid>https://frankschae.github.io/post/hybridde/</guid>
      <description>&lt;p&gt;I am delighted that I have been awarded my second GSoC stipend this year.  I look forward to carrying out the ambitious project scope with my mentors 
&lt;a href=&#34;https://github.com/ChrisRackauckas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris Rackauckas&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mschauer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moritz Schauer&lt;/a&gt;,  
&lt;a href=&#34;https://github.com/YingboMa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yingbo Ma&lt;/a&gt;, and 
&lt;a href=&#34;https://github.com/mohamed82008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mohamed Tarek&lt;/a&gt;. This year&amp;rsquo;s project is embedded within the 
&lt;a href=&#34;https://summerofcode.withgoogle.com/organizations/5765643267211264/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NumFocus&lt;/a&gt;/
&lt;a href=&#34;https://sciml.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; organization and comprises adjoint sensitivity methods for discontinuities, shadowing methods for chaotic dynamics, symbolically generated adjoint methods, and further AD tooling within the Julia Language.&lt;/p&gt;
&lt;p&gt;This first post aims to illustrate our new (adjoint) sensitivity analysis tools with respect to event handling in (ordinary) differential equations (DEs).&lt;/p&gt;
&lt;p&gt;Note: Please check the 
&lt;a href=&#34;https://docs.sciml.ai/SciMLSensitivity/dev/examples/hybrid_jump/hybrid_diffeq/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciMLSensitivity.jl docs&lt;/a&gt; for a maintained neural hybrid DE tutorial!&lt;/p&gt;
&lt;h2 id=&#34;hybrid-differential-equations&#34;&gt;Hybrid Differential Equations&lt;/h2&gt;
&lt;p&gt;DEs with additional explicit or implicit discontinuities are called hybrid DEs. Within the SciML software suite, such discontinuities may be incorporated into DE models by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;callbacks&lt;/a&gt;. Evidently, the incorporation of discontinuities allows a user to specify changes (&lt;em&gt;events&lt;/em&gt;) in the system, i.e., changes of the state or the parameters of the DE, which cannot be modeled by a plain ordinary DE. While explicit events can be described by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/#DiscreteCallback-Examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiscreteCallbacks&lt;/a&gt;, implicit events have to be specified by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/#ContinuousCallback-Examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ContinuousCallbacks&lt;/a&gt;. That is, explicit events possess explicit event times, while implicit events are triggered when a continuous function evaluates to &lt;code&gt;0&lt;/code&gt;. Thus, implicit events require some sort of rootfinding procedure.&lt;/p&gt;
&lt;p&gt;Some relevant examples for hybrid DEs with discrete or continuous callbacks are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;quantum optics experiments, where photon-counting measurements lead to jumps in the quantum state that occur with a variable rate, see for instance Appendix A in Ref.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; (&lt;code&gt;ContinuousCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;a bouncing ball&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; (&lt;code&gt;ContinuousCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;classical point process models, such as a Poisson process&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;digital controllers&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, where a continuous system dynamics is controlled by a discrete-time controller (&lt;code&gt;DiscreteCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;pharmacokinetic models&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, where explicit dosing times change the drug concentration in the blood (&lt;code&gt;DiscreteCallback&lt;/code&gt;). The simplest possible example being the one-compartment model.&lt;/li&gt;
&lt;li&gt;kicked oscillator dynamics, e.g., a harmonic oscillator that gets a kick at some time points (&lt;code&gt;DiscreteCallback&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The associated sensitivity methods that allow us to differentiate through the respective hybrid DE systems have been recently introduced in Refs. &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kicked-harmonic-oscillator&#34;&gt;Kicked harmonic oscillator&lt;/h2&gt;
&lt;p&gt;Let us consider the simple physical model of a damped harmonic oscillator, described by an ODE of the form&lt;/p&gt;
&lt;p&gt;$$
\ddot{x}(t) + a\cdot\dot{x}(t) + b \cdot x(t) = 0 ,
$$&lt;/p&gt;
&lt;p&gt;where $a=0.1$ and $b=1$ with initial conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
x(t=0) &amp;amp;= 1  \\&lt;br&gt;
v(t=0) &amp;amp;= \dot{x}(t=0) = 0.
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This second-order ODE can be 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_differential_equation#Reduction_of_order&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reduced&lt;/a&gt; to two first-order ODEs, such that we can straightforwardly simulate the resulting ODE with the &lt;code&gt;DifferentialEquations.jl&lt;/code&gt; package. (Instead of doing this reduction manually, we could also use 
&lt;a href=&#34;https://mtk.sciml.ai/stable/tutorials/higher_order/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ModelingToolkit.jl&lt;/code&gt;&lt;/a&gt; to transform the ODE in an automatic manner. Alternatively, for second-order ODEs, there is also a &lt;code&gt;SecondOrderODEProblem&lt;/code&gt; implemented.) The Julia code reads:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using DiffEqFlux, DifferentialEquations, Flux, Optim, Plots, DiffEqSensitivity
using Zygote
using Random
u0 = Float32[1.; 0.]

tspan = (0.0f0,50.0f0)

dtsave = 0.5f0
t = tspan[1]:dtsave:tspan[2]

function oscillator!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  return nothing
end

prob_data = ODEProblem(oscillator!,u0,tspan)

# ODE without kicks
pl = plot(solve(prob_data,Tsit5(),saveat=t), label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/forward_damped_oscillator_no_kicks.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/forward_damped_oscillator_no_kicks.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We now include a kick to the velocity of the oscillator at regular time steps. Here, we choose both the time difference between the kicks and the increase in velocity as &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;kicktimes = tspan[1]:1:tspan[2]
function kick!(integrator)
  integrator.u[end] += one(eltype(integrator.u))
end
cb_ = PresetTimeCallback(kicktimes,kick!,save_positions=(false,false))

sol_data = solve(prob_data,Tsit5(),callback=cb_,saveat=t)
t_data = sol_data.t
ode_data = Array(sol_data)

# visualize data
pl1 = plot(t_data,ode_data[1,:],label=&amp;quot;data x(t)&amp;quot;)
plot!(pl1,t_data,ode_data[2,:],label=&amp;quot;data v(t)&amp;quot;)

pl2 = plot(t_data[1:20],ode_data[1,1:20],label=&amp;quot;data x(t)&amp;quot;)
plot!(pl2,t_data[1:20],ode_data[2,1:20],label=&amp;quot;data v(t)&amp;quot;)
pl = plot(pl2, pl1, layout=(1,2), xlabel=&amp;quot;t&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/forward_damped_oscillator.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/forward_damped_oscillator.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

The left-hand side shows a zoom for short times to better resolve the kicks. Note that by setting &lt;code&gt;save_positions=(true,true)&lt;/code&gt;, the kicks would be saved before &lt;strong&gt;and&lt;/strong&gt; after the event such that the kicks would appear completely vertically in the plot. The data on the right-hand will be used as training data below. In the spirit of universal differential equations&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;, we now aim at learning (potentially) missing parts of the model from these data traces.&lt;/p&gt;
&lt;h3 id=&#34;high-domain-knowledge&#34;&gt;High domain knowledge&lt;/h3&gt;
&lt;p&gt;For simplicity, we assume that we have almost perfect knowledge about our system. That is, we assume to know the basic structure of the ODE, including its parameters $a$ and $b$, and that the &lt;code&gt;affect!&lt;/code&gt; function of the event only acts on the velocity. We then encode the affect as an additional component to the ODE. The task is thus to learn the dynamics of the third component of &lt;code&gt;integrator.u&lt;/code&gt;. If we further set the initial value of that component to &lt;code&gt;1&lt;/code&gt;, then the neural network only has to learn that &lt;code&gt;du[3]&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;. In other words, the output of the neural network must be &lt;code&gt;0&lt;/code&gt; for all states &lt;code&gt;u&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Random.seed!(123)
nn1 = FastChain(FastDense(2, 64, tanh),FastDense(64, 1))
p_nn1 = initial_params(nn1)

function f1!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  du[3] = nn1(u[1:2], p)[1]
  return nothing
end

affect!(integrator) = integrator.u[2] += integrator.u[3]
cb = PresetTimeCallback(kicktimes,affect!,save_positions=(false,false))
z0 = Float32[u0;one(u0[1])]
prob1 = ODEProblem(f1!,z0,tspan,p_nn1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily compare the time evolution of the neural hybrid DE with respect to the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# to visualize the predictions of the trained neural network below
function visualize(prob,p)
  _prob = remake(prob,p=p)
  ode_pred = Array(solve(_prob,Tsit5(),callback=cb,
                 saveat=dtsave))[1:2,:]
  pl1 = plot(t_data,ode_pred[1,:],label=&amp;quot;x(t)&amp;quot;)
  scatter!(pl1,t_data[1:5:end],ode_data[1,1:5:end],label=&amp;quot;data x(t)&amp;quot;)
  pl2 = plot(t_data,ode_pred[2,:],label=&amp;quot;v(t)&amp;quot;)
  scatter!(pl2,t_data[1:5:end],ode_data[2,1:5:end],label=&amp;quot;data v(t)&amp;quot;)

  pl = plot(pl1, pl2, layout=(1,2), xlabel=&amp;quot;t&amp;quot;)
  return pl, sum(abs2,ode_data .- ode_pred)
end

pl = plot(solve(prob1,Tsit5(),saveat=t,
  callback=cb
  ),label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot; &amp;quot;u3(t)&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/untrained_nn.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/untrained_nn.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;which (of course) doesn&amp;rsquo;t match the data due to the random initialization of the neural network parameters before training. The neural network can be trained, i.e., its parameters can be optimized, by minimizing a mean-squared error loss function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;### loss function
function loss(p; prob=prob1, sensealg = ReverseDiffAdjoint())
  _prob = remake(prob,p=p)
  pred = Array(solve(_prob,Tsit5(),callback=cb,
               saveat=dtsave,sensealg=sensealg))[1:2,:]
  sum(abs2,ode_data .- pred)
end

loss(p_nn1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The recently implemented tools are deeply hidden within the 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiffEqSensitivity.jl&lt;/a&gt; package. However, while the user could previously only choose discrete sensitivities such as &lt;code&gt;ReverseDiffAdjoint()&lt;/code&gt; or  &lt;code&gt;ForwardDiffAdjoint()&lt;/code&gt; that rely on direct differentiation through the solver operations to get accurate gradients, one can now also select continuous adjoint sensitivity methods such as &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;,  &lt;code&gt;InterpolatingAdjoint()&lt;/code&gt;, and &lt;code&gt;QuadratureAdjoint()&lt;/code&gt; as the &lt;code&gt;sensealg&lt;/code&gt; for hybrid DEs. Each choice has its own characteristics in terms of stability, scaling with parameters, and memory consumption, see, e.g., 
&lt;a href=&#34;https://www.youtube.com/watch?v=XRJ-rtP2fVE&amp;amp;list=PLP8iPy9hna6TxktMt-IzdU2vQpGp3bwDn&amp;amp;index=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris&#39; talk&lt;/a&gt; at the SciML symposium at SIAM CSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;###################################
# training loop
# optimize the parameters for a few epochs with ADAM
function train(prob, p_nn; sensealg=BacksolveAdjoint())
  opt = ADAM(0.0003f0)
  list_plots = []
  losses = []
  for epoch in 1:200
    println(&amp;quot;epoch: $epoch / 200&amp;quot;)
    _dy, back = Zygote.pullback(p -&amp;gt; loss(p,
      prob=prob,
      sensealg=sensealg), p_nn)
    gs = @time back(one(_dy))[1]
    push!(losses, _dy)
    if epoch % 10 == 0
      # plot every xth epoch
      pl, test_loss = visualize(prob, p_nn)
      println(&amp;quot;Loss (epoch: $epoch): $test_loss&amp;quot;)
      display(pl)
      push!(list_plots, pl)
    end
    Flux.Optimise.update!(opt, p_nn, gs)
    println(&amp;quot;&amp;quot;)
  end
  return losses, list_plots
end

# plot training loss
losses, list_plots = train(prob1, p_nn1)
pl1 = plot(losses, lw = 1.5, xlabel = &amp;quot;epoch&amp;quot;, ylabel=&amp;quot;loss&amp;quot;, legend=false)
pl2 = list_plots[end]
pl3 = plot(solve(prob1,p=p_nn1,Tsit5(),saveat=t,
   callback=cb
  ), label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot; &amp;quot;u3(t)&amp;quot;])

pl = plot(pl2,pl3)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/trained1.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/trained1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We see the expected constant value of &lt;code&gt;u[3]&lt;/code&gt;, indicating a kick to the velocity of &lt;code&gt;+=1&lt;/code&gt;, at the kicking times over the full time interval.&lt;/p&gt;
&lt;h2 id=&#34;reducing-the-domain-knowledge&#34;&gt;Reducing the domain knowledge&lt;/h2&gt;
&lt;p&gt;If less physical information is included in the model design, the training becomes more difficult, e.g., due to 
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/local_minima/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;local minima&lt;/a&gt;. Possible modification for the kicked oscillator could be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;changing the initial condition of the third component of &lt;code&gt;u&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;using another affect function &lt;code&gt;affect!(integrator) = integrator.u[2] = integrator.u[3]&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;dropping the knowledge that only &lt;code&gt;u[2]&lt;/code&gt; gets a kick by using a neural network with &lt;code&gt;2&lt;/code&gt; outputs (+ a fourth component in the ODE):&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;affect2!(integrator) = integrator.u[1:2] = integrator.u[3:4]
function f2!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  du[3:4] .= nn2(u[1:2], nn_weights)
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fitting the parameters $a$ and $b$ simultaneously:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function f3!(du,u,p,t)
  a = p[end-1]
  b = p[end]
  nn_weights = p[1:end-2]

  du[1] = u[2]
  du[2] = -b*u[1] - a*u[2]
  du[3:4] .= nn2(u[1:2], nn_weights)
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;inferring the entire underlying dynamics using a neural network with &lt;code&gt;4&lt;/code&gt; outputs:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function f4!(du,u,p,t)
  Ω = nn3(u[1:2], p)

  du[1] = Ω[1]
  du[2] = Ω[2]
  du[3:4] .= Ω[3:4]
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;With respect to the adjoint sensitivity methods for hybrid DEs, we are planning to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;refine the adjoints in case of implicit discontinuities (&lt;code&gt;ContinuousCallbacks&lt;/code&gt;) and&lt;/li&gt;
&lt;li&gt;support direct usage through the 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/types/jump_types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jump problem&lt;/a&gt; interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in the upcoming weeks. For further information, the interested reader is encouraged to look at the open 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issues&lt;/a&gt; in the DiffEqSensitivity.jl package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Frank Schäfer, Pavel Sekatski, et al., Mach. Learn.: Sci. Technol. &lt;strong&gt;2&lt;/strong&gt;, 035004 (2021) &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel, arXiv preprint arXiv:2011.03902 (2020). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Junteng Jia, Austin R. Benson, arXiv preprint arXiv:1905.10403 (2019). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Michael Poli, Stefano Massaroli, et al., arXiv preprint arXiv:2106.04165 (2021). &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chris Rackauckas, Yingbo Ma, et al., &amp;ldquo;Accelerated predictive healthcare analytics with pumas, a high performance pharmaceutical modeling and simulation platform.&amp;rdquo; (2020). &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chris Rackauckas, Yingbo Ma, et al., arXiv preprint arXiv:2001.04385 (2020). &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
