<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hybrid differential equations | FS</title>
    <link>https://frankschae.github.io/tag/hybrid-differential-equations/</link>
      <atom:link href="https://frankschae.github.io/tag/hybrid-differential-equations/index.xml" rel="self" type="application/rss+xml" />
    <description>Hybrid differential equations</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Frank Schäfer</copyright><lastBuildDate>Fri, 16 Jul 2021 13:24:04 +0200</lastBuildDate>
    <image>
      <url>https://frankschae.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Hybrid differential equations</title>
      <link>https://frankschae.github.io/tag/hybrid-differential-equations/</link>
    </image>
    
    <item>
      <title>[WIP] Sensitivity Analysis of Hybrid Differential Equations</title>
      <link>https://frankschae.github.io/post/bouncing_ball/</link>
      <pubDate>Fri, 16 Jul 2021 13:24:04 +0200</pubDate>
      <guid>https://frankschae.github.io/post/bouncing_ball/</guid>
      <description>&lt;p&gt;In this post, we discuss sensitivity analysis of hybrid differential equations&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and highlight differences between explicit&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and implicit discontinuities&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. As a paradigmatic example, we consider a bouncing ball described by the ODE&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\text{d}z(t) &amp;amp;= v(t) \text{d}t, \\&lt;br&gt;
\text{d}v(t) &amp;amp;= -g  \text{d}t
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;with initial condition&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z(t=0) &amp;amp;= z_0 = 5, \\&lt;br&gt;
v(t=0) &amp;amp;= v_0 = -0.1.
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The initial condition contains the initial height $z_0$ and initial velocity $v_0$ of the ball. We have two important parameters in this system. First, there is the gravitational constant $g=10$ modeling the acceleration of the ball due to an approximately constant gravitational field. Second, we include a dissipation factor $\gamma=0.8$ (
&lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_restitution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coefficient of restitution&lt;/a&gt;) that accounts for a non-perfect elastic bounce on the ground.&lt;/p&gt;
&lt;p&gt;We can straightforwardly integrate the ODE analytically&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z(t) &amp;amp;= z_0 + v_0 t - \frac{g}{2} t^2, \\&lt;br&gt;
v(t) &amp;amp;= v_0 - g  t
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;or numerically using the OrdinaryDiffEq package from the 
&lt;a href=&#34;https://sciml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; ecosystem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using ForwardDiff, Zygote, OrdinaryDiffEq, DiffEqSensitivity
using Plots, LaTeXStrings

##
### simulate forward
function f(du,u,p,t)
  du[1] = u[2]
  du[2] = -p[1]
end

z0 = 5.0
v0 = -0.1
t0 = 0.0
tend = 1.9
g = 10
γ = 0.8

u0 = [z0,v0]
tspan = (t0,tend)
p = [g, γ]
prob = ODEProblem(f,u0,tspan,p)

# plot forward trajectory
sol = solve(prob,Tsit5(),saveat=0.1)
pl = plot(sol, label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomleft)
hline!(pl, [0.0], label=false, color=&amp;quot;black&amp;quot;)
savefig(pl,&amp;quot;BB_forward_no_bounce.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/BB_forward_no_bounce.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/BB_forward_no_bounce.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;forward-simulation-with-events&#34;&gt;Forward simulation with events&lt;/h2&gt;
&lt;p&gt;At around $t^\star \approx 1$, the ball hits the ground $z^\star(t^\star) = 0$, and is inelastically reflected while dissipating a fraction of its energy. This discontinuity can be modeled by re-initializing the ODE with new initial conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}&lt;br&gt;
z_+&amp;amp;= \lim_{t \rightarrow {t^\star}^+} z(t) =  \lim_{t \rightarrow {t^\star}^-}  z(t) = z_- ,\\&lt;br&gt;
v_+&amp;amp;= \lim_{t \rightarrow {t^\star}^+} v(t) =  -\gamma \lim_{t \rightarrow {t^\star}^-}  v(t) =  -\gamma v_-
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;on the right-hand side of the event. Given our analytical solution for the state as a function of time, we can easily compute the event time $t^\star$ as&lt;/p&gt;
&lt;p&gt;$$
t^\star = \frac{v_0 + \sqrt{v_0^2 + 2 g z_0}}{g}.
$$&lt;/p&gt;
&lt;h3 id=&#34;explicit-events&#34;&gt;Explicit events&lt;/h3&gt;
&lt;p&gt;We can define the bounce of the ball as an explicit event by inserting the values of the initial condition and the parameters into $t^\star$. We obtain&lt;/p&gt;
&lt;p&gt;$$
t^\star = 0.99005.
$$&lt;/p&gt;
&lt;p&gt;The full trajectory $z_{\rm exp}(t)$ is determined by&lt;/p&gt;
&lt;p&gt;$$
z_{\rm exp}(t) = \begin{cases}
z^{1}_{\rm exp}(t) &amp;amp;=z_0 + v_0 t - \frac{g}{2} t^2 &amp;amp;, \forall t \leq t^\star, \\&lt;br&gt;
z^{2}_{\rm exp}(t) &amp;amp;=-0.4901 g - 0.5 g (-0.99005 + t)^2 +\\&lt;br&gt;
&amp;amp;+0.99005 v_0 + z_0 - (-0.99005 + t) (-0.99005 g + v_0)\gamma  &amp;amp;, \forall t &amp;gt; t^\star,
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;where we used:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}&lt;br&gt;
z_{-, \rm exp}&amp;amp;= z_0 + 0.99005 v_0 -0.4901 g,\\&lt;br&gt;
v_{-, \rm exp}&amp;amp;= (v_0 - 0.99005 g) .
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}&lt;br&gt;
z_{+, \rm exp}&amp;amp;= z_0 + 0.99005 v_0 -0.4901 g, \\&lt;br&gt;
v_{+, \rm exp}&amp;amp;= -(v_0 - 0.99005 g) \gamma .
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Numerically, we use a &lt;code&gt;DiscreteCallback&lt;/code&gt; in this case to simulate the system&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# DiscreteCallback (explicit event)
tstar = (v0 + sqrt(v0^2+2*z0*g))/g
condition1(u,t,integrator) = (t == tstar)
affect!(integrator) = integrator.u[2] = -integrator.p[2]*integrator.u[2]
cb1 = DiscreteCallback(condition1,affect!,save_positions=(true,true))

sol1 = solve(prob,Tsit5(),callback=cb1, saveat=0.1, tstops=[tstar])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, by choosing an explicit definition of the event, the impact time is fixed. Thus, if we perturb the initial conditions or the parameters, the event location remains at $t^\star = 0.99005$ while it should actually change (for a fixed ground).&lt;/p&gt;
&lt;h3 id=&#34;implicit-events&#34;&gt;Implicit events&lt;/h3&gt;
&lt;p&gt;The physically more meaningful description of a bouncing ball is therefore given by an implicit description of the event in form of a condition (event function)&lt;/p&gt;
&lt;p&gt;$$
g(z,v,p,t) = z(t),
$$&lt;/p&gt;
&lt;p&gt;where an event occurs if $g(z^\star,v^\star,p,t^\star) = 0$. Note that we have already used this condition to define our impact time $t^\star$ of the explicit event.&lt;/p&gt;
&lt;p&gt;As in the previous case, we can analytically compute the full trajectory of the ball. At the event time, we have&lt;/p&gt;
&lt;p&gt;\begin{aligned}&lt;br&gt;
z_{-, \rm imp}&amp;amp;= 0, \\&lt;br&gt;
v_{-, \rm imp}&amp;amp;= - \sqrt{v_0^2 + 2 g z_0}
\end{aligned}&lt;/p&gt;
&lt;p&gt;for the left and&lt;/p&gt;
&lt;p&gt;\begin{aligned}&lt;br&gt;
z_{+, \rm imp}&amp;amp;= 0 \\&lt;br&gt;
v_{+, \rm imp}&amp;amp;= \gamma \sqrt{v_0^2 + 2 g z_0}.
\end{aligned}&lt;/p&gt;
&lt;p&gt;for the right limit. Thus, the full trajectory $z_{\rm imp}(t)$ is given by&lt;/p&gt;
&lt;p&gt;$$
z_{\rm imp}(t) = \begin{cases}
z^{1}_{\rm imp}(t) &amp;amp;=z_0 + v_0 t - \frac{g}{2} t^2 &amp;amp;, \forall t \leq t^\star ,\\&lt;br&gt;
z^{2}_{\rm imp}(t) &amp;amp;= -\frac{(-g t + v_0 + \sqrt{v_0^2 + 2 g z_0})}{2 g} \times, \\&lt;br&gt;
&amp;amp;\times (-g t + v_0 + \sqrt{v_0^2 + 2 g z_0} (1 + 2 \gamma)) &amp;amp;, \forall t &amp;gt; t^\star.
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Numerically, we use a &lt;code&gt;ContinuousCallback&lt;/code&gt; in this case.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# ContinuousCallback (implicit event)
condition2(u,t,integrator) = u[1] # Event when condition(u,t,integrator) == 0
cb2 = ContinuousCallback(condition2,affect!,save_positions=(true,true))
sol2 = solve(prob,Tsit5(),callback=cb2,saveat=0.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can verify that both callbacks lead to the same forward time evolution (for fixed initial conditions and parameters).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# plot forward trajectory
pl1 = plot(sol1, label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], title=&amp;quot;explicit event&amp;quot;, labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright)
pl2 = plot(sol2, label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], title=&amp;quot;implicit event&amp;quot;, labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright)
hline!(pl1, [0.0], label=false, color=&amp;quot;black&amp;quot;)
hline!(pl2, [0.0], label=false, color=&amp;quot;black&amp;quot;)
pl = plot(pl1,pl2)
savefig(pl,&amp;quot;BB_forward.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/BB_forward.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/BB_forward.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;In addition, the implicitly defined impact time via the &lt;code&gt;ContinuousCallback&lt;/code&gt; also shifts the impact time when changing the initial conditions or the parameters&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# animate forward trajectory
sol3 = solve(remake(prob,u0=[u0[1]+0.5,u0[2]]),Tsit5(),callback=cb2,saveat=0.01)

plt2 = plot(sol2, label = false, labelfontsize=20, legendfontsize=20, lw = 1, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, color=&amp;quot;black&amp;quot;, xlims=(t0,tend))
hline!(plt2, [0.0], label=false, color=&amp;quot;black&amp;quot;)
plot!(plt2, sol3, tspan=(t0,tend), color=[1 2], label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, denseplot=true, xlims=(t0,tend), ylims=(-11,9))
# scatter!(plt2, [t2,t2], sol3(t2), color=[1, 2], label=false)

list_plots = []
for t in sol3.t
  tstart = 0.0

  plt1 = plot(sol2, label = false, labelfontsize=20, legendfontsize=20, lw = 1, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, color=&amp;quot;black&amp;quot;)
  hline!(plt1, [0.0], label=false, color=&amp;quot;black&amp;quot;)
  plot!(plt1, sol3, tspan=(t0,t), color=[1 2], label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, denseplot=true, xlims=(t0,tend), ylims=(-11,9))
  scatter!(plt1,[t,t], sol3(t), color=[1, 2], label=false)
  plt = plot(plt1,plt2)
  push!(list_plots, plt)
end

plot(list_plots[100])

anim = animate(list_plots,every=1)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/BB.gif&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/BB.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The original curve is shown in black in the figure above. In other words, the event time $t^\star=t^\star(p,z_0,v_0,t_0)$ is a function of the parameters and initial conditions, and is implicitly defined by the event condition. Therefore, the sensitivity of the event time with respect to parameters $\frac{\text{d}t^\star}{\text{d}p}$ or initial conditions $\frac{\text{d}t^\star}{\text{d}z_0}$, $\frac{\text{d}t^\star}{\text{d}v_0}$ must be taken into account.&lt;/p&gt;
&lt;h2 id=&#34;sensitivity-analysis-with-events&#34;&gt;Sensitivity analysis with events&lt;/h2&gt;
&lt;p&gt;We are often interested in computing the change of a loss function with respect to changes of the parameters or initial condition. For this purpose, let us first consider the mean square error loss function&lt;/p&gt;
&lt;p&gt;$$
L(z,y) = \sum_i(z(t_i) - y_i)^2
$$&lt;/p&gt;
&lt;p&gt;with respect to target values $y_i$ at time points $t_i$ incident before, after, or at the event time. Let $\alpha$ denote any of the inputs $(z_0,v_0,g,\gamma)$. The sensitivity with respect to $\alpha$ is then given by the chain rule:&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}L}{\text{d} \alpha} =  2\sum_i (z(t_i) - y_i) \frac{\text{d}z(t_i)}{\text{d} \alpha}.
$$&lt;/p&gt;
&lt;p&gt;For the bouncing ball, we can easily compute those sensitivities by inserting our results for $z_{\rm imp}(t_i)$ and $z_{\rm exp}(t_i)$. One can verify that the sensitivities are different in the two cases, as expected.&lt;/p&gt;
&lt;p&gt;However, in most systems, we won&amp;rsquo;t be able to solve analytically a differential equation&lt;/p&gt;
&lt;p&gt;$$
\text{d}x(t) = f(x,p,t) \text{d}t
$$&lt;/p&gt;
&lt;p&gt;with initial condition $x_0=x(t_0)$. Instead, we have to numerically solve for the state $x(t)$. Regarding the computation of the sensitivities, we may then choose one of the 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/analysis/sensitivity/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available algorithms&lt;/a&gt; for the given differential equation. Currently, &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;, &lt;code&gt;InterpolatingAdjoint()&lt;/code&gt;, &lt;code&gt;QuadratureAdjoint()&lt;/code&gt;, &lt;code&gt;ReverseDiffAdjoint()&lt;/code&gt;, &lt;code&gt;TrackerAdjoint()&lt;/code&gt;, and &lt;code&gt;ForwardDiffAdjoint()&lt;/code&gt; are compatible events in ordinary differential equations. We write the loss function in the following as a function of time, state, and parameters&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
L = L(t,x,p).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;In the following, let us focus on the &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; algorithm, which computes the sensitivities&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}L}{\text{d}x(t_{0})} &amp;amp;= \lambda(t_{0}),\\&lt;br&gt;
\frac{\text{d}L}{\text{d}p} &amp;amp;= \lambda_{p}(t_{0}),
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;with respect to the initial state and the parameters, by solving an ODE for $\lambda(t)$ in reverse time from $t_N$ to $t_0$&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}\lambda(t)}{\text{d}t} &amp;amp;= -\lambda(t)^\dagger \frac{\text{d} f(\rightarrow x(t), p, t)}{\text{d} x(t)} - \frac{\text{d} L(t, \rightarrow x(t), p)}{\text{d} x(t)}^\dagger \delta(t-t_i), \\&lt;br&gt;
\frac{\text{d}\lambda_{p}(t)}{\text{d}t} &amp;amp;= -\lambda(t)^\dagger \frac{\text{d} f(x(t), \rightarrow p, t)}{\text{d} p},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;with initial conditions:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda(t_{N})&amp;amp;= 0, \\&lt;br&gt;
\lambda_{p}(t_{N}) &amp;amp;= 0.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The arrows indicate the variable with respect to which we differentiate. Note that computing the vector-Jacobian products (vjp) in the adjoint ODE requires the value of $x(t)$ along its trajectory. In &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;, we recompute $x(t)$&amp;ndash;together with the adjoint variables&amp;ndash;backwards in time starting with its final value $x(t_N)$. A derivation of the ODE adjoint is given in 
&lt;a href=&#34;https://mitmath.github.io/18337/lecture11/adjoints&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris&#39; MIT 18.337 lecture notes&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;explicit-events-1&#34;&gt;Explicit events&lt;/h3&gt;
&lt;p&gt;To make &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; compatible with explicit events&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, we have to store the event times $t^\star_j$ as well as the state $x({t_j^\star}^-)$ and parameters $p=p({t_j^\star}^-)$ (if they are changed) at the left limit of $t^\star_j$. We then solve the adjoint ODE backwards in time between the events. As soon as we reach an event from the right limit ${t_j^\star}^+$, we update the augmented state according to&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda({t_j^\star}^-) &amp;amp;= \lambda({t_j^\star}^+)^\dagger \frac{\text{d} a(\rightarrow x({t_j^\star}^-), p({t_j^\star}^-), {t_j^\star}^-)}{\text{d} x({t_j^\star}^-)} \\&lt;br&gt;
\lambda_p({t_j^\star}^-) &amp;amp;= \lambda_p({t_j^\star}^+) -  \lambda({t_j^\star}^+)^\dagger \frac{\text{d} a(x({t_j^\star}^-), \rightarrow p({t_j^\star}^-), {t_j^\star}^-)}{\text{d} p({t_j^\star}^-)}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $a$ is the affect function applied at the discontinuity. That is, to lift the adjoint from the right to the left limit, we compute a vjp with the adjoint $\lambda({t_j^\star}^+)$ at the right limit and the Jacobian of the affect function evaluated on the left limit.&lt;/p&gt;
&lt;p&gt;In particular, we apply a loss function callback before and after this update if the state was saved in the forward evolution and entered directly into the loss function.&lt;/p&gt;
&lt;h3 id=&#34;implicit-events-1&#34;&gt;Implicit events&lt;/h3&gt;
&lt;h4 id=&#34;special-case-event-as-termination-condition&#34;&gt;special case: event as termination condition&lt;/h4&gt;
&lt;p&gt;Define $u(t) = (t, x(t))$. Let us first re-derive the case, where the implicit event terminates the ODE and where we have a loss function acting on $t^\star_1$, $x(t^\star_1)$, and $p$, as considered by Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel in their ICLR 2021 paper&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. We are interested in&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}L(u(t^\star_1(\rightarrow p), \rightarrow p), \rightarrow p)}{\text{d}p} = \frac{\text{d}L(t^\star_1({\color{black}\rightarrow} p), \text{solve}(t_0, x_0, t^\star_1({\color{black}\rightarrow}p), \rightarrow p),\rightarrow p)}{\text{d}p},
$$&lt;/p&gt;
&lt;p&gt;which indicates that changing $p$ changes both $t^\star_1$ as well as $x^\star_1$ in $t^\star_1$.&lt;/p&gt;
&lt;p&gt;In a first step, we need to compute the sensitivity of $t^\star_1(p)$ with respect to $p$ and $x_0$ based on the event condition $F(t, p) = g(u(t, p)) = 0$.  We can apply the 
&lt;a href=&#34;https://www.uni-siegen.de/fb6/analysis/overhagen/vorlesungsbeschreibungen/skripte/analysis3_1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implicit function theorem&lt;/a&gt; which yields:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}t^\star_1(p)}{\text{d}p} &amp;amp;= - \left(\frac{\text{d}g(\rightarrow t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1}\right)^{-1} \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, t^\star_1, \rightarrow p))}{\text{d}p} .\\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The total derivative&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; inside the bracket is defined as:
$$
\begin{aligned}
\frac{\text{d}g}{\text{d}t^\star_1} \stackrel{\text{def}}{=} \frac{\text{d}g(\rightarrow t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1} &amp;amp;= \frac{\text{d}g(\rightarrow t^\star_1, \text{solve}(t_0, x_0, t^\star_1, p))}{\text{d}t^\star_1} + \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1}\\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Since&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}(\text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1} = f(x^\star, p, t^\star_1)
$$&lt;/p&gt;
&lt;p&gt;by definition of the ODE, we can write&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1} = \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d} u^\star(t^\star_1)}  f(x^\star, p, t^\star_1).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Furthermore, we have
$$
\begin{aligned}
\frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, t^\star_1, \rightarrow p))}{\text{d}p} = \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, t^\star_1,\rightarrow p))}{\text{d} u^\star(t^\star_1)}^\dagger  \frac{\text{d}\text{ solve}(t_0, x_0, t^\star_1,\rightarrow p))}{\text{d}p}
\end{aligned}
$$
for the second term of $\frac{\text{d}t^\star_1(p)}{\text{d}p}$.&lt;/p&gt;
&lt;p&gt;We can now write the gradient as:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}L(t^\star_1({\color{black}\rightarrow} p), \text{solve}(t_0, x_0, t^\star_1({\color{black}\rightarrow}p), \rightarrow p),\rightarrow p)}{\text{d}p} &amp;amp;= \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  p), \rightarrow p)}{\text{d}p} \\&lt;br&gt;
+&amp;amp; \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  \rightarrow p), p)}{\text{d}p} \\&lt;br&gt;
+&amp;amp; \frac{\text{d}L(\rightarrow t^\star_1(p), \text{solve}(t_0, x_0, \rightarrow t^\star_1(p),  p), p)}{\text{d}t^\star_1} \frac{\text{d} t^\star_1(p)}{\text{d}p},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which, after insertion of our results above, can be casted into the form:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}L(t^\star_1({\color{black}\rightarrow} p), \text{solve}(t_0, x_0, t^\star_1({\color{black}\rightarrow}p), \rightarrow p),\rightarrow p)}{\text{d}p} &amp;amp;= v^\dagger \frac{\text{d}\text{ solve}(t_0, x_0, t^\star_1(p), \rightarrow p)}{\text{d}p} \\&lt;br&gt;
&amp;amp;+ \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p), p), \rightarrow p)}{\text{d}p},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
v &amp;amp;= \xi \left(-\frac{\text{d}g}{\text{d}t^\star_1}\right)^{-1} \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, t^\star_1,\rightarrow p))}{\text{d} u^\star(t^\star_1)} + \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  p), p)}{\text{d} u^\star(t^\star_1)},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where we introduced the scalar pre-factor&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\xi = \left( \frac{\text{d}L(\rightarrow t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  p), p)}{\text{d}t^\star_1} +  \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  p), p)}{\text{d} u^\star(t^\star_1)}^\dagger f(x^\star, p, t^\star_1)\right).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;This means that if we terminate the ODE integration by an implicit event, we compute the sensitivities as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;use an ODE solver to solve forward until the event is triggered
$$
u_i = \text{solve}(t_0, x_0, t^\star_1(p),  p).
$$
$u_i(t_i)=(t_i,x_i)$ are the stored values which enter the loss function.&lt;/li&gt;
&lt;li&gt;compute the loss function gradient with respect to the state at $t^\star_1$
$$
\lambda_0 = \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  p), p)}{\text{d} u^\star(t^\star_1)}.
$$&lt;/li&gt;
&lt;li&gt;(instead of using the &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; algorithm with $\lambda_0$ directly,) compute the terms:
$$
\frac{\text{d}g}{\text{d}t^\star_1} = ,
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, t^\star_1,\rightarrow p))}{\text{d} u^\star(t^\star_1)} = ,
$$
and
$$\xi_{\text{left}} = $$.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;compute the correction factor:
$$
\begin{aligned}
\lambda_{\text{left}} =
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;use&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\lambda_{\text{imp}} =
$$&lt;/p&gt;
&lt;p&gt;as initial condition to $\text{backsolve_adjoint}(\lambda_{\text{imp}}, t^\star_1, x(t^\star_1), t_0)$ which backpropagates the adjoint $\lambda_{\text{imp}}$ from $t^\star_1$ to $t_0$.&lt;/p&gt;
&lt;p&gt;If there is an additional affect function associated with the event, we must additionally compute&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda_{\text{right}} = ,\\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;such that&lt;/p&gt;
&lt;p&gt;$$
\lambda_{\text{imp}} =
$$&lt;/p&gt;
&lt;p&gt;$$
\color{red} \text{continue here&amp;hellip;}
$$&lt;/p&gt;
&lt;h4 id=&#34;generalization-several-events&#34;&gt;generalization: several events&lt;/h4&gt;
&lt;p&gt;As pointed out by Chen et al. as well as by Timo C. Wunderlich and Christian Pehle&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, one can chain together the events and differentiate through the entire time evolution on a time interval $(t_0, t_{\text{end}})$. That is, we are generally allowed to segment the time evolution over an interval $[t_0, t]$ into one from $[t_0, s]$ and a subsequent one from $[s, t]$:&lt;/p&gt;
&lt;p&gt;$$
\text{solve}(t_0, x_0, t, p)  = \text{solve}(s, \text{solve}(t_0, x_0, s, p), t-s, p),
$$&lt;/p&gt;
&lt;p&gt;such that also loss function contributions are chained. Therefore, we have the following modification:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;segment the trajectory at the event times. Use $\text{backsolve_adjoint}(\lambda_{0}, t_\text{end}, x(t_\text{end}), t^\star_N)$ to backprogagate the loss function gradient from the final state until the *right* limit of the last event location.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;compute the vjp with respect to the affect function to obtain the adjoint at the left limit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;correct $\lambda$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\color{red} \text{write general BacksolveAdjoint equations&amp;hellip;}
$$&lt;/p&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;We are still refining the adjoints in case of implicit discontinuities (&lt;code&gt;ContinuousCallbacks&lt;/code&gt;). For further information, the interested reader is encouraged to track the associated issues 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues/383&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#383&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues/374&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#374&lt;/a&gt;, and 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/445&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PR #445&lt;/a&gt; in the DiffEqSensitivity.jl package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Michael Poli, Stefano Massaroli, et al., arXiv preprint arXiv:2106.04165 (2021). &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Junteng Jia, Austin R. Benson, arXiv preprint arXiv:1905.10403 (2019). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Timo C. Wunderlich and Christian Pehle, Sci. Rep. &lt;em&gt;11&lt;/em&gt;, 12829 (2021). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel, arXiv preprint arXiv:2011.03902 (2020). &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For a function $f$ of more than one variable $y = f(t, x_1(t),x_2(t),\dots,x_N(t))$, the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Differential_of_a_function#Differentials_in_several_variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;total derivative&lt;/a&gt; with respect to the independent variable $t$ is given by the sum of all partial derivatives
$$
\begin{aligned}
\frac{\text{d}y}{\text{d}t} &amp;amp;= \frac{\text{d}f(\rightarrow t, x_1(\rightarrow t),x_2(\rightarrow t),\dots,x_N(\rightarrow t))}{\text{d}t} \\&lt;br&gt;
&amp;amp;= \frac{\text{d}f(\rightarrow t, x_1(t),x_2(t),\dots,x_N(t))}{\text{d}t} + \frac{\text{d}f(t, x_1(\rightarrow t),x_2(t),\dots,x_N(t))}{\text{d}t}\\&lt;br&gt;
&amp;amp;+ \frac{\text{d}f(t, x_1(t),x_2(\rightarrow t),\dots,x_N(t))}{\text{d}t} + \dots +  \frac{\text{d}f(t, x_1(t),x_2(t),\dots,x_N(\rightarrow t))}{\text{d}t}.
\end{aligned}
$$ &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Neural Hybrid Differential Equations</title>
      <link>https://frankschae.github.io/post/hybridde/</link>
      <pubDate>Wed, 16 Jun 2021 14:50:17 +0200</pubDate>
      <guid>https://frankschae.github.io/post/hybridde/</guid>
      <description>&lt;p&gt;I am delighted that I have been awarded my second GSoC stipend this year.  I look forward to carrying out the ambitious project scope with my mentors 
&lt;a href=&#34;https://github.com/ChrisRackauckas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris Rackauckas&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mschauer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moritz Schauer&lt;/a&gt;,  
&lt;a href=&#34;https://github.com/YingboMa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yingbo Ma&lt;/a&gt;, and 
&lt;a href=&#34;https://github.com/mohamed82008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mohamed Tarek&lt;/a&gt;. This year&amp;rsquo;s project is embedded within the 
&lt;a href=&#34;https://summerofcode.withgoogle.com/organizations/5765643267211264/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NumFocus&lt;/a&gt;/
&lt;a href=&#34;https://sciml.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; organization and comprises adjoint sensitivity methods for discontinuities, shadowing methods for chaotic dynamics, symbolically generated adjoint methods, and further AD tooling within the Julia Language.&lt;/p&gt;
&lt;p&gt;This first post aims to illustrate our new (adjoint) sensitivity analysis tools with respect to event handling in (ordinary) differential equations (DEs).&lt;/p&gt;
&lt;h2 id=&#34;hybrid-differential-equations&#34;&gt;Hybrid Differential Equations&lt;/h2&gt;
&lt;p&gt;DEs with additional explicit or implicit discontinuities are called hybrid DEs. Within the SciML software suite, such discontinuities may be incorporated into DE models by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;callbacks&lt;/a&gt;. Evidently, the incorporation of discontinuities allows a user to specify changes (&lt;em&gt;events&lt;/em&gt;) in the system, i.e., changes of the state or the parameters of the DE, which cannot be modeled by a plain ordinary DE. While explicit events can be described by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/#DiscreteCallback-Examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiscreteCallbacks&lt;/a&gt;, implicit events have to be specified by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/#ContinuousCallback-Examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ContinuousCallbacks&lt;/a&gt;. That is, explicit events possess explicit event times, while implicit events are triggered when a continuous function evaluates to &lt;code&gt;0&lt;/code&gt;. Thus, implicit events require some sort of rootfinding procedure.&lt;/p&gt;
&lt;p&gt;Some relevant examples for hybrid DEs with discrete or continuous callbacks are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;quantum optics experiments, where photon-counting measurements lead to jumps in the quantum state that occur with a variable rate, see for instance Appendix A in Ref.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; (&lt;code&gt;ContinuousCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;a bouncing ball&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; (&lt;code&gt;ContinuousCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;classical point process models, such as a Poisson process&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;digital controllers&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, where a continuous system dynamics is controlled by a discrete-time controller (&lt;code&gt;DiscreteCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;pharmacokinetic models&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, where explicit dosing times change the drug concentration in the blood (&lt;code&gt;DiscreteCallback&lt;/code&gt;). The simplest possible example being the one-compartment model.&lt;/li&gt;
&lt;li&gt;kicked oscillator dynamics, e.g., a harmonic oscillator that gets a kick at some time points (&lt;code&gt;DiscreteCallback&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The associated sensitivity methods that allow us to differentiate through the respective hybrid DE systems have been recently introduced in Refs. &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kicked-harmonic-oscillator&#34;&gt;Kicked harmonic oscillator&lt;/h2&gt;
&lt;p&gt;Let us consider the simple physical model of a damped harmonic oscillator, described by an ODE of the form&lt;/p&gt;
&lt;p&gt;$$
\ddot{x}(t) + a\cdot\dot{x}(t) + b \cdot x(t) = 0 ,
$$&lt;/p&gt;
&lt;p&gt;where $a=0.1$ and $b=1$ with initial conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
x(t=0) &amp;amp;= 1  \\&lt;br&gt;
v(t=0) &amp;amp;= \dot{x}(t=0) = 0.
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This second-order ODE can be 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_differential_equation#Reduction_of_order&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reduced&lt;/a&gt; to two first-order ODEs, such that we can straightforwardly simulate the resulting ODE with the &lt;code&gt;DifferentialEquations.jl&lt;/code&gt; package. (Instead of doing this reduction manually, we could also use 
&lt;a href=&#34;https://mtk.sciml.ai/stable/tutorials/higher_order/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ModelingToolkit.jl&lt;/code&gt;&lt;/a&gt; to transform the ODE in an automatic manner. Alternatively, for second-order ODEs, there is also a &lt;code&gt;SecondOrderODEProblem&lt;/code&gt; implemented.) The Julia code reads:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using DiffEqFlux, DifferentialEquations, Flux, Optim, Plots, DiffEqSensitivity
using Zygote
using Random
u0 = Float32[1.; 0.]

tspan = (0.0f0,50.0f0)

dtsave = 0.5f0
t = tspan[1]:dtsave:tspan[2]

function oscillator!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  return nothing
end

prob_data = ODEProblem(oscillator!,u0,tspan)

# ODE without kicks
pl = plot(solve(prob_data,Tsit5(),saveat=t), label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/forward_damped_oscillator_no_kicks.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/forward_damped_oscillator_no_kicks.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We now include a kick to the velocity of the oscillator at regular time steps. Here, we choose both the time difference between the kicks and the increase in velocity as &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;kicktimes = tspan[1]:1:tspan[2]
function kick!(integrator)
  integrator.u[end] += one(eltype(integrator.u))
end
cb_ = PresetTimeCallback(kicktimes,kick!,save_positions=(false,false))

sol_data = solve(prob_data,Tsit5(),callback=cb_,saveat=t)
t_data = sol_data.t
ode_data = Array(sol_data)

# visualize data
pl1 = plot(t_data,ode_data[1,:],label=&amp;quot;data x(t)&amp;quot;)
plot!(pl1,t_data,ode_data[2,:],label=&amp;quot;data v(t)&amp;quot;)

pl2 = plot(t_data[1:20],ode_data[1,1:20],label=&amp;quot;data x(t)&amp;quot;)
plot!(pl2,t_data[1:20],ode_data[2,1:20],label=&amp;quot;data v(t)&amp;quot;)
pl = plot(pl2, pl1, layout=(1,2), xlabel=&amp;quot;t&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/forward_damped_oscillator.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/forward_damped_oscillator.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

The left-hand side shows a zoom for short times to better resolve the kicks. Note that by setting &lt;code&gt;save_positions=(true,true)&lt;/code&gt;, the kicks would be saved before &lt;strong&gt;and&lt;/strong&gt; after the event such that the kicks would appear completely vertically in the plot. The data on the right-hand will be used as training data below. In the spirit of universal differential equations&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;, we now aim at learning (potentially) missing parts of the model from these data traces.&lt;/p&gt;
&lt;h3 id=&#34;high-domain-knowledge&#34;&gt;High domain knowledge&lt;/h3&gt;
&lt;p&gt;For simplicity, we assume that we have almost perfect knowledge about our system. That is, we assume to know the basic structure of the ODE, including its parameters $a$ and $b$, and that the &lt;code&gt;affect!&lt;/code&gt; function of the event only acts on the velocity. We then encode the affect as an additional component to the ODE. The task is thus to learn the dynamics of the third component of &lt;code&gt;integrator.u&lt;/code&gt;. If we further set the initial value of that component to &lt;code&gt;1&lt;/code&gt;, then the neural network only has to learn that &lt;code&gt;du[3]&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;. In other words, the output of the neural network must be &lt;code&gt;0&lt;/code&gt; for all states &lt;code&gt;u&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Random.seed!(123)
nn1 = FastChain(FastDense(2, 64, tanh),FastDense(64, 1))
p_nn1 = initial_params(nn1)

function f1!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  du[3] = nn1(u[1:2], p)[1]
  return nothing
end

affect!(integrator) = integrator.u[2] += integrator.u[3]
cb = PresetTimeCallback(kicktimes,affect!,save_positions=(false,false))
z0 = Float32[u0;one(u0[1])]
prob1 = ODEProblem(f1!,z0,tspan,p_nn1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily compare the time evolution of the neural hybrid DE with respect to the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# to visualize the predictions of the trained neural network below
function visualize(prob,p)
  _prob = remake(prob,p=p)
  ode_pred = Array(solve(_prob,Tsit5(),callback=cb,
                 saveat=dtsave))[1:2,:]
  pl1 = plot(t_data,ode_pred[1,:],label=&amp;quot;x(t)&amp;quot;)
  scatter!(pl1,t_data[1:5:end],ode_data[1,1:5:end],label=&amp;quot;data x(t)&amp;quot;)
  pl2 = plot(t_data,ode_pred[2,:],label=&amp;quot;v(t)&amp;quot;)
  scatter!(pl2,t_data[1:5:end],ode_data[2,1:5:end],label=&amp;quot;data v(t)&amp;quot;)

  pl = plot(pl1, pl2, layout=(1,2), xlabel=&amp;quot;t&amp;quot;)
  return pl, sum(abs2,ode_data .- ode_pred)
end

pl = plot(solve(prob1,Tsit5(),saveat=t,
  callback=cb
  ),label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot; &amp;quot;u3(t)&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/untrained_nn.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/untrained_nn.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;which (of course) doesn&amp;rsquo;t match the data due to the random initialization of the neural network parameters before training. The neural network can be trained, i.e., its parameters can be optimized, by minimizing a mean-squared error loss function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;### loss function
function loss(p; prob=prob1, sensealg = ReverseDiffAdjoint())
  _prob = remake(prob,p=p)
  pred = Array(solve(_prob,Tsit5(),callback=cb,
               saveat=dtsave,sensealg=sensealg))[1:2,:]
  sum(abs2,ode_data .- pred)
end

loss(p_nn1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The recently implemented tools are deeply hidden within the 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiffEqSensitivity.jl&lt;/a&gt; package. However, while the user could previously only choose discrete sensitivities such as &lt;code&gt;ReverseDiffAdjoint()&lt;/code&gt; or  &lt;code&gt;ForwardDiffAdjoint()&lt;/code&gt; that rely on direct differentiation through the solver operations to get accurate gradients, one can now also select continuous adjoint sensitivity methods such as &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;,  &lt;code&gt;InterpolatingAdjoint()&lt;/code&gt;, and &lt;code&gt;QuadratureAdjoint()&lt;/code&gt; as the &lt;code&gt;sensealg&lt;/code&gt; for hybrid DEs. Each choice has its own characteristics in terms of stability, scaling with parameters, and memory consumption, see, e.g., 
&lt;a href=&#34;https://www.youtube.com/watch?v=XRJ-rtP2fVE&amp;amp;list=PLP8iPy9hna6TxktMt-IzdU2vQpGp3bwDn&amp;amp;index=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris&#39; talk&lt;/a&gt; at the SciML symposium at SIAM CSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;###################################
# training loop
# optimize the parameters for a few epochs with ADAM
function train(prob, p_nn; sensealg=BacksolveAdjoint())
  opt = ADAM(0.0003f0)
  list_plots = []
  losses = []
  for epoch in 1:200
    println(&amp;quot;epoch: $epoch / 200&amp;quot;)
    _dy, back = Zygote.pullback(p -&amp;gt; loss(p,
      prob=prob,
      sensealg=sensealg), p_nn)
    gs = @time back(one(_dy))[1]
    push!(losses, _dy)
    if epoch % 10 == 0
      # plot every xth epoch
      pl, test_loss = visualize(prob, p_nn)
      println(&amp;quot;Loss (epoch: $epoch): $test_loss&amp;quot;)
      display(pl)
      push!(list_plots, pl)
    end
    Flux.Optimise.update!(opt, p_nn, gs)
    println(&amp;quot;&amp;quot;)
  end
  return losses, list_plots
end

# plot training loss
losses, list_plots = train(prob1, p_nn1)
pl1 = plot(losses, lw = 1.5, xlabel = &amp;quot;epoch&amp;quot;, ylabel=&amp;quot;loss&amp;quot;, legend=false)
pl2 = list_plots[end]
pl3 = plot(solve(prob1,p=p_nn1,Tsit5(),saveat=t,
   callback=cb
  ), label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot; &amp;quot;u3(t)&amp;quot;])

pl = plot(pl2,pl3)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/trained1.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/trained1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We see the expected constant value of &lt;code&gt;u[3]&lt;/code&gt;, indicating a kick to the velocity of &lt;code&gt;+=1&lt;/code&gt;, at the kicking times over the full time interval.&lt;/p&gt;
&lt;h2 id=&#34;reducing-the-domain-knowledge&#34;&gt;Reducing the domain knowledge&lt;/h2&gt;
&lt;p&gt;If less physical information is included in the model design, the training becomes more difficult, e.g., due to 
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/local_minima/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;local minima&lt;/a&gt;. Possible modification for the kicked oscillator could be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;changing the initial condition of the third component of &lt;code&gt;u&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;using another affect function &lt;code&gt;affect!(integrator) = integrator.u[2] = integrator.u[3]&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;dropping the knowledge that only &lt;code&gt;u[2]&lt;/code&gt; gets a kick by using a neural network with &lt;code&gt;2&lt;/code&gt; outputs (+ a fourth component in the ODE):&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;affect2!(integrator) = integrator.u[1:2] = integrator.u[3:4]
function f2!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  du[3:4] .= nn2(u[1:2], nn_weights)
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fitting the parameters $a$ and $b$ simultaneously:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function f3!(du,u,p,t)
  a = p[end-1]
  b = p[end]
  nn_weights = p[1:end-2]

  du[1] = u[2]
  du[2] = -b*u[1] - a*u[2]
  du[3:4] .= nn2(u[1:2], nn_weights)
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;inferring the entire underlying dynamics using a neural network with &lt;code&gt;4&lt;/code&gt; outputs:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function f4!(du,u,p,t)
  Ω = nn3(u[1:2], p)

  du[1] = Ω[1]
  du[2] = Ω[2]
  du[3:4] .= Ω[3:4]
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;With respect to the adjoint sensitivity methods for hybrid DEs, we are planning to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;refine the adjoints in case of implicit discontinuities (&lt;code&gt;ContinuousCallbacks&lt;/code&gt;) and&lt;/li&gt;
&lt;li&gt;support direct usage through the 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/types/jump_types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jump problem&lt;/a&gt; interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in the upcoming weeks. For further information, the interested reader is encouraged to look at the open 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issues&lt;/a&gt; in the DiffEqSensitivity.jl package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Frank Schäfer, Pavel Sekatski, et al., Mach. Learn.: Sci. Technol. &lt;strong&gt;2&lt;/strong&gt;, 035004 (2021) &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel, arXiv preprint arXiv:2011.03902 (2020). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Junteng Jia, Austin R. Benson, arXiv preprint arXiv:1905.10403 (2019). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Michael Poli, Stefano Massaroli, et al., arXiv preprint arXiv:2106.04165 (2021). &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chris Rackauckas, Yingbo Ma, et al., &amp;ldquo;Accelerated predictive healthcare analytics with pumas, a high performance pharmaceutical modeling and simulation platform.&amp;rdquo; (2020). &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chris Rackauckas, Yingbo Ma, et al., arXiv preprint arXiv:2001.04385 (2020). &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
