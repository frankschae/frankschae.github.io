<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adjoint sensitivity methods | FS</title>
    <link>https://frankschae.github.io/tag/adjoint-sensitivity-methods/</link>
      <atom:link href="https://frankschae.github.io/tag/adjoint-sensitivity-methods/index.xml" rel="self" type="application/rss+xml" />
    <description>Adjoint sensitivity methods</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Frank Schäfer</copyright><lastBuildDate>Fri, 16 Jul 2021 13:24:04 +0200</lastBuildDate>
    <image>
      <url>https://frankschae.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Adjoint sensitivity methods</title>
      <link>https://frankschae.github.io/tag/adjoint-sensitivity-methods/</link>
    </image>
    
    <item>
      <title>Sensitivity Analysis of Hybrid Differential Equations</title>
      <link>https://frankschae.github.io/post/bouncing_ball/</link>
      <pubDate>Fri, 16 Jul 2021 13:24:04 +0200</pubDate>
      <guid>https://frankschae.github.io/post/bouncing_ball/</guid>
      <description>&lt;p&gt;In this post, we discuss sensitivity analysis of hybrid differential equations&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and highlight differences between explicit&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and implicit discontinuities&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. As a paradigmatic example, we consider a bouncing ball described by the ODE&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\text{d}z(t) &amp;amp;= v(t) \text{d}t, \\&lt;br&gt;
\text{d}v(t) &amp;amp;= -g  \text{d}t
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;with initial condition&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z(t=0) &amp;amp;= z_0 = 5, \\&lt;br&gt;
v(t=0) &amp;amp;= v_0 = -0.1.
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The initial condition contains the initial height $z_0$ and initial velocity $v_0$ of the ball. We have two important parameters in this system. First, there is the gravitational constant $g=10$ modeling the acceleration of the ball due to an approximately constant gravitational field. Second, we include a dissipation factor $\gamma=0.8$ (
&lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_restitution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coefficient of restitution&lt;/a&gt;) that accounts for a non-perfect elastic bounce on the ground.&lt;/p&gt;
&lt;p&gt;We can straightforwardly integrate the ODE analytically&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z(t) &amp;amp;= z_0 + v_0 t - \frac{g}{2} t^2, \\&lt;br&gt;
v(t) &amp;amp;= v_0 - g  t
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;or numerically using the OrdinaryDiffEq package from the 
&lt;a href=&#34;https://sciml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; ecosystem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using ForwardDiff, Zygote, OrdinaryDiffEq, DiffEqSensitivity
using Plots, LaTeXStrings

##
### simulate forward
function f(du,u,p,t)
  du[1] = u[2]
  du[2] = -p[1]
end

z0 = 5.0
v0 = -0.1
t0 = 0.0
tend = 1.9
g = 10
γ = 0.8

u0 = [z0,v0]
tspan = (t0,tend)
p = [g, γ]
prob = ODEProblem(f,u0,tspan,p)

# plot forward trajectory
sol = solve(prob,Tsit5(),saveat=0.1)
pl = plot(sol, label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomleft)
hline!(pl, [0.0], label=false, color=&amp;quot;black&amp;quot;)
savefig(pl,&amp;quot;BB_forward_no_bounce.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/BB_forward_no_bounce.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/BB_forward_no_bounce.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;forward-simulation-with-events&#34;&gt;Forward simulation with events&lt;/h2&gt;
&lt;p&gt;At around $t^\star \approx 1$, the ball hits the ground $z^\star(t^\star) = 0$, and is inelastically reflected while dissipating a fraction of its energy. This discontinuity can be modeled by re-initializing the ODE with new initial conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}&lt;br&gt;
z_+&amp;amp;= \lim_{t \rightarrow {t^\star}^+} z(t) =  \lim_{t \rightarrow {t^\star}^-}  z(t) = z_- ,\\&lt;br&gt;
v_+&amp;amp;= \lim_{t \rightarrow {t^\star}^+} v(t) =  -\gamma \lim_{t \rightarrow {t^\star}^-}  v(t) =  -\gamma v_-
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;on the right-hand side of the event. Given our analytical solution for the state as a function of time, we can easily compute the event time $t^\star$ as&lt;/p&gt;
&lt;p&gt;$$
t^\star = \frac{v_0 + \sqrt{v_0^2 + 2 g z_0}}{g}.
$$&lt;/p&gt;
&lt;h3 id=&#34;explicit-events&#34;&gt;Explicit events&lt;/h3&gt;
&lt;p&gt;We can define the bounce of the ball as an explicit event by inserting the values of the initial condition and the parameters into $t^\star$. We obtain&lt;/p&gt;
&lt;p&gt;$$
t^\star = 0.99005.
$$&lt;/p&gt;
&lt;p&gt;The full trajectory $z_{\rm exp}(t)$ is determined by&lt;/p&gt;
&lt;p&gt;$$
z_{\rm exp}(t) = \begin{cases}
z^{1}_{\rm exp}(t) &amp;amp;=z_0 + v_0 t - \frac{g}{2} t^2 &amp;amp;, \forall t \leq t^\star, \\&lt;br&gt;
z^{2}_{\rm exp}(t) &amp;amp;=-0.4901 g - 0.5 g (-0.99005 + t)^2 +\\&lt;br&gt;
&amp;amp;+0.99005 v_0 + z_0 - (-0.99005 + t) (-0.99005 g + v_0)\gamma  &amp;amp;, \forall t &amp;gt; t^\star,
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;where we used:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}&lt;br&gt;
z_{-, \rm exp}&amp;amp;= z_0 + 0.99005 v_0 -0.4901 g,\\&lt;br&gt;
v_{-, \rm exp}&amp;amp;= (v_0 - 0.99005 g) .
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}&lt;br&gt;
z_{+, \rm exp}&amp;amp;= z_0 + 0.99005 v_0 -0.4901 g, \\&lt;br&gt;
v_{+, \rm exp}&amp;amp;= -(v_0 - 0.99005 g) \gamma .
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Numerically, we use a &lt;code&gt;DiscreteCallback&lt;/code&gt; in this case to simulate the system&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# DiscreteCallback (explicit event)
tstar = (v0 + sqrt(v0^2+2*z0*g))/g
condition1(u,t,integrator) = (t == tstar)
affect!(integrator) = integrator.u[2] = -integrator.p[2]*integrator.u[2]
cb1 = DiscreteCallback(condition1,affect!,save_positions=(true,true))

sol1 = solve(prob,Tsit5(),callback=cb1, saveat=0.1, tstops=[tstar])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, by choosing an explicit definition of the event, the impact time is fixed. Thus, if we perturb the initial conditions or the parameters, the event location remains at $t^\star = 0.99005$ while it should actually change (for a fixed ground).&lt;/p&gt;
&lt;h3 id=&#34;implicit-events&#34;&gt;Implicit events&lt;/h3&gt;
&lt;p&gt;The physically more meaningful description of a bouncing ball is therefore given by an implicit description of the event in form of a condition (event function)&lt;/p&gt;
&lt;p&gt;$$
g(z,v,p,t) = z(t),
$$&lt;/p&gt;
&lt;p&gt;where an event occurs if $g(z^\star,v^\star,p,t^\star) = 0$. Note that we have already used this condition to define our impact time $t^\star$ of the explicit event.&lt;/p&gt;
&lt;p&gt;As in the previous case, we can analytically compute the full trajectory of the ball. At the event time, we have&lt;/p&gt;
&lt;p&gt;\begin{aligned}&lt;br&gt;
z_{-, \rm imp}&amp;amp;= 0, \\&lt;br&gt;
v_{-, \rm imp}&amp;amp;= - \sqrt{v_0^2 + 2 g z_0}
\end{aligned}&lt;/p&gt;
&lt;p&gt;for the left and&lt;/p&gt;
&lt;p&gt;\begin{aligned}&lt;br&gt;
z_{+, \rm imp}&amp;amp;= 0 \\&lt;br&gt;
v_{+, \rm imp}&amp;amp;= \gamma \sqrt{v_0^2 + 2 g z_0}.
\end{aligned}&lt;/p&gt;
&lt;p&gt;for the right limit. Thus, the full trajectory $z_{\rm imp}(t)$ is given by&lt;/p&gt;
&lt;p&gt;$$
z_{\rm imp}(t) = \begin{cases}
z^{1}_{\rm imp}(t) &amp;amp;=z_0 + v_0 t - \frac{g}{2} t^2 &amp;amp;, \forall t \leq t^\star ,\\&lt;br&gt;
z^{2}_{\rm imp}(t) &amp;amp;= -\frac{(-g t + v_0 + \sqrt{v_0^2 + 2 g z_0})}{2 g} \times, \\&lt;br&gt;
&amp;amp;\times (-g t + v_0 + \sqrt{v_0^2 + 2 g z_0} (1 + 2 \gamma)) &amp;amp;, \forall t &amp;gt; t^\star.
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Numerically, we use a &lt;code&gt;ContinuousCallback&lt;/code&gt; in this case.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# ContinuousCallback (implicit event)
condition2(u,t,integrator) = u[1] # Event when condition(u,t,integrator) == 0
cb2 = ContinuousCallback(condition2,affect!,save_positions=(true,true))
sol2 = solve(prob,Tsit5(),callback=cb2,saveat=0.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can verify that both callbacks lead to the same forward time evolution (for fixed initial conditions and parameters).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# plot forward trajectory
pl1 = plot(sol1, label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], title=&amp;quot;explicit event&amp;quot;, labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright)
pl2 = plot(sol2, label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], title=&amp;quot;implicit event&amp;quot;, labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright)
hline!(pl1, [0.0], label=false, color=&amp;quot;black&amp;quot;)
hline!(pl2, [0.0], label=false, color=&amp;quot;black&amp;quot;)
pl = plot(pl1,pl2)
savefig(pl,&amp;quot;BB_forward.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/BB_forward.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/BB_forward.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;In addition, the implicitly defined impact time via the &lt;code&gt;ContinuousCallback&lt;/code&gt; also shifts the impact time when changing the initial conditions or the parameters&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# animate forward trajectory
sol3 = solve(remake(prob,u0=[u0[1]+0.5,u0[2]]),Tsit5(),callback=cb2,saveat=0.01)

plt2 = plot(sol2, label = false, labelfontsize=20, legendfontsize=20, lw = 1, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, color=&amp;quot;black&amp;quot;, xlims=(t0,tend))
hline!(plt2, [0.0], label=false, color=&amp;quot;black&amp;quot;)
plot!(plt2, sol3, tspan=(t0,tend), color=[1 2], label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, denseplot=true, xlims=(t0,tend), ylims=(-11,9))
# scatter!(plt2, [t2,t2], sol3(t2), color=[1, 2], label=false)

list_plots = []
for t in sol3.t
  tstart = 0.0

  plt1 = plot(sol2, label = false, labelfontsize=20, legendfontsize=20, lw = 1, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, color=&amp;quot;black&amp;quot;)
  hline!(plt1, [0.0], label=false, color=&amp;quot;black&amp;quot;)
  plot!(plt1, sol3, tspan=(t0,t), color=[1 2], label = [&amp;quot;z(t)&amp;quot; &amp;quot;v(t)&amp;quot;], labelfontsize=20, legendfontsize=20, lw = 2, xlabel = &amp;quot;t&amp;quot;, legend=:bottomright, denseplot=true, xlims=(t0,tend), ylims=(-11,9))
  scatter!(plt1,[t,t], sol3(t), color=[1, 2], label=false)
  plt = plot(plt1,plt2)
  push!(list_plots, plt)
end

plot(list_plots[100])

anim = animate(list_plots,every=1)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/BB.gif&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/BB.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The original curve is shown in black in the figure above. In other words, the event time $t^\star=t^\star(p,z_0,v_0,t_0)$ is a function of the parameters and initial conditions, and is implicitly defined by the event condition. Therefore, the sensitivity of the event time with respect to parameters $\frac{\text{d}t^\star}{\text{d}p}$ or initial conditions $\frac{\text{d}t^\star}{\text{d}z_0}$, $\frac{\text{d}t^\star}{\text{d}v_0}$ must be taken into account.&lt;/p&gt;
&lt;h2 id=&#34;sensitivity-analysis-with-events&#34;&gt;Sensitivity analysis with events&lt;/h2&gt;
&lt;p&gt;We are often interested in computing the change of a loss function with respect to changes of the parameters or initial condition. For this purpose, let us first consider the mean square error loss function&lt;/p&gt;
&lt;p&gt;$$
L(z,y) = \sum_i(z(t_i) - y_i)^2
$$&lt;/p&gt;
&lt;p&gt;with respect to target values $y_i$ at time points $t_i$ incident before, after, or at the event time. Let $\alpha$ denote any of the inputs $(z_0,v_0,g,\gamma)$. The sensitivity with respect to $\alpha$ is then given by the chain rule:&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}L}{\text{d} \alpha} =  2\sum_i (z(t_i) - y_i) \frac{\text{d}z(t_i)}{\text{d} \alpha}.
$$&lt;/p&gt;
&lt;p&gt;For the bouncing ball, we can easily compute those sensitivities by inserting our results for $z_{\rm imp}(t_i)$ and $z_{\rm exp}(t_i)$. One can verify that the sensitivities are different in the two cases, as expected.&lt;/p&gt;
&lt;p&gt;However, in most systems, we won&amp;rsquo;t be able to solve analytically a differential equation&lt;/p&gt;
&lt;p&gt;$$
\text{d}x(t) = f(x,p,t) \text{d}t
$$&lt;/p&gt;
&lt;p&gt;with initial condition $x_0=x(t_0)$. Instead, we have to numerically solve for the state $x(t)$. Regarding the computation of the sensitivities, we may then choose one of the 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/analysis/sensitivity/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available algorithms&lt;/a&gt; for the given differential equation. Currently, &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;, &lt;code&gt;InterpolatingAdjoint()&lt;/code&gt;, &lt;code&gt;QuadratureAdjoint()&lt;/code&gt;, &lt;code&gt;ReverseDiffAdjoint()&lt;/code&gt;, &lt;code&gt;TrackerAdjoint()&lt;/code&gt;, and &lt;code&gt;ForwardDiffAdjoint()&lt;/code&gt; are compatible events in ordinary differential equations. We write the loss function in the following as a function of time, state, and parameters&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
L = L(t,x,p).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;In the following, let us focus on the &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; algorithm, which computes the sensitivities&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}L}{\text{d}x(t_{0})} &amp;amp;= \lambda(t_{0}),\\&lt;br&gt;
\frac{\text{d}L}{\text{d}p} &amp;amp;= \lambda_{p}(t_{0}),
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;with respect to the initial state and the parameters, by solving an ODE for $\lambda(t)$ in reverse time from $t_N$ to $t_0$&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}\lambda(t)}{\text{d}t} &amp;amp;= -\lambda(t)^\dagger \frac{\text{d} f(\rightarrow x(t), p, t)}{\text{d} x(t)} - \frac{\text{d} L(t, \rightarrow x(t), p)}{\text{d} x(t)}^\dagger \delta(t-t_i), \\&lt;br&gt;
\frac{\text{d}\lambda_{p}(t)}{\text{d}t} &amp;amp;= -\lambda(t)^\dagger \frac{\text{d} f(x(t), \rightarrow p, t)}{\text{d} p},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;with initial conditions:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda(t_{N})&amp;amp;= 0, \\&lt;br&gt;
\lambda_{p}(t_{N}) &amp;amp;= 0.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The arrows indicate the variable with respect to which we differentiate. Note that computing the vector-Jacobian products (vjp) in the adjoint ODE requires the value of $x(t)$ along its trajectory. In &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;, we recompute $x(t)$&amp;ndash;together with the adjoint variables&amp;ndash;backwards in time starting with its final value $x(t_N)$. A derivation of the ODE adjoint is given in 
&lt;a href=&#34;https://mitmath.github.io/18337/lecture11/adjoints&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris&#39; MIT 18.337 lecture notes&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;explicit-events-1&#34;&gt;Explicit events&lt;/h3&gt;
&lt;p&gt;To make &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; compatible with explicit events&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, we have to store the event times $t^\star_j$ as well as the state $x({t_j^\star}^-)$ and parameters $p=p({t_j^\star}^-)$ (if they are changed) at the left limit of $t^\star_j$. We then solve the adjoint ODE backwards in time between the events. As soon as we reach an event from the right limit ${t_j^\star}^+$, we update the augmented state according to&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda({t_j^\star}^-) &amp;amp;= \lambda({t_j^\star}^+)^\dagger \frac{\text{d} a(\rightarrow x({t_j^\star}^-), p({t_j^\star}^-), {t_j^\star}^-)}{\text{d} x({t_j^\star}^-)} \\&lt;br&gt;
\lambda_p({t_j^\star}^-) &amp;amp;= \lambda_p({t_j^\star}^+) -  \lambda({t_j^\star}^+)^\dagger \frac{\text{d} a(x({t_j^\star}^-), \rightarrow p({t_j^\star}^-), {t_j^\star}^-)}{\text{d} p({t_j^\star}^-)}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $a$ is the affect function applied at the discontinuity. That is, to lift the adjoint from the right to the left limit, we compute a vjp with the adjoint $\lambda({t_j^\star}^+)$ at the right limit and the Jacobian of the affect function evaluated on the left limit.&lt;/p&gt;
&lt;p&gt;In particular, we apply a loss function callback before and after this update if the state was saved in the forward evolution and entered directly into the loss function.&lt;/p&gt;
&lt;h3 id=&#34;implicit-events-1&#34;&gt;Implicit events&lt;/h3&gt;
&lt;h4 id=&#34;special-case-event-as-termination-condition&#34;&gt;special case: event as termination condition&lt;/h4&gt;
&lt;p&gt;Define $u(t) = (t, x(t))$. Let us first re-derive the case, where the implicit event terminates the ODE and where we have a loss function acting on $t^\star_1$, $x(t^\star_1)$, and $p$, as considered by Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel in their ICLR 2021 paper&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. We are interested in&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}L(u(t^\star_1(\rightarrow p), \rightarrow p), \rightarrow p)}{\text{d}p} = \frac{\text{d}L(t^\star_1({\color{black}\rightarrow} p), \text{solve}(t_0, x_0, t^\star_1({\color{black}\rightarrow}p), \rightarrow p),\rightarrow p)}{\text{d}p},
$$&lt;/p&gt;
&lt;p&gt;which indicates that changing $p$ changes both $t^\star_1$ as well as $x^\star_1$ in $t^\star_1$.&lt;/p&gt;
&lt;p&gt;In a first step, we need to compute the sensitivity of $t^\star_1(p)$ with respect to $p$ and $x_0$ based on the event condition $F(t, p) = g(u(t, p)) = 0$.  We can apply the 
&lt;a href=&#34;https://www.uni-siegen.de/fb6/analysis/overhagen/vorlesungsbeschreibungen/skripte/analysis3_1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implicit function theorem&lt;/a&gt; which yields:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}t^\star_1(p)}{\text{d}p} &amp;amp;= - \left(\frac{\text{d}g(\rightarrow t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1}\right)^{-1} \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, t^\star_1, \rightarrow p))}{\text{d}p} .\\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The total derivative&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; inside the bracket is defined as:
$$
\begin{aligned}
\frac{\text{d}g}{\text{d}t^\star_1} \stackrel{\text{def}}{=} \frac{\text{d}g(\rightarrow t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1} &amp;amp;= \frac{\text{d}g(\rightarrow t^\star_1, \text{solve}(t_0, x_0, t^\star_1, p))}{\text{d}t^\star_1} + \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1}\\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Since&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}(\text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1} = f(x^\star, p^\star, t^\star_1)
$$&lt;/p&gt;
&lt;p&gt;by definition of the ODE, we can write&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d}t^\star_1} = \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, \rightarrow t^\star_1, p))}{\text{d} u^\star(t^\star_1)}  f(x^\star, p^\star, t^\star_1).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Furthermore, we have
$$
\begin{aligned}
\frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, t^\star_1, \rightarrow p))}{\text{d}p} = \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, t^\star_1,\rightarrow p))}{\text{d} u^\star(t^\star_1)}^\dagger  \frac{\text{d}\text{ solve}(t_0, x_0, t^\star_1,\rightarrow p))}{\text{d}p}
\end{aligned}
$$
for the second term of $\frac{\text{d}t^\star_1(p)}{\text{d}p}$. We define&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}g}{\text{d}u^\star_1} \stackrel{\text{def}}{=} \frac{\text{d}g(t^\star_1, \text{solve}(t_0, x_0, t^\star_1,\rightarrow p))}{\text{d} u^\star(t^\star_1)}
$$&lt;/p&gt;
&lt;p&gt;We can now write the gradient as:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}L(t^\star_1({\color{black}\rightarrow} p), \text{solve}(t_0, x_0, t^\star_1({\color{black}\rightarrow}p), \rightarrow p),\rightarrow p)}{\text{d}p} &amp;amp;= \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  p), \rightarrow p)}{\text{d}p} \\&lt;br&gt;
+&amp;amp; \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  \rightarrow p), p)}{\text{d}p} \\&lt;br&gt;
+&amp;amp; \frac{\text{d}L(\rightarrow t^\star_1(p), \text{solve}(t_0, x_0, \rightarrow t^\star_1(p),  p), p)}{\text{d}t^\star_1} \frac{\text{d} t^\star_1(p)}{\text{d}p},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which, after insertion of our results above, can be casted into the form:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\text{d}L(t^\star_1({\color{black}\rightarrow} p), \text{solve}(t_0, x_0, t^\star_1({\color{black}\rightarrow}p), \rightarrow p),\rightarrow p)}{\text{d}p} &amp;amp;= v^\dagger \frac{\text{d}\text{ solve}(t_0, x_0, t^\star_1(p), \rightarrow p)}{\text{d}p} \\&lt;br&gt;
&amp;amp;+ \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p), p), \rightarrow p)}{\text{d}p},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
v &amp;amp;= \xi \left(-\frac{\text{d}g}{\text{d}t^\star_1}\right)^{-1} \frac{\text{d}g}{\text{d}u^\star_1} + \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  p), p)}{\text{d} u^\star(t^\star_1)},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where we introduced the scalar pre-factor&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\xi = \left( \frac{\text{d}L(\rightarrow t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  p), p)}{\text{d}t^\star_1} +  \frac{\text{d}L(t^\star_1(p), \text{solve}(t_0, x_0, t^\star_1(p),  p), p)}{\text{d} u^\star(t^\star_1)}^\dagger f(x^\star, p^\star, t^\star_1)\right).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;This means that if we terminate the ODE integration by an implicit event, we compute the sensitivities as follows (for simplicity we drop terms due to an explicit dependence of the loss function on the parameters or time):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;use an ODE solver to solve forward until the event is triggered
$$
u_i = \text{solve}(t_0, x_0, t^\star_1(p),  p).
$$
$u_i(t_i)=(t_i,x_i)$ are the stored values which enter the loss function.&lt;/li&gt;
&lt;li&gt;compute the loss function gradient with respect to the state at $t^\star_1$.
$$
\lambda_-^\text{0} = \frac{\text{d}L(t^\star_1(p), \rightarrow \text{solve}(t_0, x_0, t^\star_1(p),  p), p)}{\text{d} u^\star(t^\star_1)}.
$$&lt;/li&gt;
&lt;li&gt;(instead of using the &lt;code&gt;BacksolveAdjoint()&lt;/code&gt; algorithm with $\lambda_-^\text{0}$ directly,) use the corrected version containing the dependence on the event time. For this, compute  $\frac{\text{d}g}{\text{d}t^\star_1}, \frac{\text{d}g}{\text{d}u^\star_1}$, and $f(x^\star, p, t^\star_1)$.
Then, the corrected version of the adjoint is given by&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\lambda_- = - \left( {\lambda_-^\text{0}}^\dagger f(x^\star, p^\star, t^\star_1) \right)\left(\frac{\text{d}g}{\text{d}t^\star_1}\right)^{-1} \frac{\text{d}g}{\text{d}u^\star_1} + \lambda_-^\text{0}.
$$&lt;/p&gt;
&lt;p&gt;$\lambda_-$ can then be used as initial condition to $\text{backsolve_adjoint}(\lambda_-, t^\star_1, x(t^\star_1), t_0)$ which backpropagates the adjoint $\lambda_-$ from $t^\star_1$ to $t_0$.&lt;/p&gt;
&lt;p&gt;If there is an additional affect function $a$ associated with the event, i.e. a right limit, we must additionally compute&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda_+^\text{0} =  \frac{\text{d}L(t^\star_1(p), \rightarrow a\left(\text{solve}(t_0, x_0, t^\star_1(p),  p)\right), p)}{\text{d} u^\star(t^\star_1))}.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Compute the vjp as in the case of a &amp;lsquo;DiscreteCallback&amp;rsquo;&lt;/p&gt;
&lt;p&gt;$$
\lambda_-^\text{0} = {\lambda_+^\text{0}}^\dagger \frac{\text{d} a(\rightarrow x({t_j^\star}^-), p({t_j^\star}^-), {t_j^\star}^-)}{\text{d} x({t_j^\star}^-)}
$$&lt;/p&gt;
&lt;p&gt;and correct it as above&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda_- = - \left( {\lambda_-^\text{0}}^\dagger f(x({t_1^\star}^-), p({t_1^\star}^-), t^\star_1) \right)\left(\frac{\text{d}g}{\text{d}{t_1^\star}^-}\right)^{-1} \frac{\text{d}g}{\text{d}{u^\star_1}^-} + \lambda_-^\text{0}.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If both limits contribute to the loss function, the contributions are added.&lt;/p&gt;
&lt;h4 id=&#34;generalization-several-events&#34;&gt;generalization: several events&lt;/h4&gt;
&lt;p&gt;As pointed out by Chen et al. as well as by Timo C. Wunderlich and Christian Pehle&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, one can chain together the events and differentiate through the entire time evolution on a time interval $(t_0, t_{\text{end}})$. That is, we are generally allowed to segment the time evolution over an interval $[t_0, t]$ into one from $[t_0, s]$ and a subsequent one from $[s, t]$:&lt;/p&gt;
&lt;p&gt;$$
\text{solve}(t_0, x_0, t, p)  = \text{solve}(s, \text{solve}(t_0, x_0, s, p), t-s, p),
$$&lt;/p&gt;
&lt;p&gt;such that also loss function contributions are chained. Therefore, we have the following modification:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Segment the trajectory at the event times. Use $\text{backsolve_adjoint}(\lambda_{0}, t_\text{end}, x(t_\text{end}), t^\star_N)$ to backprogagate the loss function gradient from the final state until the right limit of the last event location.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition to the steps above, subtract a correction:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\lambda_\text{c} = - \left( {\lambda_+}^\dagger f(x({t_N^\star}^+), p({t_N^\star}^+), t^\star_N) \right)\left(\frac{\text{d}g}{\text{d}{t_N^\star}^-}\right)^{-1} \frac{\text{d}g}{\text{d}{u^\star_N}^-},
$$&lt;/p&gt;
&lt;p&gt;where $\lambda_+$ is the right-hand limit of the adjoint state before the loss gradient ($\lambda_+^\text{0}$ above) was added. Iterate over the remaining events.&lt;/p&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;We are still refining the adjoints in case of implicit discontinuities (&lt;code&gt;ContinuousCallbacks&lt;/code&gt;). For further information, the interested reader is encouraged to track the associated issues 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues/383&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#383&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues/374&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#374&lt;/a&gt;, and 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/pull/445&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PR #445&lt;/a&gt; in the DiffEqSensitivity.jl package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Michael Poli, Stefano Massaroli, et al., arXiv preprint arXiv:2106.04165 (2021). &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Junteng Jia, Austin R. Benson, arXiv preprint arXiv:1905.10403 (2019). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Timo C. Wunderlich and Christian Pehle, Sci. Rep. &lt;em&gt;11&lt;/em&gt;, 12829 (2021). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel, arXiv preprint arXiv:2011.03902 (2020). &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For a function $f$ of more than one variable $y = f(t, x_1(t),x_2(t),\dots,x_N(t))$, the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Differential_of_a_function#Differentials_in_several_variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;total derivative&lt;/a&gt; with respect to the independent variable $t$ is given by the sum of all partial derivatives
$$
\begin{aligned}
\frac{\text{d}y}{\text{d}t} &amp;amp;= \frac{\text{d}f(\rightarrow t, x_1(\rightarrow t),x_2(\rightarrow t),\dots,x_N(\rightarrow t))}{\text{d}t} \\&lt;br&gt;
&amp;amp;= \frac{\text{d}f(\rightarrow t, x_1(t),x_2(t),\dots,x_N(t))}{\text{d}t} + \frac{\text{d}f(t, x_1(\rightarrow t),x_2(t),\dots,x_N(t))}{\text{d}t}\\&lt;br&gt;
&amp;amp;+ \frac{\text{d}f(t, x_1(t),x_2(\rightarrow t),\dots,x_N(t))}{\text{d}t} + \dots +  \frac{\text{d}f(t, x_1(t),x_2(t),\dots,x_N(\rightarrow t))}{\text{d}t}.
\end{aligned}
$$ &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Shadowing Methods for Forward and Adjoint Sensitivity Analysis of Chaotic Systems</title>
      <link>https://frankschae.github.io/post/shadowing/</link>
      <pubDate>Fri, 02 Jul 2021 11:08:22 +0200</pubDate>
      <guid>https://frankschae.github.io/post/shadowing/</guid>
      <description>&lt;p&gt;In this post, we dig into sensitivity analysis of chaotic systems. Chaotic systems are dynamical, deterministic systems that are extremely sensitive to small changes in the initial state or the system parameters. Specifically, the dependence of a chaotic system on its initial conditions is well known as the &amp;ldquo;butterfly effect&amp;rdquo;. Chaotic models are encountered in various fields ranging from simple examples such as the double pendulum to highly complicated fluid or climate models.&lt;/p&gt;
&lt;p&gt;Sensitivity analysis methods have proven to be very powerful for solving inverse problems such as parameter estimation or optimal control&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. However, conventional sensitivity analysis methods may fail in chaotic systems due to the ill-conditioning of the initial value problem. Sophisticated methods, such as least squares shadowing&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; (LSS) or non-intrusive least squares shadowing&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; (NILSS) have been developed in the last decade. Essentially, these methods transform the initial value problem to a well conditioned optimization problem &amp;ndash; the least squares shadowing problem. In this second part of my GSoC project, I implemented the LSS and the NILSS method within the 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiffEqSensitivity.jl&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;The objective for LSS and NILSS is a long-time average quantity. More precisely, we define the instantaneous objective by $g(u,p)$, where $u$ is the state and $p$ is the parameter of the differential equation. Then, the objective is obtained by averaging $g$ over an infinitely long trajectory:&lt;/p&gt;
&lt;p&gt;$$
\langle g \rangle_∞ = \lim_{T \rightarrow ∞} \langle g \rangle_T,
$$
where
$$
\langle g \rangle_T = \frac{1}{T} \int_0^T g(u,s) \text{d}t.
$$
Under the assumption of ergodicity, $\langle g \rangle_∞$ only depends on $p$.&lt;/p&gt;
&lt;h2 id=&#34;the-lorenz-system&#34;&gt;The Lorenz system&lt;/h2&gt;
&lt;p&gt;One of the most important chaotic models is the Lorenz system which is a simplified model for atmospheric convection. The Lorenz system has three states $x$, $y$, and $z$, as well as three parameters $\rho$, $\sigma$, and $\beta$. Its time evolution is given by the ODE:&lt;/p&gt;
&lt;p&gt;$$
\begin{pmatrix}
\text{d}x \\&lt;br&gt;
\text{d}y \\&lt;br&gt;
\text{d}z \\&lt;br&gt;
\end{pmatrix} = \begin{pmatrix}
\sigma (y-x)\\&lt;br&gt;
x(\rho-z) - y\\&lt;br&gt;
x y - \beta z \\&lt;br&gt;
\end{pmatrix}\text{d}t
$$&lt;/p&gt;
&lt;p&gt;For simplicity, let us fix $\sigma=10$ and $\beta=8/3$ and focus only on the sensitivity with respect to $\rho$. The classic Lorenz attractor is obtained when using $\rho=28$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Random; Random.seed!(1234)
using OrdinaryDiffEq
using Statistics
using QuadGK, ForwardDiff, Calculus
using DiffEqSensitivity
using SparseArrays, LinearAlgebra

# simulate 1 trajectory of the Lorenz system forward
function lorenz!(du,u,p,t)
  du[1] = 10*(u[2]-u[1])
  du[2] = u[1]*(p[1]-u[3]) - u[2]
  du[3] = u[1]*u[2] - (8//3)*u[3]
end

p = [28.0]
tspan_init = (0.0,30.0)
tspan_attractor = (30.0,50.0)
u0 = rand(3)
prob_init = ODEProblem(lorenz!,u0,tspan_init,p)
sol_init = solve(prob_init,Tsit5())
prob_attractor = ODEProblem(lorenz!,sol_init[end],tspan_attractor,p)
sol_attractor = solve(prob_attractor,Vern9(),abstol=1e-14,reltol=1e-14)

using Plots, LaTeXStrings
pl1 = plot(sol_init,vars=(1,2,3), legend=true,
  label = &amp;quot;initial&amp;quot;,
  labelfontsize=20,
  lw = 2,
  xlabel = L&amp;quot;x&amp;quot;, ylabel = L&amp;quot;y&amp;quot;, zlabel = L&amp;quot;z&amp;quot;,
  xlims=(-25,30),ylims=(-30,30),zlims=(5,49)
 )
plot!(pl1, sol_attractor,vars=(1,2,3), label=&amp;quot;attractor&amp;quot;,xlims=(-25,30),ylims=(-30,30),zlims=(5,49)
 )
savefig(pl1, &amp;quot;Lorenz_forward.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/Lorenz_forward.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/Lorenz_forward.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, we separated the trajectory in two parts: We plot the initial transient dynamics starting from random initial conditions towards the attractor in blue and the subsequent time evolution lying entirely on the attractor in orange.&lt;/p&gt;
&lt;p&gt;Following Refs.&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, we choose&lt;/p&gt;
&lt;p&gt;$$
\langle z \rangle_∞ = \lim_{T \rightarrow ∞} \frac{1}{T} \int_0^T z \text{d}t
$$&lt;/p&gt;
&lt;p&gt;as the objective, where we only use the trajectory that lies completely on the attractor (i.e., the orange trajectory in the plot on top). Let us first study the objective as a function of $\rho$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_objective(sol)
  quadgk(t-&amp;gt; sol(t)[end]/(tspan_attractor[2]-tspan_attractor[1]) ,tspan_attractor[1],tspan_attractor[2], atol=1e-14, rtol=1e-10)[1]
end

pl2 = plot(sol_attractor.t, getindex.(sol_attractor.u,3), ylabel=L&amp;quot;z(t)&amp;quot;, xlabel=L&amp;quot;t&amp;quot;, label=false, labelfontsize=20,lw = 2)
mean_z = [mean(getindex.(sol_attractor.u,3))]
int_z = compute_objective(sol_attractor)
hline!(pl2, [int_z], label=L&amp;quot;\langle z\rangle&amp;quot;, lw = 2)
savefig(pl2, &amp;quot;zsingle.png&amp;quot;)

# for each value of the parameter, solve 20 times the initial value problem
# wrap the procedure inside a function depending on p
function Lorenz_solve(p)
  u0 = rand(3)
  prob_init = ODEProblem(lorenz!,u0,tspan_init,p)
  sol_init = solve(prob_init,Tsit5())
  prob_attractor = ODEProblem(lorenz!,sol_init[end],tspan_attractor,p)
  sol_attractor = solve(prob_attractor,Vern9(),abstol=1e-14,reltol=1e-14)
  sol_attractor, prob_attractor
end

Niter = 10
ps = collect(0.0:1.0:50.0)
probs = []
sols = []
zmean = []
zstd = []
for ρ in ps
  @show ρ
  ztmp = []
  for i=1:Niter
    sol, prob = Lorenz_solve([ρ])
    zbar = compute_objective(sol)
    push!(sols, sol)
    push!(probs, prob)
    push!(ztmp, zbar)
  end
  push!(zmean,mean(ztmp))
  push!(zstd,std(ztmp))
end

pl3 = plot(ps,zmean, ribbon = zstd, ylabel=L&amp;quot;\langle z\rangle&amp;quot;, xlabel=L&amp;quot;\rho&amp;quot;, legend=false, labelfontsize=20, lw = 2, xlims=(0,50),ylims=(0,50))
savefig(pl3, &amp;quot;zvsrho.png&amp;quot;)

pl4 = plot(pl2,pl3, margin=3Plots.mm, layout = (1, 2), size=(600,300))
savefig(pl4, &amp;quot;z.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We obtain:&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/z.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/z.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;That is, we find a slope of approximately one (almost everywhere except at the kink $\rho\approx 23$), and, therefore, we expect a sensitivity of&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}\langle z \rangle_∞}{\text{d} \rho} \approx 1.
$$&lt;/p&gt;
&lt;h2 id=&#34;conventional-forward-mode-sensitivity-analysis-and-finite-differencing&#34;&gt;Conventional forward-mode sensitivity analysis and finite-differencing&lt;/h2&gt;
&lt;p&gt;For non-chaotic systems, we would just use the 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/analysis/sensitivity/#Sensitivity-Algorithms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;standard discrete or continuous forward sensitivity methods&lt;/a&gt; or even finite-differencing.  If we try to compute the sensitivity for the Lorenz system:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function G(p, prob=prob_attractor)
  tmp_prob = remake(prob,p=p)
  tmp_sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14)
  res = compute_objective(tmp_sol)
  @info res
  res
end
sense_forward = ForwardDiff.gradient(G,p)
sense_calculus = Calculus.gradient(G,p)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we find diverging values:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \frac{\text{d}\langle z \rangle_\infty}{\text{d} \rho} \Bigg\rvert_{\rho=28} \approx -49899 {\text{ (ForwardDiff)}}  \\&lt;br&gt;
&amp;amp;\frac{\text{d}\langle z \rangle_\infty}{\text{d} \rho} \Bigg\rvert_{\rho=28} \approx 472 {\text{ (Calculus)}}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;As pointed out in the NILSS paper, this is because the limit of $T\rightarrow ∞$ for a fixed initial state does &lt;em&gt;not&lt;/em&gt; commute with the differentiation:&lt;/p&gt;
&lt;p&gt;$$
\frac{\text{d}}{\text{d} \rho} \langle z \rangle_∞ \neq \lim_{T \rightarrow ∞} \frac{\partial}{\partial \rho} \langle z \rangle_T
$$&lt;/p&gt;
&lt;p&gt;Similarly, using 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/analysis/uncertainty_quantification/#Example-3:-Adaptive-ProbInts-on-the-Lorenz-Attractor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uncertainty quantification&lt;/a&gt; one realizes that due to finite numerical precision and the associated unavoidable errors that are amplified exponentially, one cannot follow the true solution of a chaotic system for long times. We can visualize this by solving the Lorenz system twice with exactly the same parameters and initial condition but with different floating point number precision. In the following animation, we see an $O(1)$ difference between both trajectories after a few Lyapunov lengths:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;prob_attractor1 = ODEProblem(lorenz!,sol_init[end],(0.0, 50.0),p)
prob_attractor2 = ODEProblem(lorenz!,convert.(Float32, sol_init[end]),(0f0, 50f0),convert.(Float32,p))
sol1 = solve(prob_attractor1,Tsit5(),abstol=1e-6,reltol=1e-6, saveat=0.01)
sol2 = solve(prob_attractor2,Tsit5(),abstol=1f-6,reltol=1f-6, saveat=0.01f0)

list_plots = []
t1 = 0.0
for i in 1:500
  t2 = i*0.1
  plt1 = plot(sol1, vars=(1,2,3), tspan=(t1,t2), denseplot=true, legend=true,
     label = &amp;quot;Float64&amp;quot;, labelfontsize=20, lw = 2,
     xlabel = L&amp;quot;x&amp;quot;, ylabel = L&amp;quot;y&amp;quot;, zlabel = L&amp;quot;z&amp;quot;,
     xlims=(-20,25),ylims=(-28,25),zlims=(5,48))
  plot!(plt1, sol2,vars=(1,2,3), tspan=(t1,t2), denseplot=true, label=&amp;quot;Float32&amp;quot;,
        xlims=(-20,25),ylims=(-28,25),zlims=(5,48))
  push!(list_plots, plt1)
end

anim = animate(list_plots,every=1)

pl1 = plot(sol1,vars=(1,2,3), legend=true,
  label = &amp;quot;Float64&amp;quot;, labelfontsize=20, lw = 2,
  xlabel = L&amp;quot;x&amp;quot;, ylabel = L&amp;quot;y&amp;quot;, zlabel = L&amp;quot;z&amp;quot;,
  xlims=(-20,25),ylims=(-28,25),zlims=(5,48)
 )
plot!(pl1, sol2,vars=(1,2,3), label=&amp;quot;Float32&amp;quot;, xlims=(-20,25),ylims=(-28,25),zlims=(5,48)
 )

savefig(pl1, &amp;quot;Lorenz_Floats.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/Lorenz.gif&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/Lorenz.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Without animation:&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/Lorenz_Floats.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/Lorenz_Floats.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Luckily, the 
&lt;a href=&#34;https://mathworld.wolfram.com/ShadowingTheorem.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shadowing lemma&lt;/a&gt; states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although a numerically computed chaotic trajectory diverges exponentially from the true trajectory with the same initial coordinates, there exists an errorless trajectory with a slightly different initial condition that stays near (&amp;ldquo;shadows&amp;rdquo;) the numerically computed one.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;shadowing-methods&#34;&gt;Shadowing methods&lt;/h2&gt;
&lt;p&gt;The central idea of the shadowing methods is to distill the long-time effect (which actually shifts the attractor) due to a variation of the system parameters (upwards in the $z$-direction with increasing $\rho$ for the Lorenz system) from the transient effect, i.e., the butterfly effect that looks like exponentially diverging trajectories due to variations of the initial conditions.  That implies that we aim at finding two trajectories, one with $p$ and one with $p+\delta p$, which do &lt;em&gt;not&lt;/em&gt; diverge exponentially from each other (which exist thanks to the shadowing lemma). In this case, their difference will only contain the long-time effect. More details can be found in Refs. &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, including a visualization of both effects in Fig. 1 of Ref. &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;lss-and-nilss-for-the-lorenz-system&#34;&gt;LSS and NILSS for the Lorenz system&lt;/h2&gt;
&lt;p&gt;Switching to LSS or NILSS within the 
&lt;a href=&#34;https://sciml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; ecosystem is straightforward by either defining the associated LSS (&lt;code&gt;ForwardLSSProblem&lt;/code&gt; or &lt;code&gt;AdjointLSSProblem&lt;/code&gt;) or NILSS problem (&lt;code&gt;NILSSProblem&lt;/code&gt;) type manually:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# objective
g(u,p,t) = u[end]

####
# LSS
####
lss_problem = ForwardLSSProblem(sol_attractor, ForwardLSS(alpha=DiffEqSensitivity.CosWindowing()), g)
@show shadow_forward(lss_problem) # 1.0095888187322035

lss_problem = ForwardLSSProblem(sol_attractor, ForwardLSS(alpha=DiffEqSensitivity.Cos2Windowing()), g)
@show shadow_forward(lss_problem) # 1.0343951385924328

lss_problem = ForwardLSSProblem(sol_attractor, ForwardLSS(alpha=10.0), g)
@show shadow_forward(lss_problem) # 1.0284286902740765

adjointlss_problem = AdjointLSSProblem(sol_attractor, AdjointLSS(alpha=10.0), g)
@show shadow_adjoint(adjointlss_problem) # 1.028428690274077
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or by setting the &lt;code&gt;sensealg=&lt;/code&gt; kwarg in &lt;code&gt;solve()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# select via sensealg in solve
using Zygote

function GLSS(p; sensealg=ForwardLSS(), dt=0.01, g=nothing)
  _prob = remake(prob_attractor,p=p)
  _sol = solve(_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=dt,sensealg=sensealg, g=g)
  sum(getindex.(_sol.u,3))
end

dp1 = Zygote.gradient((p)-&amp;gt;GLSS(p),p) # 0.9694728321500617
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we have implemented three different options for forward shadowing with &lt;code&gt;LSS()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CosWindowing()&lt;/code&gt; (default)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Cos2Windowing()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;time dilation with a factor of $\alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, an adjoint implementation &lt;code&gt;AdjointLSS()&lt;/code&gt; is available that is particularly recommended for a large number of system parameters.  Based on the values computed above, we can easily check that &lt;code&gt;AdjointLSS(alpha=10.0)&lt;/code&gt; agrees perfectly with &lt;code&gt;ForwardLSS(alpha=10.0)&lt;/code&gt;. In all cases considered, we find the expected sensitivity value of $\approx 1$.&lt;/p&gt;
&lt;p&gt;However, the use of &lt;code&gt;LSS()&lt;/code&gt; is (typically) much more expensive than the use of &lt;code&gt;NILSS()&lt;/code&gt;, because &lt;code&gt;LSS()&lt;/code&gt; needs to solve a large linear system. This linear system scales with the number of independent variables in the differential equation times the number of time steps and, thus, it can become very large.  The computational and memory costs of &lt;code&gt;NILSS()&lt;/code&gt; scale with the number of positive (unstable) Lyapunov exponents, since it constrains the optimization problem in the LSS method to its unstable subspace. In many cases, this number is much smaller than the number of independent variables, hence making &lt;code&gt;NILSS()&lt;/code&gt; more efficient.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;NILSS()&lt;/code&gt; algorithm, the user can control the number of steps per segment as well as the number of segments.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;####
# NILSS
####

# make sure trajectory is fully on the attractor
Random.seed!(1234)
tspan_init = (0.0,100.0)
tspan_attractor = (100.0,120.0)
u0 = rand(3)
prob_init = ODEProblem(lorenz!,u0,tspan_init,p)
sol_init = solve(prob_init,Tsit5())
prob_attractor = ODEProblem(lorenz!,sol_init[end],tspan_attractor,p)

nseg = 100 # number of segments on time interval
nstep = 2001 # number of steps on each segment

nilss_prob = NILSSProblem(prob_attractor, NILSS(nseg, nstep), g)
@show DiffEqSensitivity.shadow_forward(nilss_prob,Tsit5()) # 0.9966924374966089

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the number of segments is chosen too small, a warning is thrown:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;nseg = 20 # number of segments on time interval
nstep = 2001 # number of steps on each segment

nilss_prob = NILSSProblem(prob_attractor, NILSS(nseg, nstep), g)
@show DiffEqSensitivity.shadow_forward(nilss_prob,Tsit5()) # 1.0416028730638789

# Warning: Detected a large value of ξ at the beginning of a segment.
# └ @ DiffEqSensitivity ~/.julia/dev/DiffEqSensitivity/src/nilss.jl:474
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the future, we might add an option for the automate control of these variables following the proposal in the NILSS paper&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;With respect to the shadowing methods for chaotic systems, we are planning to implement further methods, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NILSAS&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;FD-NILSS&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in the upcoming weeks. For further information and a collection of other methods, the interested reader is invited to track the corresponding 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues/102&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issue&lt;/a&gt; in the DiffEqSensitivity.jl package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Frank Schäfer, Michal Kloc, et al., Mach. Learn.: Sci. Technol. &lt;strong&gt;1&lt;/strong&gt;, 035009 (2020). &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Frank Schäfer, Pavel Sekatski, et al., Mach. Learn.: Sci. Technol. &lt;strong&gt;2&lt;/strong&gt;, 035004 (2021). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chris Rackauckas, Yingbo Ma, et al., arXiv preprint arXiv:2001.04385 (2020). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Qiqi Wang, Rui Hu, et al. J. Comput. Phys &lt;strong&gt;26&lt;/strong&gt;, 210-224 (2014) &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Angxiu Ni and Qiqi Wang. J. Comput. Phys &lt;strong&gt;347&lt;/strong&gt;,  56-77 (2017). &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Angxiu Ni and Chaitanya Talnikar, J. Comput. Phys &lt;strong&gt;395&lt;/strong&gt;, 690-709, (2019) &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Angxiu Ni, Qiqi Wang et al., J. Comput. Phys &lt;strong&gt;394&lt;/strong&gt;, 615-631 (2019) &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Neural Hybrid Differential Equations</title>
      <link>https://frankschae.github.io/post/hybridde/</link>
      <pubDate>Wed, 16 Jun 2021 14:50:17 +0200</pubDate>
      <guid>https://frankschae.github.io/post/hybridde/</guid>
      <description>&lt;p&gt;I am delighted that I have been awarded my second GSoC stipend this year.  I look forward to carrying out the ambitious project scope with my mentors 
&lt;a href=&#34;https://github.com/ChrisRackauckas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris Rackauckas&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mschauer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moritz Schauer&lt;/a&gt;,  
&lt;a href=&#34;https://github.com/YingboMa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yingbo Ma&lt;/a&gt;, and 
&lt;a href=&#34;https://github.com/mohamed82008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mohamed Tarek&lt;/a&gt;. This year&amp;rsquo;s project is embedded within the 
&lt;a href=&#34;https://summerofcode.withgoogle.com/organizations/5765643267211264/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NumFocus&lt;/a&gt;/
&lt;a href=&#34;https://sciml.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; organization and comprises adjoint sensitivity methods for discontinuities, shadowing methods for chaotic dynamics, symbolically generated adjoint methods, and further AD tooling within the Julia Language.&lt;/p&gt;
&lt;p&gt;This first post aims to illustrate our new (adjoint) sensitivity analysis tools with respect to event handling in (ordinary) differential equations (DEs).&lt;/p&gt;
&lt;h2 id=&#34;hybrid-differential-equations&#34;&gt;Hybrid Differential Equations&lt;/h2&gt;
&lt;p&gt;DEs with additional explicit or implicit discontinuities are called hybrid DEs. Within the SciML software suite, such discontinuities may be incorporated into DE models by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;callbacks&lt;/a&gt;. Evidently, the incorporation of discontinuities allows a user to specify changes (&lt;em&gt;events&lt;/em&gt;) in the system, i.e., changes of the state or the parameters of the DE, which cannot be modeled by a plain ordinary DE. While explicit events can be described by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/#DiscreteCallback-Examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiscreteCallbacks&lt;/a&gt;, implicit events have to be specified by 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/callback_functions/#ContinuousCallback-Examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ContinuousCallbacks&lt;/a&gt;. That is, explicit events possess explicit event times, while implicit events are triggered when a continuous function evaluates to &lt;code&gt;0&lt;/code&gt;. Thus, implicit events require some sort of rootfinding procedure.&lt;/p&gt;
&lt;p&gt;Some relevant examples for hybrid DEs with discrete or continuous callbacks are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;quantum optics experiments, where photon-counting measurements lead to jumps in the quantum state that occur with a variable rate, see for instance Appendix A in Ref.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; (&lt;code&gt;ContinuousCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;a bouncing ball&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; (&lt;code&gt;ContinuousCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;classical point process models, such as a Poisson process&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;digital controllers&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, where a continuous system dynamics is controlled by a discrete-time controller (&lt;code&gt;DiscreteCallback&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;pharmacokinetic models&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, where explicit dosing times change the drug concentration in the blood (&lt;code&gt;DiscreteCallback&lt;/code&gt;). The simplest possible example being the one-compartment model.&lt;/li&gt;
&lt;li&gt;kicked oscillator dynamics, e.g., a harmonic oscillator that gets a kick at some time points (&lt;code&gt;DiscreteCallback&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The associated sensitivity methods that allow us to differentiate through the respective hybrid DE systems have been recently introduced in Refs. &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kicked-harmonic-oscillator&#34;&gt;Kicked harmonic oscillator&lt;/h2&gt;
&lt;p&gt;Let us consider the simple physical model of a damped harmonic oscillator, described by an ODE of the form&lt;/p&gt;
&lt;p&gt;$$
\ddot{x}(t) + a\cdot\dot{x}(t) + b \cdot x(t) = 0 ,
$$&lt;/p&gt;
&lt;p&gt;where $a=0.1$ and $b=1$ with initial conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
x(t=0) &amp;amp;= 1  \\&lt;br&gt;
v(t=0) &amp;amp;= \dot{x}(t=0) = 0.
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This second-order ODE can be 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_differential_equation#Reduction_of_order&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reduced&lt;/a&gt; to two first-order ODEs, such that we can straightforwardly simulate the resulting ODE with the &lt;code&gt;DifferentialEquations.jl&lt;/code&gt; package. (Instead of doing this reduction manually, we could also use 
&lt;a href=&#34;https://mtk.sciml.ai/stable/tutorials/higher_order/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ModelingToolkit.jl&lt;/code&gt;&lt;/a&gt; to transform the ODE in an automatic manner. Alternatively, for second-order ODEs, there is also a &lt;code&gt;SecondOrderODEProblem&lt;/code&gt; implemented.) The Julia code reads:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using DiffEqFlux, DifferentialEquations, Flux, Optim, Plots, DiffEqSensitivity
using Zygote
using Random
u0 = Float32[1.; 0.]

tspan = (0.0f0,50.0f0)

dtsave = 0.5f0
t = tspan[1]:dtsave:tspan[2]

function oscillator!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  return nothing
end

prob_data = ODEProblem(oscillator!,u0,tspan)

# ODE without kicks
pl = plot(solve(prob_data,Tsit5(),saveat=t), label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/forward_damped_oscillator_no_kicks.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/forward_damped_oscillator_no_kicks.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We now include a kick to the velocity of the oscillator at regular time steps. Here, we choose both the time difference between the kicks and the increase in velocity as &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;kicktimes = tspan[1]:1:tspan[2]
function kick!(integrator)
  integrator.u[end] += one(eltype(integrator.u))
end
cb_ = PresetTimeCallback(kicktimes,kick!,save_positions=(false,false))

sol_data = solve(prob_data,Tsit5(),callback=cb_,saveat=t)
t_data = sol_data.t
ode_data = Array(sol_data)

# visualize data
pl1 = plot(t_data,ode_data[1,:],label=&amp;quot;data x(t)&amp;quot;)
plot!(pl1,t_data,ode_data[2,:],label=&amp;quot;data v(t)&amp;quot;)

pl2 = plot(t_data[1:20],ode_data[1,1:20],label=&amp;quot;data x(t)&amp;quot;)
plot!(pl2,t_data[1:20],ode_data[2,1:20],label=&amp;quot;data v(t)&amp;quot;)
pl = plot(pl2, pl1, layout=(1,2), xlabel=&amp;quot;t&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/forward_damped_oscillator.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/forward_damped_oscillator.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

The left-hand side shows a zoom for short times to better resolve the kicks. Note that by setting &lt;code&gt;save_positions=(true,true)&lt;/code&gt;, the kicks would be saved before &lt;strong&gt;and&lt;/strong&gt; after the event such that the kicks would appear completely vertically in the plot. The data on the right-hand will be used as training data below. In the spirit of universal differential equations&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;, we now aim at learning (potentially) missing parts of the model from these data traces.&lt;/p&gt;
&lt;h3 id=&#34;high-domain-knowledge&#34;&gt;High domain knowledge&lt;/h3&gt;
&lt;p&gt;For simplicity, we assume that we have almost perfect knowledge about our system. That is, we assume to know the basic structure of the ODE, including its parameters $a$ and $b$, and that the &lt;code&gt;affect!&lt;/code&gt; function of the event only acts on the velocity. We then encode the affect as an additional component to the ODE. The task is thus to learn the dynamics of the third component of &lt;code&gt;integrator.u&lt;/code&gt;. If we further set the initial value of that component to &lt;code&gt;1&lt;/code&gt;, then the neural network only has to learn that &lt;code&gt;du[3]&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;. In other words, the output of the neural network must be &lt;code&gt;0&lt;/code&gt; for all states &lt;code&gt;u&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Random.seed!(123)
nn1 = FastChain(FastDense(2, 64, tanh),FastDense(64, 1))
p_nn1 = initial_params(nn1)

function f1!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  du[3] = nn1(u[1:2], p)[1]
  return nothing
end

affect!(integrator) = integrator.u[2] += integrator.u[3]
cb = PresetTimeCallback(kicktimes,affect!,save_positions=(false,false))
z0 = Float32[u0;one(u0[1])]
prob1 = ODEProblem(f1!,z0,tspan,p_nn1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily compare the time evolution of the neural hybrid DE with respect to the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# to visualize the predictions of the trained neural network below
function visualize(prob,p)
  _prob = remake(prob,p=p)
  ode_pred = Array(solve(_prob,Tsit5(),callback=cb,
                 saveat=dtsave))[1:2,:]
  pl1 = plot(t_data,ode_pred[1,:],label=&amp;quot;x(t)&amp;quot;)
  scatter!(pl1,t_data[1:5:end],ode_data[1,1:5:end],label=&amp;quot;data x(t)&amp;quot;)
  pl2 = plot(t_data,ode_pred[2,:],label=&amp;quot;v(t)&amp;quot;)
  scatter!(pl2,t_data[1:5:end],ode_data[2,1:5:end],label=&amp;quot;data v(t)&amp;quot;)

  pl = plot(pl1, pl2, layout=(1,2), xlabel=&amp;quot;t&amp;quot;)
  return pl, sum(abs2,ode_data .- ode_pred)
end

pl = plot(solve(prob1,Tsit5(),saveat=t,
  callback=cb
  ),label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot; &amp;quot;u3(t)&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/untrained_nn.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/untrained_nn.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;which (of course) doesn&amp;rsquo;t match the data due to the random initialization of the neural network parameters before training. The neural network can be trained, i.e., its parameters can be optimized, by minimizing a mean-squared error loss function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;### loss function
function loss(p; prob=prob1, sensealg = ReverseDiffAdjoint())
  _prob = remake(prob,p=p)
  pred = Array(solve(_prob,Tsit5(),callback=cb,
               saveat=dtsave,sensealg=sensealg))[1:2,:]
  sum(abs2,ode_data .- pred)
end

loss(p_nn1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The recently implemented tools are deeply hidden within the 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiffEqSensitivity.jl&lt;/a&gt; package. However, while the user could previously only choose discrete sensitivities such as &lt;code&gt;ReverseDiffAdjoint()&lt;/code&gt; or  &lt;code&gt;ForwardDiffAdjoint()&lt;/code&gt; that rely on direct differentiation through the solver operations to get accurate gradients, one can now also select continuous adjoint sensitivity methods such as &lt;code&gt;BacksolveAdjoint()&lt;/code&gt;,  &lt;code&gt;InterpolatingAdjoint()&lt;/code&gt;, and &lt;code&gt;QuadratureAdjoint()&lt;/code&gt; as the &lt;code&gt;sensealg&lt;/code&gt; for hybrid DEs. Each choice has its own characteristics in terms of stability, scaling with parameters, and memory consumption, see, e.g., 
&lt;a href=&#34;https://www.youtube.com/watch?v=XRJ-rtP2fVE&amp;amp;list=PLP8iPy9hna6TxktMt-IzdU2vQpGp3bwDn&amp;amp;index=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris&#39; talk&lt;/a&gt; at the SciML symposium at SIAM CSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;###################################
# training loop
# optimize the parameters for a few epochs with ADAM
function train(prob, p_nn; sensealg=BacksolveAdjoint())
  opt = ADAM(0.0003f0)
  list_plots = []
  losses = []
  for epoch in 1:200
    println(&amp;quot;epoch: $epoch / 200&amp;quot;)
    _dy, back = Zygote.pullback(p -&amp;gt; loss(p,
      prob=prob,
      sensealg=sensealg), p_nn)
    gs = @time back(one(_dy))[1]
    push!(losses, _dy)
    if epoch % 10 == 0
      # plot every xth epoch
      pl, test_loss = visualize(prob, p_nn)
      println(&amp;quot;Loss (epoch: $epoch): $test_loss&amp;quot;)
      display(pl)
      push!(list_plots, pl)
    end
    Flux.Optimise.update!(opt, p_nn, gs)
    println(&amp;quot;&amp;quot;)
  end
  return losses, list_plots
end

# plot training loss
losses, list_plots = train(prob1, p_nn1)
pl1 = plot(losses, lw = 1.5, xlabel = &amp;quot;epoch&amp;quot;, ylabel=&amp;quot;loss&amp;quot;, legend=false)
pl2 = list_plots[end]
pl3 = plot(solve(prob1,p=p_nn1,Tsit5(),saveat=t,
   callback=cb
  ), label=[&amp;quot;x(t)&amp;quot; &amp;quot;v(t)&amp;quot; &amp;quot;u3(t)&amp;quot;])

pl = plot(pl2,pl3)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/trained1.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/trained1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We see the expected constant value of &lt;code&gt;u[3]&lt;/code&gt;, indicating a kick to the velocity of &lt;code&gt;+=1&lt;/code&gt;, at the kicking times over the full time interval.&lt;/p&gt;
&lt;h2 id=&#34;reducing-the-domain-knowledge&#34;&gt;Reducing the domain knowledge&lt;/h2&gt;
&lt;p&gt;If less physical information is included in the model design, the training becomes more difficult, e.g., due to 
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/local_minima/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;local minima&lt;/a&gt;. Possible modification for the kicked oscillator could be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;changing the initial condition of the third component of &lt;code&gt;u&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;using another affect function &lt;code&gt;affect!(integrator) = integrator.u[2] = integrator.u[3]&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;dropping the knowledge that only &lt;code&gt;u[2]&lt;/code&gt; gets a kick by using a neural network with &lt;code&gt;2&lt;/code&gt; outputs (+ a fourth component in the ODE):&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;affect2!(integrator) = integrator.u[1:2] = integrator.u[3:4]
function f2!(du,u,p,t)
  du[1] = u[2]
  du[2] = - u[1] - 1//10*u[2]
  du[3:4] .= nn2(u[1:2], nn_weights)
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fitting the parameters $a$ and $b$ simultaneously:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function f3!(du,u,p,t)
  a = p[end-1]
  b = p[end]
  nn_weights = p[1:end-2]

  du[1] = u[2]
  du[2] = -b*u[1] - a*u[2]
  du[3:4] .= nn2(u[1:2], nn_weights)
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;inferring the entire underlying dynamics using a neural network with &lt;code&gt;4&lt;/code&gt; outputs:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function f4!(du,u,p,t)
  Ω = nn3(u[1:2], p)

  du[1] = Ω[1]
  du[2] = Ω[2]
  du[3:4] .= Ω[3:4]
  return nothing
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;With respect to the adjoint sensitivity methods for hybrid DEs, we are planning to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;refine the adjoints in case of implicit discontinuities (&lt;code&gt;ContinuousCallbacks&lt;/code&gt;) and&lt;/li&gt;
&lt;li&gt;support direct usage through the 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/types/jump_types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jump problem&lt;/a&gt; interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in the upcoming weeks. For further information, the interested reader is encouraged to look at the open 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issues&lt;/a&gt; in the DiffEqSensitivity.jl package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Frank Schäfer, Pavel Sekatski, et al., Mach. Learn.: Sci. Technol. &lt;strong&gt;2&lt;/strong&gt;, 035004 (2021) &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel, arXiv preprint arXiv:2011.03902 (2020). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Junteng Jia, Austin R. Benson, arXiv preprint arXiv:1905.10403 (2019). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Michael Poli, Stefano Massaroli, et al., arXiv preprint arXiv:2106.04165 (2021). &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chris Rackauckas, Yingbo Ma, et al., &amp;ldquo;Accelerated predictive healthcare analytics with pumas, a high performance pharmaceutical modeling and simulation platform.&amp;rdquo; (2020). &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chris Rackauckas, Yingbo Ma, et al., arXiv preprint arXiv:2001.04385 (2020). &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Control of (Stochastic) Quantum Dynamics with Differentiable Programming</title>
      <link>https://frankschae.github.io/project/dp_for_control/</link>
      <pubDate>Wed, 10 Mar 2021 00:20:29 +0100</pubDate>
      <guid>https://frankschae.github.io/project/dp_for_control/</guid>
      <description>&lt;p&gt;Conceptually, it is straightforward to determine the time evolution of a quantum system for a fixed initial state given its (time-dependent) Hamiltonian or Lindbladian. Depending on the physical context, the dynamics is described by an ordinary or stochastic differential equation. In quantum state control, which is of paramount importance for quantum computation, we aim at solving the inverse problem. That is, starting from a distribution of initial states, we seek protocols that allow us to reach a desired target state by optimization of free parameters of the differential equation (control drives) in a certain time interval. To solve this control problem, we implement the system dynamics as part of a fully differentiable program and use a loss function that quantifies the distance from the target state. Specifically, we employ a neural network that maps an observation of the state of the qubit to a control drive defined via the differential equation for each time interval. To implement efficient training, we backpropagate the gradient information from the loss function through the SDE solver using adjoint sensitivity methods. Such a procedure should ultimately combine powerful tools from machine learning and scientific computation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GSoC 2020: High weak order SDE solvers and their utility in neural SDEs</title>
      <link>https://frankschae.github.io/post/gsoc2020-high-weak-order-solvers-sde-adjoints/</link>
      <pubDate>Sat, 30 May 2020 15:10:33 +0200</pubDate>
      <guid>https://frankschae.github.io/post/gsoc2020-high-weak-order-solvers-sde-adjoints/</guid>
      <description>&lt;p&gt;First and foremost, I would like to thank my mentors 
&lt;a href=&#34;https://github.com/ChrisRackauckas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris Rackauckas&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mschauer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moritz Schauer&lt;/a&gt;, and  
&lt;a href=&#34;https://github.com/YingboMa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yingbo Ma&lt;/a&gt; for their willingness to supervise me in this Google Summer of Code project.
Although we are still at the very beginning of the project, we already had plenty of very inspiring discussion. I will spend the following months implementing both  &lt;strong&gt;new high weak order solvers&lt;/strong&gt; as well as &lt;strong&gt;adjoint sensitivity methods&lt;/strong&gt; for stochastic differential equations (SDEs).
The project is embedded within the 
&lt;a href=&#34;https://sciml.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciML&lt;/a&gt; organization which, among others, unifies the latest toolsets from scientific machine learning and differential equation solver software.
Ultimately, the planned contributions will allow researchers to simulate (or even 
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/LV-stochastic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;control&lt;/a&gt;) stochastic dynamics. Also inverse problems, where 
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/NN-SDE/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SDE models are fit to data&lt;/a&gt;, fall into the scope.
Therefore, relevant applications are found in many fields ranging from the simulation of (bio-)chemical processes over financial modeling to quantum mechanics.&lt;/p&gt;
&lt;p&gt;This post is supposed to summarize what we have implemented in this first period and what we are going to do next. Future posts are going to dig into the individual subjects in more details.&lt;/p&gt;
&lt;h2 id=&#34;high-weak-order-solvers&#34;&gt;High Weak Order Solvers&lt;/h2&gt;
&lt;p&gt;Currently, the 
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StochasticDiffEq&lt;/a&gt; package contains state-of-the-art solvers for the strong approximation of SDEs, i.e., solvers that allow one to reconstruct correctly the numerical solution of an SDE in a pathwise sense.
In general, an accurate estimation of multiple stochastic integrals is then required to produce a strong method of order greater than 1/2.&lt;/p&gt;
&lt;p&gt;However in many situations, we are only aiming for computing an estimation for the &lt;strong&gt;expected value of the solution&lt;/strong&gt;.
In such situations, methods for the &lt;strong&gt;weak approximation&lt;/strong&gt; are sufficient. The less restrictive formulation of the objective for weak methods has the advantage that they are computationally cheaper than strong methods.
&lt;strong&gt;High weak order solvers&lt;/strong&gt; are particularly appealing, as they allow for using much larger time steps while attaining the same error in the mean, as compared with SDE solvers having a smaller weak order convergence.
As an example, when Monte Carlo methods are used for SDE models, it is indeed often sufficient to be able to accurately sample random trajectories of the SDE, and it is not important to accurately approximate a particular trajectory. The former is exactly what a solver with high weak order provides.&lt;/p&gt;
&lt;h3 id=&#34;second-order-runge-kutta-methods-for-ito-sdes&#34;&gt;Second order Runge-Kutta methods for Ito SDEs&lt;/h3&gt;
&lt;p&gt;In the beginning of the community bonding period I finished the implementations of the &lt;code&gt;DRI1()&lt;/code&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;code&gt;RI1()&lt;/code&gt;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; methods. Both are representing second order Runge-Kutta schemes and were introduced by Rößler. Interestingly, these methods are designed to scale well with the number of Wiener processes &lt;code&gt;m&lt;/code&gt;. Specifically, only &lt;code&gt;2m-1&lt;/code&gt; random variables have to be drawn (in contrast to &lt;code&gt;m(m+1)/2&lt;/code&gt; from previous methods). Additionally, the number of function evaluations for the drift and the diffusion terms is independent of &lt;code&gt;m&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As an example, we can check the second order convergence property on a multi-dimensional SDE with non-commuting noise&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;p&gt;$$
\scriptstyle d \begin{pmatrix} X_1 \\  X_2 \end{pmatrix} = \begin{pmatrix} -\frac{273}{512} &amp;amp;  \phantom{X_2}0 \\  -\frac{1}{160} \phantom{X_2}  &amp;amp; -\frac{785}{512}+\frac{\sqrt{2}}{8} \end{pmatrix}  \begin{pmatrix} X_1 \\  X_2 \end{pmatrix} dt + \begin{pmatrix} \frac{1}{4} X_1 &amp;amp;  \frac{1}{16} X_1 \\  \frac{1-2\sqrt{2}}{4} X_2 &amp;amp; \frac{1}{10}X_1  +\frac{1}{16} X_2 \end{pmatrix} d \begin{pmatrix} W_1 \\  W_2 \end{pmatrix} &lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;with initial value $$ X(t=0)=  \begin{pmatrix} 1 \\ 1\end{pmatrix}.$$&lt;/p&gt;
&lt;p&gt;For the function $f(x)=(x_1)^2$, we can analytically compute the expected value of the solution&lt;/p&gt;
&lt;p&gt;$$ \rm{E}\left[ f(X(t)) \right] =  \exp(-t),$$&lt;/p&gt;
&lt;p&gt;which we use to test the weak convergence order of the algorithms in the following.&lt;/p&gt;
&lt;p&gt;To compute the expected value numerically, we sample an ensemble of &lt;code&gt;numtraj = 1e7&lt;/code&gt; trajectories for different step sizes &lt;code&gt;dt&lt;/code&gt;. The code for a single  &lt;code&gt;dt&lt;/code&gt; reads:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using StochasticDiffEq
numtraj = 1e7
u₀ = [1.0,1.0]
function f!(du,u,p,t)
  du[1] = -273//512*u[1]
  du[2] = -1//160*u[1]-(-785//512+sqrt(2)/8)*u[2]
end
function g!(du,u,p,t)
  du[1,1] = 1//4*u[1]
  du[1,2] = 1//16*u[1]
  du[2,1] = (1-2*sqrt(2))/4*u[1]
  du[2,2] = 1//10*u[1]+1//16*u[2]
end
dt = 1//8
tspan = (0.0,10.0)
prob = SDEProblem(f!,g!,u₀,tspan,noise_rate_prototype=zeros(2,2))

h(z) = z^2

ensemble_prob = EnsembleProblem(prob;
        output_func = (sol,i) -&amp;gt; (h(sol[end][1]),false)
        )
sol = solve(ensemble_prob, DRI1();
        dt=dt,
        save_start=false,
        save_everystep=false,
        weak_timeseries_errors=false,
        weak_dense_errors=false,
        trajectories=numtraj)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then compute the error of the numerically obtained expected value of the ensemble simulation with respect to the analytical result:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LinearAlgebra.norm(Statistics.mean(sol.u)-exp(-tspan[2]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Repeating this procedure for some more values of &lt;code&gt;dt&lt;/code&gt;, the log-log plot of the error as a function of &lt;code&gt;dt&lt;/code&gt; displays nicely the second order convergence (slope $\approx 2.2$).&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/DRI1.png&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/DRI1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;In the next couple of weeks, my focus will be on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;adding other high weak order solvers,&lt;/li&gt;
&lt;li&gt;implementing adaptive time stepping.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More of our near-term goals are collected in this 
&lt;a href=&#34;https://github.com/SciML/StochasticDiffEq.jl/issues/182&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;adjoint-sensitivity-methods-for-sdes&#34;&gt;Adjoint Sensitivity Methods for SDEs&lt;/h2&gt;
&lt;p&gt;In 
&lt;a href=&#34;https://mitmath.github.io/18337/lecture10/estimation_identification&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parameter estimation/inverse problems&lt;/a&gt;, one is interested to know the optimal choice of parameters &lt;code&gt;p&lt;/code&gt; such that a model &lt;code&gt;f(p)&lt;/code&gt;, e.g., a differential equation, optimally fits some data, y. The shooting method approaches this task by introducing some sort of loss function $L$. A common choice is the mean squared error&lt;/p&gt;
&lt;p&gt;$$
L = |f(p)-y|^2.
$$&lt;/p&gt;
&lt;p&gt;An optimizer is then used to update the parameters $p$ such that $L$ is minimized. For this fit, local optimizers use the gradient $\frac{dL}{dp}$ to minimize the loss function and ultimately solve the inverse problem.
One possibility to obtain the gradient information for (stochastic) differential equations is to use automatic differentiation (AD).
While forward mode AD is memory efficient, it scales poorly in time with increasing number of parameters. On the contrary, reverse-mode AD, i.e., a direct backpropagation through the solver, has a huge memory footprint.&lt;/p&gt;
&lt;p&gt;Alternatively to the &amp;ldquo;direct&amp;rdquo; AD approaches, the &lt;strong&gt;adjoint sensitivity method&lt;/strong&gt; can be used&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. The adjoint sensitivity method is well known to compute gradients of solutions to ordinary differential equations (ODEs) with respect to the parameters and initial states entering the ODE. The method was recently generalized to SDEs&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.
Importantly, this new approach has different complexities in terms of memory consumption or computation time as compared with forward- or reverse-mode AD (NP vs N+P where N is the number of state variables and P is the number of parameters).&lt;/p&gt;
&lt;p&gt;It turns out that the aforementioned gradients in the stochastic adjoint sensitivity method are given by solving an SDE with an &lt;strong&gt;augmented state backwards in time&lt;/strong&gt; launched at the end state of the forward evolution.  In other words, we first compute the forward time evolution of the model from the start time $t_0$ to the end time $t_1$. Subsequently, we reverse the SDE and run a second time evolution from $t_1$ to $t_0$. Please note that the authors in Ref. &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; are implementing a slightly modfified version where the time evolution of the augmented state runs from $-t_1$ to $-t_0$. We however are indeed using the former variant as it allows us to reuse/generalize many functions that were implemented in the 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiffEqSensitivity&lt;/a&gt; package for ODE adjoints earlier.&lt;/p&gt;
&lt;h3 id=&#34;reverse-sde-time-evolution&#34;&gt;Reverse SDE time evolution&lt;/h3&gt;
&lt;p&gt;The reversion of an SDE is more difficult than the reversion of an ODE. However, for SDEs written in the Stratonovich sense, it turns out that reversion can be achieved by negative signs in front of the drift and diffusion terms.
As one needs to follow the same trajectory backward, the noise sampled in the forward pass must be reconstructed.
In general, we would like to use adaptive time-stepping solvers which require some form of interpolation for the noise values.
After some fixes for the 
&lt;a href=&#34;https://docs.sciml.ai/latest/features/noise_process/#Adaptive-NoiseWrapper-Example-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available noise processes&lt;/a&gt;, we are now able to reverse a stochastic time evolution either by using &lt;code&gt;NoiseGrid&lt;/code&gt; which linearly interpolates between values of the noise on a given grid or by using a very general &lt;code&gt;NoiseWrapper&lt;/code&gt; which interpolates in a distributionally-exact manner based on Brownian bridges.&lt;/p&gt;
&lt;p&gt;As an example, the code below computes first the forward evolution of an SDE&lt;/p&gt;
&lt;p&gt;$$ dX  =  \alpha X dt + \beta X dW$$&lt;/p&gt;
&lt;p&gt;with $\alpha=1.01$, $\beta=0.87$, $x(0)=1/2$, in the time span ($t_{0}=0$, $t_{1}=1)$. This forward evolution is shown in blue in the animation below. Subsequently, also the reverse time evolution (red) launched at time $t_{1}=1$ with initial value $x(t=1)$, propagated in negative time direction until $t_{0}=0$, is computed. We see that both trajectories match very well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;  using StochasticDiffEq, DiffEqNoiseProcess

  α=1.01
  β=0.87

  dt = 1e-3
  tspan = (0.0,1.0)
  u₀=1/2

  tarray =  collect(tspan[1]:dt:tspan[2])

  f!(du,u,p,t) = du .= α*u
  g!(du,u,p,t) = du .= β*u


  prob = SDEProblem(f!,g!,[u₀],tspan)
  sol =solve(prob,EulerHeun(),dt=dt,save_noise=true, adaptive=false)

  _sol = deepcopy(sol) # to make sure the plot is correct
  W1 = NoiseGrid(reverse!(_sol.t),reverse!(_sol.W.W))
  prob1 = SDEProblem(f!,g!,sol[end],reverse(tspan),noise=W1)
  sol1 = solve(prob1,EulerHeun(),dt=dt)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://frankschae.github.io/img/animation.gif&#34; &gt;


  &lt;img src=&#34;https://frankschae.github.io/img/animation.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;gradients-of-diagonal-sdes&#34;&gt;Gradients of diagonal SDEs&lt;/h3&gt;
&lt;p&gt;I have already started to implement the stochastic adjoint sensitivity method for SDEs possessing diagonal noise. Currently, only out-of-place SDE functions are supported but I am optimistic that soon also the inplace formulation works.&lt;/p&gt;
&lt;p&gt;Let us consider again the linear SDE with multiplicative noise from above (with the same parameters). This SDE represents one of the few exact solvable cases. In the Stratonovich sense, the solution is given as&lt;/p&gt;
&lt;p&gt;$$ X(t) =  X(0) \exp(\alpha t + \beta W(t)).$$&lt;/p&gt;
&lt;p&gt;We might be interested in optimizing the parameters $\alpha$ and $\beta$ to minimize a certain loss function acting on the solution $X(t)$. For such an optimization task, a useful search direction is indicated by the gradient of the loss function with respect to the parameters. The latter however requires the differentiation through the SDE solver &amp;ndash; if no analytical solution of the SDE is available.&lt;/p&gt;
&lt;p&gt;As an example, let us consider a mean squared error loss&lt;/p&gt;
&lt;p&gt;$$
L(X(t)) = \sum_i |X(t_i)|^2,
$$&lt;/p&gt;
&lt;p&gt;acting on the solution $X(t)$ for some fixed time points $t_i$. Then, the analytical forms for the gradients here read&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{d L}{d \alpha} &amp;amp;= 2 \sum_i t_i |X(t_i)|^2 \\&lt;br&gt;
\frac{d L}{d \beta}  &amp;amp;= 2 \sum_i W(t_i) |X(t_i)|^2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;for $\alpha$ and $\beta$, respectively. We can confirm that this agrees with the gradients as obtained by the stochastic adjoint sensitivity method&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Test, LinearAlgebra
using DiffEqSensitivity, StochasticDiffEq
using Random

seed = 100
Random.seed!(seed)

u₀ = [0.5]
tstart = 0.0
tend = 0.1
dt = 0.005
trange = (tstart, tend)
t = tstart:dt:tend
tarray = collect(t)

function g(u,p,t)
  sum(u.^2.0)
end

function dg!(out,u,p,t,i)
  (out.=-2.0*u)
end

p2 = [1.01,0.87]

f(u,p,t) = p[1]*u
σ(u,p,t) = p[2]*u


Random.seed!(seed)
prob = SDEProblem(f,σ,u₀,trange,p2)
sol = solve(prob,RKMil(interpretation=:Stratonovich),dt=tend/1e7,adaptive=false,save_noise=true)
res_u0, res_p = adjoint_sensitivities(sol,EulerHeun(),dg!,t,dt=tend/1e7,sensealg=BacksolveAdjoint())


noise = vec((@. sol.W(tarray)))
Wextracted = [W[1][1] for W in noise]
resp1 = 2*sum(@. tarray*u₀^2*exp(2*(p2[1])*tarray+2*p2[2]*Wextracted))
resp2 = 2*sum(@. Wextracted*u₀^2*exp(2*(p2[1])*tarray+2*p2[2]*Wextracted))
resp = [resp1, resp2]

@test isapprox(res_p&#39;, resp, rtol = 1e-6)
# True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With respect to the adjoint sensitivity methods, we are planning to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;finish the current backsolve adjoint version,&lt;/li&gt;
&lt;li&gt;allow for computing the gradients of non-commuting SDEs,&lt;/li&gt;
&lt;li&gt;implement also an interpolation adjoint version,&lt;/li&gt;
&lt;li&gt;benchmark it with respect to AD approaches&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in the upcoming weeks. For more information, the interested reader might take a look at the open 
&lt;a href=&#34;https://github.com/SciML/DiffEqSensitivity.jl/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issues&lt;/a&gt; in the DiffEqSensitivity package.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments, please don’t hesitate to contact me!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Kristian Debrabant, Andreas Rößler, Applied Numerical Mathematics &lt;strong&gt;59&lt;/strong&gt;, 582–594 (2009). &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Andreas Rößler, Journal on Numerical Analysis &lt;strong&gt;47&lt;/strong&gt;, 1713–1738 (2009). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Steven G. Johnson, &amp;ldquo;Notes on Adjoint Methods for 18.335.&amp;rdquo; Introduction to Numerical Methods (2012). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud, arXiv preprint arXiv:2001.01328 (2020). &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
