[{"authors":["admin"],"categories":null,"content":"I am a PhD candidate in physics within the \u0026ldquo;Quantum Computing and Quantum Technology\u0026rdquo; PhD school at the University of Basel. Currently, I am participating in the Google Summer of Code 2020 program. My project focusses on the implementation of \u0026ldquo;High weak order stochastic differential equation solvers and their utility in neural stochastic differential equations\u0026rdquo; in Julia and it is carried in collaboration with my mentors Chris Rackauckas, Moritz Schauer, and Yingbo Ma.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://frankschae.github.io/author/frank-schafer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/frank-schafer/","section":"authors","summary":"I am a PhD candidate in physics within the \u0026ldquo;Quantum Computing and Quantum Technology\u0026rdquo; PhD school at the University of Basel. Currently, I am participating in the Google Summer of Code 2020 program.","tags":null,"title":"Frank Schäfer","type":"authors"},{"authors":[],"categories":[],"content":"First and foremost, I would like to thank Chris Rackauckas, Moritz Schauer, and Yingbo Ma for their willingness to supervise me in this Google Summer of Code project. Although we are still at the very beginning of the project, we already had plenty of very inspiring discussion. I will spend the following months implementing both new high weak order solvers as well as adjoint sensitivity methods for stochastic differential equations (SDEs). Ultimately, these tools will allow researchers to simulate (or even control) the dynamics as generated by SDEs. Such set-ups are found in many fields ranging from the simulation of (bio-)chemical processes over financial modeling to quantum mechanics.\nHigh Weak Order Solvers Currently, the StochasticDiffEq package contains state-of-the-art solvers for the strong approximation of SDEs, i.e., solvers that allow one to reconstruct correctly the numerical solution of a SDE in a pathwise sense. However, in many situations methods for the weak approximation, which only aim for computing an estimation for the expected value of the solution, are sufficient. The less restrictive formulation of weak methods has the advantage that they are computationally cheaper than strong methods.\nSecond order Runge-Kutta methods for Ito SDEs In the beginning of the community bonding period I finished the implementations of the DRI1()1 and RI1()2 methods. Both are representing second order Runge-Kutta schemes and were introduced by Rößler. Interestingly, these methods are designed to scale well with the number of Wiener processes m. Specifically, only 2m-1 random variables have to be drawn and, additionally, the number of function evaluations for the drift and the diffusion terms is independent of m.\nAs an example, we can check the second order convergence property on a multi-dimensional SDE with non-commuting noise1:\n$$ \\scriptstyle d \\begin{pmatrix} X^1 \\\\ X^2 \\end{pmatrix} = \\begin{pmatrix} \\frac{-273}{512} \u0026amp; \\phantom{X^2}0 \\\\ -\\frac{1}{160} \\phantom{X^2} \u0026amp; -\\frac{785}{512}+\\frac{\\sqrt{2}}{8} \\end{pmatrix} \\begin{pmatrix} X^1 \\\\ X^2 \\end{pmatrix} dt + \\begin{pmatrix} \\frac{1}{4} X^1 \u0026amp; \\frac{1}{16} X^1 \\\\ \\frac{1-2\\sqrt{2}}{4} X^2 \u0026amp; \\frac{1}{10}X^1 +\\frac{1}{16} X^2 \\end{pmatrix} d \\begin{pmatrix} W^1 \\\\ W^2 \\end{pmatrix} $$\nwith initial value $$ X(t=0)= \\begin{pmatrix} 1 \\\\ 1\\end{pmatrix}.$$\nIf we choose a function $f(x)=(x^1)^2$, we obtain\n$$ \\rm{E}\\left[ f(X(t)) \\right] = \\exp(-t)$$\nfor the expectation value of the solution.\nA solution of this problem for a fixed dt can now be obtained by the following lines of code:\nusing StochasticDiffEq numtraj = 1e7 u₀ = [1.0,1.0] function f!(du,u,p,t) du[1] = -273//512*u[1] du[2] = -1//160*u[1]-(-785//512+sqrt(2)/8)*u[2] end function g!(du,u,p,t) du[1,1] = 1//4*u[1] du[1,2] = 1//16*u[1] du[2,1] = (1-2*sqrt(2))/4*u[1] du[2,2] = 1//10*u[1]+1//16*u[2] end dt = 1//8 tspan = (0.0,10.0) prob = SDEProblem(f!,g!,u₀,tspan,noise_rate_prototype=zeros(2,2)) h(z) = z^2 ensemble_prob = EnsembleProblem(prob; output_func = (sol,i) -\u0026gt; (h(sol[end][1]),false) ) solve(ensemble_prob, DRI1(); dt=dt, save_start=false, save_everystep=false, weak_timeseries_errors=false, weak_dense_errors=false, trajectories=numtraj)  Finally, the convergence plot displays nicely the second order convergence (slope $\\approx 2.2$)\n  In the next couple of weeks, my focus will be on\n adding other high weak order solvers, implementing adaptive time stepping.  More of our near-term goals are collected in this issue.\nAdjoint sensitivity methods for SDEs The adjoint sensitivity method is well known to compute gradients of solutions to ordinary differential equations (ODEs). Recently, this method was generalized to SDEs3. Importantly, this new method has different complexities in terms of memory consumption or computation time as compared with forward- or reverse-mode automatic differentiation (AD) approaches. While forward mode AD is memory efficient, it scales poorly in time with increasing number of parameters. On the contrary, reverse-mode AD, i.e., a direct backpropagation through the solver, has a huge memory footprint.\nI started to implement the algorithm3 (c.f., Fig. 2) in a slightly modified way: To compute the gradients, we reverse the SDE such that we run the time evolution from $t_1$ to $t_0$ (instead of a evolution from $-t_1$ to $-t_0$). Here, $t_0$ represents the start time and $t_1$ represents the end time of the time evolution. This modification allows us to reuse/generalize many functions that were implemented for ODE adjoints earlier.\nReverse SDE time evolution The reversion of a SDE is less trivial than the reversion of an ODE. However, for SDEs written in the Stratonovich sense, it turns out that reversion can be achieved by negative signs in front of the drift and diffusion terms. After some fixes for the available noise processes, we are now able to reverse a stochastic time evolution either by using a very general NoiseWrapper that interpolates in a distributionally-exact manner based on Brownian bridges, or by using NoiseGrid which linearly interpolates between values of the noise on a given grid.\nAs an example, the code below computes the forward evolution of a SDE\n$$ dX = \\alpha X dt + \\beta X dW$$\nwith $\\alpha=1.01$, $\\beta=0.87$, $x(0)=1/2$, in the time span (0,1), and, subsequently, also the reverse time evolution (1,0) with initial value $x(t=1)$.\nusing StochasticDiffEq, DiffEqNoiseProcess α=1.01 β=0.87 dt = 1e-3 tspan = (0.0,1.0) u₀=1/2 tarray = collect(tspan[1]:dt:tspan[2]) f!(du,u,p,t) = du .= α*u g!(du,u,p,t) = du .= β*u prob = SDEProblem(f!,g!,[u₀],tspan) sol =solve(prob,EulerHeun(),dt=dt,save_noise=true, adaptive=false) _sol = deepcopy(sol) # to make sure the plot is correct W1 = NoiseGrid(reverse!(_sol.t),reverse!(_sol.W.W)) prob1 = SDEProblem(f!,g!,sol[end],reverse(tspan),noise=W1) sol1 = solve(prob1,EulerHeun(),dt=dt)    Gradients of diagonal SDEs I have already started to implement the stochastic adjoint sensitivity method for SDEs possessing diagonal noise. Currently, only non-mutating SDE functions are supported but I am optimistic that soon also the inplace formulation works.\nLet us consider again the linear SDE with multiplicative noise from above (with the same parameters). This SDE represents one of the few exact solvable cases. In the Stratonovich sense, the solution is given as\n$$ X(t) = X(0) \\exp(\\alpha t + \\beta W(t)).$$\nWe might be interested in optimizing the parameters $\\alpha$ and $\\beta$ to minimize a certain loss function acting on the solution $X(t)$. For such an optimization task, a useful search direction is indicated by the gradient of the loss function with respect to the parameters. The latter however requires the differentiation through the SDE solver \u0026ndash; if no analytical solution of the SDE is available.\nAs a prototypical scenario, let us consider a mean squared error loss\n$$ L(X(t)) = \\sum_i |X(t_i)|^2, $$\nacting on the solution $X(t)$ for some fixed time points $t_i$. Then, the analytical forms for the gradients here read\n$$ \\begin{aligned} \\frac{d L}{d \\alpha} \u0026amp;= 2 \\sum_i t_i |X(t_i)|^2 \\\\\n\\frac{d L}{d \\beta} \u0026amp;= 2 \\sum_i W(t_i) |X(t_i)|^2 \\end{aligned} $$\nfor $\\alpha$ and $\\beta$, respectively. We can confirm that this agrees with the gradients as obtained by the stochastic adjoint sensitivity method\nusing Test, LinearAlgebra using DiffEqSensitivity, StochasticDiffEq using Random seed = 100 Random.seed!(seed) u₀ = [0.5] tstart = 0.0 tend = 0.1 dt = 0.005 trange = (tstart, tend) t = tstart:dt:tend tarray = collect(t) function g(u,p,t) sum(u.^2.0) end function dg!(out,u,p,t,i) (out.=-2.0*u) end p2 = [1.01,0.87] f(u,p,t) = p[1]*u σ(u,p,t) = p[2]*u Random.seed!(seed) prob = SDEProblem(f,σ,u₀,trange,p2) sol = solve(prob,RKMil(interpretation=:Stratonovich),dt=tend/1e7,adaptive=false,save_noise=true) res_u0, res_p = adjoint_sensitivities(sol,EulerHeun(),dg!,t,dt=tend/1e7,sensealg=BacksolveAdjoint()) noise = vec((@. sol.W(tarray))) Wextracted = [W[1][1] for W in noise] resp1 = 2*sum(@. tarray*u₀^2*exp(2*(p2[1])*tarray+2*p2[2]*Wextracted)) resp2 = 2*sum(@. Wextracted*u₀^2*exp(2*(p2[1])*tarray+2*p2[2]*Wextracted)) resp = [resp1, resp2] @test isapprox(res_p', resp, rtol = 1e-6) # True  With respect to the adjoint sensitivity methods, we are looking forward\n to finish the current backsolve adjoint version, to allow for computing the gradients of non-commuting SDEs, to implement also an interpolation adjoint version  in the upcoming weeks. For more information, the interested reader might take a look at the open issues in the DiffEqSensitivity package.\n  Kristian Debrabant, Andreas Rößler, Applied Numerical Mathematics 59, 582–594 (2009). \u0026#x21a9;\u0026#xfe0e;\n Andreas Rößler, Journal on Numerical Analysis 47, 1713–1738 (2009). \u0026#x21a9;\u0026#xfe0e;\n Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud, arXiv preprint arXiv:2001.01328 (2020). \u0026#x21a9;\u0026#xfe0e;\n   ","date":1590844233,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590844233,"objectID":"b18c7f652abe8e3ab1ef5f8324b079fc","permalink":"https://frankschae.github.io/post/gsoc2020-high-weak-order-solvers-sde-adjoints/","publishdate":"2020-05-30T15:10:33+02:00","relpermalink":"/post/gsoc2020-high-weak-order-solvers-sde-adjoints/","section":"post","summary":"First and foremost, I would like to thank Chris Rackauckas, Moritz Schauer, and Yingbo Ma for their willingness to supervise me in this Google Summer of Code project. Although we are still at the very beginning of the project, we already had plenty of very inspiring discussion.","tags":["GSoC 2020","High weak order solver","SRK methods","Adjoint sensitivity methods"],"title":"GSoC 2020: High weak order SDE solvers and their utility in neural SDEs","type":"post"},{"authors":[],"categories":[],"content":"","date":1589109744,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589109744,"objectID":"79e44c0f8a0e36807f6c8ada19854586","permalink":"https://frankschae.github.io/publication/njp/","publishdate":"2020-05-10T13:22:24+02:00","relpermalink":"/publication/njp/","section":"publication","summary":"Machine-learning driven models have proven to be powerful tools for the identification of phases of matter. In particular, unsupervised methods hold the promise to help discover new phases of matter without the need for any prior theoretical knowledge. While for phases characterized by a broken symmetry, the use of unsupervised methods has proven to be successful, topological phases without a local order parameter seem to be much harder to identify without supervision. Here, we use an unsupervised approach to identify boundaries of the topological phases. We train artificial neural nets to relate configurational data or measurement outcomes to quantities like temperature or tuning parameters in the Hamiltonian. The accuracy of these predictive models can then serve as an indicator for phase transitions. We successfully illustrate this approach on both the classical Ising gauge theory as well as on the quantum ground state of a generalized toric code.","tags":[],"title":"E. Greplova, A. Valenti, G. Boschung, F. Schäfer, N. Lörch, and S. Huber, New J. Phys. 22, 045003 (2020)","type":"publication"},{"authors":[],"categories":[],"content":"","date":1589109737,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589109737,"objectID":"ce947d9c8d777c18560c094394319a35","permalink":"https://frankschae.github.io/publication/entropy/","publishdate":"2020-05-10T13:22:17+02:00","relpermalink":"/publication/entropy/","section":"publication","summary":"We examine the spectral structure and many-body dynamics of two and three repulsively interacting bosons trapped in a one-dimensional double-well, for variable barrier height, inter-particle interaction strength, and initial conditions. By exact diagonalization of the many-particle Hamiltonian, we specifically explore the dynamical behavior of the particles launched either at the single-particle ground state or saddle-point energy, in a time-independent potential. We complement these results by a characterization of the cross-over from diabatic to quasi-adiabatic evolution under finite-time switching of the potential barrier, via the associated time evolution of a single particle’s von Neumann entropy. This is achieved with the help of the multiconfigurational time-dependent Hartree method for indistinguishable particles (MCTDH-X)—which also allows us to extrapolate our results for increasing particle numbers.","tags":["Bosonic systems","Exact Diagonalization","MCTDH-X"],"title":"F. Schäfer, M. A. Bastarrachea-Magnani, A. U. J. Lode, L. de Forges de Parny, and A. Buchleitner, Entropy 22, 382 (2020)","type":"publication"},{"authors":[],"categories":[],"content":"","date":1589109731,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589109731,"objectID":"919eb72cd3c1e9e9150d95b768cf58a8","permalink":"https://frankschae.github.io/publication/jphysb/","publishdate":"2020-05-10T13:22:11+02:00","relpermalink":"/publication/jphysb/","section":"publication","summary":"We investigate multiple scattering of scalar waves by an ensemble of N resonant point scatterers in three dimensions. For up to N = 21 scatterers, we numerically optimize the positions of the individual scatterers, to maximize the total scattering cross section for an incoming plane wave, on the one hand, and to minimize the decay rate associated to a long-lived scattering resonance, on the other. In both cases, the optimum is achieved by configurations where all scatterers are placed on a line parallel to the direction of the incoming plane wave. The associated maximal scattering cross section increases quadratically with the number of scatterers for large N, whereas the minimal decay rate—which is realized by configurations that are not the same as those that maximize the scattering cross section—decreases exponentially as a function of N. Finally, we also analyze the stability of our optimized configurations with respect to small random displacements of the scatterers. These results demonstrate that optimized configurations of scatterers bear a considerable potential for applications such as quantum memories or mirrors consisting of only a few atoms.","tags":["multiple scattering","numerical optimization"],"title":"F. Schäfer, F. Eckert and T. Wellens, J. Phys. B 50, 235502 (2017)","type":"publication"},{"authors":[],"categories":[],"content":"","date":1589109296,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589109296,"objectID":"d90fc2504c29d3f2758c889682fc049f","permalink":"https://frankschae.github.io/publication/pre99/","publishdate":"2020-05-10T13:14:56+02:00","relpermalink":"/publication/pre99/","section":"publication","summary":"We introduce an alternative method to identify phase boundaries in physical systems. It is based on training a predictive model such as a neural network to infer a physical system's parameters from its state. The deviation of the inferred parameters from the underlying correct parameters will be most susceptible and diverge maximally in the vicinity of phase boundaries. Therefore, peaks in the vector field divergence of the model's predictions are used as indication of phase transitions. Our method is applicable for phase diagrams of arbitrary parameter dimension and without prior information about the phases. Application to both the two-dimensional Ising model and the dissipative Kuramoto-Hopf model show promising results.","tags":["Machine Learning","Phase Transitions"],"title":"F. Schäfer and N. Lörch, Phys. Rev. E 99, 062107 (2019)","type":"publication"}]